{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "military-character",
      "metadata": {
        "id": "military-character"
      },
      "source": [
        "# Lab 6: BERT for hate speech detection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EslAmsE-tWMB",
      "metadata": {
        "id": "EslAmsE-tWMB"
      },
      "source": [
        "In this lab, we will take you through a practical use of Transformers. This notebook shows you how to use [Hugging face](https://huggingface.co/)'s package to import and train pretrained models for the tasks of hate speech classification and machine translation.\n",
        "\n",
        "We first show you all necessay components to use the ``transformers`` package before asking you to implement some code in the later sections.\n",
        "\n",
        "\n",
        "**Note:** The training of models will take quite some time so make sure to run this session with the GPU enabled. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rwXjbNUJHzZ0",
      "metadata": {
        "id": "rwXjbNUJHzZ0"
      },
      "source": [
        "## Setting up the Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vina6dhaHz1N",
      "metadata": {
        "id": "Vina6dhaHz1N"
      },
      "source": [
        "First, we need to install Hugging Face [transformers](https://huggingface.co/transformers/index.html) and [Sentence piece Tokenizers](https://github.com/google/sentencepiece) with the following commands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "other-scottish",
      "metadata": {
        "id": "other-scottish"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pandas in /homes/cb221/.local/lib/python3.8/site-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /homes/cb221/.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /homes/cb221/.local/lib/python3.8/site-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /homes/cb221/.local/lib/python3.8/site-packages (from pandas) (1.24.2)\n",
            "Requirement already satisfied: six>=1.5 in /homes/cb221/.local/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "#! pip install torch\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "modern-olympus",
      "metadata": {
        "id": "modern-olympus"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in /homes/cb221/.local/lib/python3.8/site-packages (4.26.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /homes/cb221/.local/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /homes/cb221/.local/lib/python3.8/site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /homes/cb221/.local/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /homes/cb221/.local/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /homes/cb221/.local/lib/python3.8/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /homes/cb221/.local/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /homes/cb221/.local/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /homes/cb221/.local/lib/python3.8/site-packages (from transformers) (1.24.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /homes/cb221/.local/lib/python3.8/site-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /homes/cb221/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /data2/packages/anaconda3/envs/py38_pytorch/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /homes/cb221/.local/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /homes/cb221/.local/lib/python3.8/site-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /homes/cb221/.local/lib/python3.8/site-packages (from requests->transformers) (3.0.1)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sentencepiece in /homes/cb221/.local/lib/python3.8/site-packages (0.1.97)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: ipywidgets in /homes/cb221/.local/lib/python3.8/site-packages (8.0.4)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /homes/cb221/.local/lib/python3.8/site-packages (from ipywidgets) (5.9.0)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0 in /homes/cb221/.local/lib/python3.8/site-packages (from ipywidgets) (4.0.5)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /homes/cb221/.local/lib/python3.8/site-packages (from ipywidgets) (6.21.1)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /homes/cb221/.local/lib/python3.8/site-packages (from ipywidgets) (8.9.0)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0 in /homes/cb221/.local/lib/python3.8/site-packages (from ipywidgets) (3.0.5)\n",
            "Requirement already satisfied: packaging in /homes/cb221/.local/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (23.0)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /homes/cb221/.local/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.6)\n",
            "Requirement already satisfied: psutil in /homes/cb221/.local/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.4)\n",
            "Requirement already satisfied: tornado>=6.1 in /homes/cb221/.local/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.2)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /homes/cb221/.local/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (5.2.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /homes/cb221/.local/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (25.0.0)\n",
            "Requirement already satisfied: comm>=0.1.1 in /homes/cb221/.local/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /homes/cb221/.local/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: nest-asyncio in /homes/cb221/.local/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /homes/cb221/.local/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (8.0.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /homes/cb221/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
            "Requirement already satisfied: stack-data in /homes/cb221/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in /homes/cb221/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
            "Requirement already satisfied: decorator in /homes/cb221/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /homes/cb221/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /homes/cb221/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
            "Requirement already satisfied: backcall in /homes/cb221/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /homes/cb221/.local/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /homes/cb221/.local/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.3 in /homes/cb221/.local/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (6.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /homes/cb221/.local/lib/python3.8/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /homes/cb221/.local/lib/python3.8/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /homes/cb221/.local/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /homes/cb221/.local/lib/python3.8/site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /homes/cb221/.local/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
            "Requirement already satisfied: pure-eval in /homes/cb221/.local/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: executing>=1.2.0 in /homes/cb221/.local/lib/python3.8/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
            "Requirement already satisfied: six in /homes/cb221/.local/lib/python3.8/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /homes/cb221/.local/lib/python3.8/site-packages (from importlib-metadata>=4.8.3->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (3.12.1)\n",
            "/bin/bash: jupyter: command not found\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install ipywidgets\n",
        "!jupyter nbextension enable --py widgetsnbextension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "developing-france",
      "metadata": {
        "id": "developing-france"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  print('WARNING: You may want to change the runtime to GPU for faster training!')\n",
        "  DEVICE = 'cpu'\n",
        "else:\n",
        "  DEVICE = 'cuda:0'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58jPlYaWk7fh",
      "metadata": {
        "id": "58jPlYaWk7fh"
      },
      "source": [
        "If you work in Colab, mount your google drive to save models and training checkpoints. Run the following code to connect your google drive to colab. Click on the link and copy and past the code you saw into the input box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "qjdWhQHBk6RW",
      "metadata": {
        "id": "qjdWhQHBk6RW"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks/'\n",
        "%mkdir './Lab 6'\n",
        "%cd './Lab 6' "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qJ-jH0JoVG1v",
      "metadata": {
        "id": "qJ-jH0JoVG1v"
      },
      "source": [
        "## Hate Speech Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cognitive-syndicate",
      "metadata": {
        "id": "cognitive-syndicate"
      },
      "source": [
        "### Downloading the dataset\n",
        "\n",
        "For the task of hate speech classification, we will work with the [Offensive Language Identification Dataset - OLID ](https://scholar.harvard.edu/malmasi/olid). It is a dataset of tweets hierarchically annotated on three levels: \n",
        "\n",
        "* Level A: Offensive Language Detection\n",
        "* Level B: Categorization of Offensive Language\n",
        "* Level C: Offensive Language Target Identification\n",
        "\n",
        "\n",
        "Let's download it first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "eISPHZ-7HwBk",
      "metadata": {
        "id": "eISPHZ-7HwBk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./data’: File exists\n",
            "/data2/users/cb221/NLPLabs-2023/lab06-BERT-for-hate-speech-detection/data\n",
            "/data2/users/cb221/NLPLabs-2023/lab06-BERT-for-hate-speech-detection\n"
          ]
        }
      ],
      "source": [
        "%mkdir ./data\n",
        "%cd ./data\n",
        "\n",
        "if not os.path.isfile('pretrain.txt'): \n",
        "  !wget -O pretrain.txt https://www.dropbox.com/s/bavjtyx0ndty7xt/pretrain.txt?dl=0\n",
        "\n",
        "if not os.path.isfile('OLIDv1.0.zip'): \n",
        "  !wget -O OLIDv1.0.zip https://sites.google.com/site/offensevalsharedtask/olid/OLIDv1.0.zip\n",
        "  ! unzip OLIDv1.0.zip\n",
        "  \n",
        "%cd ..\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TsIt3OVmNnRY",
      "metadata": {
        "id": "TsIt3OVmNnRY"
      },
      "source": [
        "Let's have a look at the data we downloaded.\n",
        "\n",
        "As mentioned above, the ``OLID`` dataset has been labeled for three subtask, therefore we have three different labels sets per tweet: \n",
        "* Task A: Not Offensive (``NOT``) and Offensive (``OFF``).\n",
        "* Task B: Targeted Insult (``TIN``), Untargeted (``UNT``) and ``NULL`` for not offensive tweets.\n",
        "* Task C: Individual (``IND``), Group (``GRP``), Other (``OTH``) and ``NULL`` for not offensive and non targeted tweets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "rFwKBesVNmmr",
      "metadata": {
        "id": "rFwKBesVNmmr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 13240\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>subtask_a</th>\n",
              "      <th>subtask_b</th>\n",
              "      <th>subtask_c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>86426</td>\n",
              "      <td>@USER She should ask a few native Americans wh...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>90194</td>\n",
              "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>TIN</td>\n",
              "      <td>IND</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16820</td>\n",
              "      <td>Amazon is investigating Chinese employees who ...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>62688</td>\n",
              "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>UNT</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>43605</td>\n",
              "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id                                              tweet subtask_a  \\\n",
              "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
              "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
              "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
              "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
              "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
              "\n",
              "  subtask_b subtask_c  \n",
              "0       UNT       NaN  \n",
              "1       TIN       IND  \n",
              "2       NaN       NaN  \n",
              "3       UNT       NaN  \n",
              "4       NaN       NaN  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('./data/olid-training-v1.0.tsv',delimiter=\"\\t\")\n",
        "\n",
        "print(f'Number of training samples: {len(df)}')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "published-southeast",
      "metadata": {
        "id": "published-southeast"
      },
      "source": [
        "### Loading and preprocessing the corpus \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nw6m7B6rEL6E",
      "metadata": {
        "id": "nw6m7B6rEL6E"
      },
      "source": [
        "Let's define ``reader_train`` and ``reader_test`` that will prepare our data corpus and labels for both train and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "correct-tuner",
      "metadata": {
        "id": "correct-tuner"
      },
      "outputs": [],
      "source": [
        "def reader_train(file_name):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    fin = open(file_name)\n",
        "    title = fin.readline()\n",
        "    set_a = ['NOT' , 'OFF']\n",
        "    set_b = ['NULL', 'TIN', 'UNT']\n",
        "    set_c = ['NULL', 'IND', 'GRP', 'OTH']\n",
        "    while True:\n",
        "        line = fin.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        items = line.split('\\t')\n",
        "        text = items[1]\n",
        "        label_a = set_a.index(items[2].strip())\n",
        "        label_b = set_b.index(items[3].strip())\n",
        "        label_c = set_c.index(items[4].strip())\n",
        "\n",
        "        if len(text) > 0:\n",
        "            texts.append(text)\n",
        "            labels.append([label_a, label_b, label_c])\n",
        "            \n",
        "    return {'texts':texts, 'labels':labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "designed-screening",
      "metadata": {
        "id": "designed-screening"
      },
      "outputs": [],
      "source": [
        "def reader_test(test_textlist, test_labellist):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    text_dict = {}\n",
        "    \n",
        "    # build text_dict\n",
        "    for file_text in test_textlist:\n",
        "        fin = open(file_text)\n",
        "        title = fin.readline()\n",
        "        while True:\n",
        "            line = fin.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            items = line.split('\\t')\n",
        "            if items[0] not in text_dict:\n",
        "                text_dict[items[0]] = items[1]\n",
        "        fin.close()\n",
        "    label_dict_list = []\n",
        "    \n",
        "    # build label_dict\n",
        "    for i, file_label in enumerate(test_labellist):\n",
        "        label_dict_list.append({})\n",
        "        fin = open(file_label)\n",
        "        title = fin.readline()\n",
        "        while True:\n",
        "            line = fin.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            items = line.split(',')\n",
        "            label_dict_list[i][items[0]] = items[1]\n",
        "        fin.close()    \n",
        "    \n",
        "    set_a = ['NOT' , 'OFF']\n",
        "    set_b = ['NULL', 'TIN', 'UNT']\n",
        "    set_c = ['NULL', 'IND', 'GRP', 'OTH']\n",
        "    \n",
        "    for idx, text in text_dict.items():\n",
        "        if len(text) > 0:\n",
        "            texts.append(text)\n",
        "            if idx in label_dict_list[0]:\n",
        "                label_a = label_dict_list[0][idx]\n",
        "            else:\n",
        "                label_a = 'OFF'\n",
        "            if idx in label_dict_list[1]:\n",
        "                label_b = label_dict_list[1][idx]\n",
        "            else:\n",
        "                label_b = 'NULL'\n",
        "            if idx in label_dict_list[2]:\n",
        "                label_c = label_dict_list[2][idx]\n",
        "            else:\n",
        "                label_c = 'NULL'\n",
        "            \n",
        "            label_a = set_a.index(label_a.strip())\n",
        "            label_b = set_b.index(label_b.strip())\n",
        "            label_c = set_c.index(label_c.strip())\n",
        "        \n",
        "            labels.append([label_a, label_b, label_c])\n",
        "            \n",
        "    return {'texts':texts, 'labels':labels}            \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "T4kLz8nca7ix",
      "metadata": {
        "id": "T4kLz8nca7ix"
      },
      "source": [
        "We also define our custom ``OlidDataset`` class which allows us to control how we handle the iteration and batches.\n",
        "\n",
        "At each iteration over the dataset object, the function ``__get_item__`` is called and returns a list of dictionnaries with the tweets and their 3 labels. \n",
        "Then, the ``collate_fn`` function will process the list of samples into their encodings and return a batch when called by the iterator during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "amber-exposure",
      "metadata": {
        "id": "amber-exposure"
      },
      "outputs": [],
      "source": [
        "class OlidDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, input_set):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = input_set['texts']\n",
        "        self.labels = input_set['labels']\n",
        "        \n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        texts = []\n",
        "        labels_a = []\n",
        "        labels_b = []\n",
        "        labels_c = []\n",
        "        for b in batch:\n",
        "            texts.append(b['text'])\n",
        "            labels_a.append(b['label_a'])\n",
        "            labels_b.append(b['label_b'])\n",
        "            labels_c.append(b['label_c'])\n",
        "\n",
        "        #The maximum sequence size for BERT is 512 but here the tokenizer truncate sentences longer than 128 tokens.  \n",
        "        # We also pad shorter sentences to a length of 128 tokens\n",
        "        encodings = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "        labels = {}\n",
        "        encodings['label_a'] =  torch.tensor(labels_a)\n",
        "        encodings['label_b'] =  torch.tensor(labels_b)\n",
        "        encodings['label_c'] =  torch.tensor(labels_c)\n",
        "        \n",
        "        return encodings\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "       \n",
        "        item = {'text': self.texts[idx],\n",
        "                'label_a': self.labels[idx][0],\n",
        "                'label_b': self.labels[idx][1],\n",
        "                'label_c': self.labels[idx][2]}\n",
        "        return item"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66CvfwkHbXO_",
      "metadata": {
        "id": "66CvfwkHbXO_"
      },
      "source": [
        "\n",
        "Now let's put it all together and load our data. Here we use a pre-made tokenizer that was used for our BERT model. Here we pick the pre-trained model ``bert-base-cased``. There are several other models of various sizes (base, large).\n",
        "\n",
        "**Note:** ``bert-base-cased`` is case-sensitive and it differenciates English from english. An non case-sensitive variant is ``bert-base-uncased``.\n",
        "\n",
        "You can always use another [tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html), but we will get better results using the same tokenizer as the one used to pre-train the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "nHzy19X9TUqc",
      "metadata": {
        "id": "nHzy19X9TUqc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertTokenizer(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# we can check the parameters of this tokenizer\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "deluxe-biography",
      "metadata": {
        "id": "deluxe-biography"
      },
      "outputs": [],
      "source": [
        "trainset = reader_train('./data/olid-training-v1.0.tsv')\n",
        "testset = reader_test(['./data/testset-levela.tsv','./data/testset-levelb.tsv','./data/testset-levelc.tsv'], \n",
        "                      ['./data/labels-levela.csv','./data/labels-levelb.csv','./data/labels-levelc.csv'])\n",
        "\n",
        "train_dataset = OlidDataset(tokenizer, trainset)\n",
        "test_dataset = OlidDataset(tokenizer, testset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oxEXetnGKVj2",
      "metadata": {
        "id": "oxEXetnGKVj2"
      },
      "source": [
        "The following code let's you play around with our ``train_dataset`` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1Q2VvuoaKFXB",
      "metadata": {
        "id": "1Q2VvuoaKFXB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids: [[101, 137, 1646, 9637, 1153, 1431, 2367, 170, 1374, 2900, 4038, 1184, 1147, 1321, 1113, 1142, 1110, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 137, 1646, 9637, 137, 1646, 9637, 3414, 1313, 1128, 787, 1231, 6882, 106, 106, 106, 137, 1646, 9637, 108, 9960, 10583, 108, 8499, 10973, 10973, 100, 158, 20550, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 9786, 1110, 11950, 1922, 4570, 1150, 1132, 4147, 4422, 2233, 1106, 1503, 118, 1710, 18275, 1116, 1702, 1111, 1126, 2652, 1107, 1103, 6591, 24210, 119, 158, 20550, 108, 9786, 108, 9960, 10583, 108, 148, 22689, 108, 24890, 11607, 1592, 108, 157, 15678, 1942, 102, 0, 0, 0], [101, 107, 137, 1646, 9637, 6518, 1431, 112, 1396, 1942, 9899, 1179, 107, 107, 1142, 2727, 1104, 4170, 1106, 170, 15406, 119, 100, 107, 107, 107, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 137, 1646, 9637, 137, 1646, 9637, 7661, 1458, 7691, 1116, 111, 1821, 1643, 132, 5696, 1116, 1106, 1815, 1154, 1894, 2231, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 137, 1646, 9637, 16951, 1132, 1155, 19892, 26793, 1186, 106, 106, 106, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 137, 1646, 9637, 137, 1646, 9637, 2048, 1185, 1279, 106, 1706, 6289, 4170, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 137, 1646, 9637, 1108, 6290, 1198, 2520, 1164, 1142, 25338, 1233, 1155, 3367, 4598, 1116, 1176, 1115, 1138, 1151, 1383, 12534, 119, 1122, 787, 188, 11516, 1215, 1106, 13330, 1366, 1113, 1558, 2492, 1176, 2560, 1654, 1105, 12010, 102, 0, 0, 0, 0, 0, 0, 0, 0], [101, 137, 1646, 9637, 26123, 1167, 2854, 13782, 2312, 106, 106, 106, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 137, 1646, 9637, 1803, 2144, 787, 189, 1444, 1330, 140, 21986, 2428, 106, 1284, 1640, 1138, 1536, 108, 10605, 18066, 2162, 11470, 1204, 108, 16951, 175, 115, 115, 2226, 1146, 1412, 1632, 1583, 106, 108, 154, 20324, 1116, 108, 157, 25980, 8221, 2107, 8954, 2349, 1186, 102]]\n",
            "token_type_ids: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "attention_mask: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
            "label_a: [1, 1, 0, 1, 0, 1, 1, 1, 0, 1]\n",
            "label_b: [2, 1, 0, 2, 0, 1, 2, 1, 0, 1]\n",
            "label_c: [0, 1, 0, 0, 0, 3, 0, 2, 0, 1]\n"
          ]
        }
      ],
      "source": [
        "#returns first item as dictionnary\n",
        "#print(train_dataset[0])\n",
        "\n",
        "# put all train set into one batch for the collate_fn function\n",
        "batch = [sample for sample in train_dataset]\n",
        "\n",
        "encodings = train_dataset.collate_fn(batch[:10])\n",
        "\n",
        "for key, value in encodings.items():\n",
        "  print(f\"{key}: {value.numpy().tolist()}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "opponent-closure",
      "metadata": {
        "id": "opponent-closure"
      },
      "source": [
        "### Finetuning a pre-trained BERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dpaMPoB5cNwt",
      "metadata": {
        "id": "dpaMPoB5cNwt"
      },
      "source": [
        "\n",
        "As you can recall from the lecture, BERT is a model trained on Masked language Modeling(MLM) and Next Sentence Prediction(NSP), however is not trained to do to do sentence classification. We then need to adapt it for hate speech classification and finetune the pre-trained model on our dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n2zPF-H3nHDH",
      "metadata": {
        "id": "n2zPF-H3nHDH"
      },
      "source": [
        "Let's have a look at ``bert_base-uncased`` summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "pk_QEpoGnZhL",
      "metadata": {
        "id": "pk_QEpoGnZhL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model size: 108310272\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "#180 M\n",
        "print(f\"Model size: {model.num_parameters()}\")\n",
        "\n",
        "#model summary\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GBOHvhd-n00L",
      "metadata": {
        "id": "GBOHvhd-n00L"
      },
      "source": [
        "Note that the model has only encoder layers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ESBsGVb_e2QT",
      "metadata": {
        "id": "ESBsGVb_e2QT"
      },
      "source": [
        "#### BERT Model\n",
        "\n",
        "To define our model, we will build on top of a Huggingface pre-trained model and adapt it to our task. We will use ``BertModel`` to extract embeddings and add a ``Linear`` layer to classify samples. Hugging face implementation of BERT can handle different variations of the model, which we define and pass its parameter values via``config``.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feb9AN0YFWDe",
      "metadata": {
        "id": "feb9AN0YFWDe"
      },
      "source": [
        "The code below defines a model adapted to classify tweets on Level A, Offensive Language Detection. We will implement Task B and C later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "bottom-tribute",
      "metadata": {
        "id": "bottom-tribute"
      },
      "outputs": [],
      "source": [
        "class BERT_hate_speech(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        # BERT Model\n",
        "        self.bert = BertModel(config)\n",
        "        \n",
        "        # Task A\n",
        "        self.projection_a = torch.nn.Sequential(torch.nn.Dropout(0.2),\n",
        "                                                torch.nn.Linear(config.hidden_size, 2))\n",
        "        \n",
        "        # Task B\n",
        "        # TBA\n",
        "        \n",
        "        # Task C\n",
        "        # TBA\n",
        "        \n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None):\n",
        " \n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # Logits A\n",
        "        logits_a = self.projection_a(outputs[1])\n",
        "        \n",
        "        return logits_a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yMyL5gGKfECs",
      "metadata": {
        "id": "yMyL5gGKfECs"
      },
      "source": [
        "#### Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QaKWwqYMlCVX",
      "metadata": {
        "id": "QaKWwqYMlCVX"
      },
      "source": [
        "Finally, we should define our training loop. Fortunately, the ``transformers`` package provides us with a [``Trainer``](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer) class wich takes care of the training of transformers models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MjljHz4nlAbJ",
      "metadata": {
        "id": "MjljHz4nlAbJ"
      },
      "source": [
        "We build our custom ``Trainer`` class to incorporate our own ``compute_loss`` function over the three labels. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "engaged-perspective",
      "metadata": {
        "id": "engaged-perspective"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Trainer_hate_speech(Trainer):\n",
        "    def compute_loss(self, model, inputs):\n",
        "        labels = {}\n",
        "        labels['label_a'] = inputs.pop('label_a')\n",
        "        labels['label_b'] = inputs.pop('label_b')\n",
        "        labels['label_c'] = inputs.pop('label_c')\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # TASK A\n",
        "        loss_task_a = nn.CrossEntropyLoss()\n",
        "        labels_a = labels['label_a']\n",
        "        loss_a = loss_task_a(outputs.view(-1, 2), labels_a.view(-1))\n",
        "\n",
        "        loss = loss_a\n",
        "        \n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p0WDX456F4YB",
      "metadata": {
        "id": "p0WDX456F4YB"
      },
      "source": [
        "\n",
        "Now let's finetune the pretrained model on our ``OlidDataset``.\n",
        "\n",
        "In our function ``main_hate_speech`` we define the arguments for the ``Trainer`` object and launch the training with ``trainer.train``. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "killing-population",
      "metadata": {
        "id": "killing-population"
      },
      "outputs": [],
      "source": [
        "def main_hate_speech():\n",
        "\n",
        "    #call our custom BERT model and pass as parameter the name of an available pretrained model\n",
        "    model = BERT_hate_speech.from_pretrained(\"bert-base-cased\")\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./experiment/hate_speech',\n",
        "        learning_rate = 0.0001,\n",
        "        logging_steps= 100,\n",
        "        per_device_train_batch_size=32,\n",
        "        num_train_epochs = 3,\n",
        "        remove_unused_columns=False # This argument prevents the collator to drop data from our batch when customizing the data collator\n",
        "    )\n",
        "    trainer = Trainer_hate_speech(\n",
        "        model=model,                         \n",
        "        args=training_args,                 \n",
        "        train_dataset=train_dataset,                   \n",
        "        data_collator=train_dataset.collate_fn,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model('./models/ht_bert_finetuned/')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OanfT22IduY6",
      "metadata": {
        "id": "OanfT22IduY6"
      },
      "source": [
        "Let's run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "looking-escape",
      "metadata": {
        "id": "looking-escape"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /homes/cb221/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /homes/cb221/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BERT_hate_speech: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BERT_hate_speech from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BERT_hate_speech from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BERT_hate_speech were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['projection_a.1.weight', 'projection_a.1.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 600.18 MiB already allocated; 13.69 MiB free; 654.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main_hate_speech()\n",
            "Cell \u001b[0;32mIn[21], line 14\u001b[0m, in \u001b[0;36mmain_hate_speech\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[39m=\u001b[39m BERT_hate_speech\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mbert-base-cased\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      7\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./experiment/hate_speech\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     learning_rate \u001b[39m=\u001b[39m \u001b[39m0.0001\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     remove_unused_columns\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m trainer \u001b[39m=\u001b[39m Trainer_hate_speech(\n\u001b[1;32m     15\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,                         \n\u001b[1;32m     16\u001b[0m     args\u001b[39m=\u001b[39;49mtraining_args,                 \n\u001b[1;32m     17\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mtrain_dataset,                   \n\u001b[1;32m     18\u001b[0m     data_collator\u001b[39m=\u001b[39;49mtrain_dataset\u001b[39m.\u001b[39;49mcollate_fn,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[39m#trainer.train()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m trainer\u001b[39m.\u001b[39msave_model(\u001b[39m'\u001b[39m\u001b[39m./models/ht_bert_finetuned/\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:445\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m tokenizer\n\u001b[1;32m    444\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplace_model_on_device:\n\u001b[0;32m--> 445\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_move_model_to_device(model, args\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    447\u001b[0m \u001b[39m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_model_parallel:\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py:688\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_move_model_to_device\u001b[39m(\u001b[39mself\u001b[39m, model, device):\n\u001b[0;32m--> 688\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mparallel_mode \u001b[39m==\u001b[39m ParallelMode\u001b[39m.\u001b[39mTPU \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mtie_weights\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:1749\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1745\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1746\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1747\u001b[0m     )\n\u001b[1;32m   1748\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/data2/packages/anaconda3/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:989\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    987\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
            "File \u001b[0;32m/data2/packages/anaconda3/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/data2/packages/anaconda3/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "    \u001b[0;31m[... skipping similar frames: Module._apply at line 641 (3 times)]\u001b[0m\n",
            "File \u001b[0;32m/data2/packages/anaconda3/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:641\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    640\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 641\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    643\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    644\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    645\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    646\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    652\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m/data2/packages/anaconda3/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:664\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    663\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 664\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    665\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    666\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m/data2/packages/anaconda3/envs/py38_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:987\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    985\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    986\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 987\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 600.18 MiB already allocated; 13.69 MiB free; 654.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "main_hate_speech()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6Q0sYQeoM90U",
      "metadata": {
        "id": "6Q0sYQeoM90U"
      },
      "source": [
        "#### Evaluation\n",
        "Once we trained our model, we can evaluate it on our test set."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tt8KRJs2OSo1",
      "metadata": {
        "id": "tt8KRJs2OSo1"
      },
      "source": [
        "Let's define a helper function ``predict_hatespeech`` that will extract the predicted label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "leJGKWvXGDf7",
      "metadata": {
        "id": "leJGKWvXGDf7"
      },
      "outputs": [],
      "source": [
        "def predict_hatespeech(input, tokenizer, model): \n",
        "  model.eval()\n",
        "  encodings = tokenizer(input, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "  \n",
        "  output = model(**encodings)\n",
        "  preds = torch.max(output, 1)\n",
        "\n",
        "  return {'prediction':preds[1], 'confidence':preds[0]}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_y2mgB3-PQpI",
      "metadata": {
        "id": "_y2mgB3-PQpI"
      },
      "source": [
        "Now let's define a function that will evaluate our model on the test set we prepared."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2olq7egUHYnH",
      "metadata": {
        "id": "2olq7egUHYnH"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, tokenizer, data_loader):\n",
        "\n",
        "  total_count = 0\n",
        "  correct_count = 0 \n",
        "\n",
        "  preds = []\n",
        "  tot_labels = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in tqdm(data_loader): \n",
        "\n",
        "      labels = {}\n",
        "      labels['label_a'] = data['label_a']\n",
        "\n",
        "      tweets = data['text']\n",
        "\n",
        "      pred = predict_hatespeech(tweets, tokenizer, model)\n",
        "\n",
        "      preds.append(pred['prediction'].tolist())\n",
        "      tot_labels.append(labels['label_a'].tolist())\n",
        "\n",
        "  # with the saved predictions and labels we can compute accuracy, precision, recall and f1-score\n",
        "  report = classification_report(tot_labels, preds, target_names=[\"Not offensive\",\"Offensive\"], output_dict= True)\n",
        "\n",
        "  return report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "Sps-kXvWeNoO",
      "metadata": {
        "id": "Sps-kXvWeNoO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading file vocab.txt from cache at /homes/cb221/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /homes/cb221/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /homes/cb221/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "Can't load the configuration of './models/ht_bert_finetuned/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './models/ht_bert_finetuned/' is the correct path to a directory containing a config.json file",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:620\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    621\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    622\u001b[0m         configuration_file,\n\u001b[1;32m    623\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    624\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    625\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    626\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    627\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    628\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    629\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    630\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    631\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    632\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    633\u001b[0m     )\n\u001b[1;32m    634\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    410\u001b[0m         path_or_repo_id,\n\u001b[1;32m    411\u001b[0m         filename,\n\u001b[1;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 114\u001b[0m     validate_repo_id(arg_value)\n\u001b[1;32m    116\u001b[0m \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:166\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m repo_id\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    167\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './models/ht_bert_finetuned/'. Use `repo_type` argument if needed.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m#your saved model name here\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./models/ht_bert_finetuned/\u001b[39m\u001b[39m'\u001b[39m \n\u001b[0;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m BERT_hate_speech\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[1;32m      7\u001b[0m \u001b[39m# we don't batch our test set unless it's too big\u001b[39;00m\n\u001b[1;32m      8\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(test_dataset)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:2079\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2077\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   2078\u001b[0m     config_path \u001b[39m=\u001b[39m config \u001b[39mif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 2079\u001b[0m     config, model_kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mconfig_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m   2080\u001b[0m         config_path,\n\u001b[1;32m   2081\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2082\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2083\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   2084\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   2085\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   2086\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2087\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   2088\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2089\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   2090\u001b[0m         _from_auto\u001b[39m=\u001b[39;49mfrom_auto_class,\n\u001b[1;32m   2091\u001b[0m         _from_pipeline\u001b[39m=\u001b[39;49mfrom_pipeline,\n\u001b[1;32m   2092\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2093\u001b[0m     )\n\u001b[1;32m   2094\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2095\u001b[0m     model_kwargs \u001b[39m=\u001b[39m kwargs\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:538\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    461\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPretrainedConfig\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    462\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[39m    Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39m    assert unused_kwargs == {\"foo\": False}\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[39m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m     config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    539\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type:\n\u001b[1;32m    540\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    541\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are using a model of type \u001b[39m\u001b[39m{\u001b[39;00mconfig_dict[\u001b[39m'\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m to instantiate a model of type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    542\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m         )\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:565\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    564\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    566\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    567\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:641\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    639\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    640\u001b[0m         \u001b[39m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[0;32m--> 641\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load the configuration of \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m from \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    644\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m name. Otherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    645\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m containing a \u001b[39m\u001b[39m{\u001b[39;00mconfiguration_file\u001b[39m}\u001b[39;00m\u001b[39m file\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    646\u001b[0m         )\n\u001b[1;32m    648\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    649\u001b[0m     \u001b[39m# Load config dict\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     config_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_dict_from_json_file(resolved_config_file)\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load the configuration of './models/ht_bert_finetuned/'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure './models/ht_bert_finetuned/' is the correct path to a directory containing a config.json file"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "#your saved model name here\n",
        "model_name = './models/ht_bert_finetuned/' \n",
        "model = BERT_hate_speech.from_pretrained(model_name)\n",
        "\n",
        "# we don't batch our test set unless it's too big\n",
        "test_loader = DataLoader(test_dataset)\n",
        "\n",
        "report = evaluate(model, tokenizer, test_loader)\n",
        "\n",
        "print(report)\n",
        "\n",
        "print(report['accuracy'])\n",
        "print(report['Not offensive']['f1-score'])\n",
        "print(report['Offensive']['f1-score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g9eJi1IaOGZ6",
      "metadata": {
        "id": "g9eJi1IaOGZ6"
      },
      "source": [
        "Let's test our model on a few sentences to get an intuition. Feel free to play around."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49umxeBiK3Ec",
      "metadata": {
        "id": "49umxeBiK3Ec"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "model = BERT_hate_speech.from_pretrained('./models/ht_bert_finetuned/')\n",
        "\n",
        "print(predict_hatespeech(\"I go see pinguins at the zoo.\", tokenizer, model))\n",
        "print(predict_hatespeech(\"Bananas are stupid\", tokenizer, model))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JaD0CmXMhEmA",
      "metadata": {
        "id": "JaD0CmXMhEmA"
      },
      "source": [
        "### Pre-training and finetuning BERT\n",
        "\n",
        "In this section, we will implement our own masked language modeling (MLM) training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KmT9ztohfiW3",
      "metadata": {
        "id": "KmT9ztohfiW3"
      },
      "source": [
        "#### Pre-training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9-mAsIUYZ7Ex",
      "metadata": {
        "id": "9-mAsIUYZ7Ex"
      },
      "source": [
        "**Question 1: Add MLM head for pretraining**\n",
        "Your task is to fill in the following classes to implement MLM training: \n",
        "\n",
        "* ``PretrainDataset()``\n",
        "* ``Trainer_MLM()``\n",
        "* ``BERT_pretrain()``\n",
        "* ``main_pretrain()``"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C-3Jvmryw-ko",
      "metadata": {
        "id": "C-3Jvmryw-ko"
      },
      "source": [
        "To train our model in a MLM fashion, we need to make some adjustment to our ``Dataset`` class. We want to train BERT to predict an X% of tokens (in the original paper it is 15%) of which 80% will be replaced by a ``[MASK]`` token, 10% with a random token and 10% remain the same token.\n",
        "\n",
        "We introduce the function ``mask_tokens`` that will take care of that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "differential-ordinary",
      "metadata": {
        "id": "differential-ordinary"
      },
      "outputs": [],
      "source": [
        "class PretrainDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, input_file):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.texts = self.read_text(input_file)\n",
        "\n",
        "        self.mlm_probability = 0.15\n",
        "        \n",
        "    def read_text(self, input_file):\n",
        "\n",
        "        ## Question 1 ##\n",
        "\n",
        "        fin = open(input_file)\n",
        "        return fin.readlines()\n",
        "        \n",
        "    def collate_fn(self, batch):\n",
        "       \n",
        "        ## Question 1 ##\n",
        "\n",
        "        batch = self.tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "\n",
        "        inputs, labels = self.mask_tokens(batch[\"input_ids\"])\n",
        "        return {\"input_ids\": inputs, \"labels\": labels}\n",
        "    \n",
        "        return encodings\n",
        "    \n",
        "    def mask_tokens(self, inputs):\n",
        "        \"\"\"\n",
        "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
        "        \"\"\"\n",
        "        if self.tokenizer.mask_token is None:\n",
        "            raise ValueError(\n",
        "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.\"\n",
        "            )\n",
        "        labels = inputs.clone()\n",
        "\n",
        "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
        "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
        "        special_tokens_mask = [\n",
        "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
        "        ]\n",
        "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        "        \n",
        "        if self.tokenizer._pad_token is not None:\n",
        "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
        "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
        "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
        "\n",
        "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
        "\n",
        "        # 10% of the time, we replace masked input tokens with random word\n",
        "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
        "        inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "        return inputs, labels\n",
        "    \n",
        "    def __len__(self):\n",
        "        \n",
        "        ## Question 1 ##\n",
        "\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        ## Question 1 ##\n",
        " \n",
        "        text = self.texts[idx]\n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-4yxeOG1Z94H",
      "metadata": {
        "id": "-4yxeOG1Z94H"
      },
      "source": [
        "The next step is to add a MLM head to our model. \n",
        "Use the ``BertOnlyMLMHead`` to add a MLM classifier to BERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beginning-fluid",
      "metadata": {
        "id": "beginning-fluid"
      },
      "outputs": [],
      "source": [
        "from transformers.models.bert.modeling_bert import BertOnlyMLMHead\n",
        "\n",
        "class BERT_pretrain(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        ## Question 1 ##\n",
        "        # BERT Model\n",
        "        self.bert = BertModel(config)\n",
        "        \n",
        "        \n",
        "        ## Question 1 ##\n",
        "        # MLM head\n",
        "        self.cls = BertOnlyMLMHead(config)\n",
        "        \n",
        "        \n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        ## Question 1 ##\n",
        "\n",
        "        # MLM output\n",
        "        prediction_scores = self.cls(outputs[0])\n",
        "        \n",
        "        return prediction_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46_QOzQfLgME",
      "metadata": {
        "id": "46_QOzQfLgME"
      },
      "source": [
        "We will define a new Trainer class for pre-training. \n",
        "\n",
        "**Note:** We could use the standard ``Trainer`` class to train our model. Then we would need to make ``BERT_pretrain`` output  ``loss`` and BERT ``outputs`` as a tuple``(loss, outputs)``.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xoG-AvUdzyqt",
      "metadata": {
        "id": "xoG-AvUdzyqt"
      },
      "outputs": [],
      "source": [
        "class Trainer_MLM(Trainer):\n",
        "    def compute_loss(self, model, inputs):\n",
        "        \n",
        "        labels = inputs['labels']\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # MLM loss\n",
        "        lm_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        loss_mlm = lm_loss(outputs.view(-1, model.config.vocab_size), labels.view(-1))\n",
        "        \n",
        "        loss = loss_mlm\n",
        "        \n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q3xGoHTSh-Yu",
      "metadata": {
        "id": "q3xGoHTSh-Yu"
      },
      "source": [
        "Finally, put everything together in the ``main_pretrain()`` class. \n",
        "\n",
        "In the code below, write code to pre-train your custom MLM model on ``pretrain.txt`` file found in the ``data`` folder.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "familiar-singles",
      "metadata": {
        "id": "familiar-singles"
      },
      "outputs": [],
      "source": [
        "def main_pretrain():\n",
        "    \n",
        "    ## Question 1 ##\n",
        "\n",
        "    model = BERT_pretrain.from_pretrained(\"bert-base-cased\")\n",
        "    \n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "    pretrain_dataset = PretrainDataset(tokenizer, 'data/pretrain.txt')\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./experiment/pretrain',\n",
        "        learning_rate = 0.00005,\n",
        "        num_train_epochs =1,\n",
        "        save_steps = 10000,  #saves a checkpoint file every 10000 iterations\n",
        "        per_device_train_batch_size=64,\n",
        "        remove_unused_columns=False\n",
        "    )\n",
        "    trainer = Trainer_MLM(\n",
        "        model=model,                         \n",
        "        args=training_args,                 \n",
        "        train_dataset=pretrain_dataset,                    \n",
        "        data_collator=pretrain_dataset.collate_fn\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    \n",
        "    trainer.save_model('./models/ht_bert_pretrained/')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4-tsbImSJvyf",
      "metadata": {
        "id": "4-tsbImSJvyf"
      },
      "source": [
        "Running the pretraining will take ~ 2 hours with one epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "greenhouse-signal",
      "metadata": {
        "id": "greenhouse-signal"
      },
      "outputs": [],
      "source": [
        " main_pretrain()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "incorporated-sleep",
      "metadata": {
        "id": "incorporated-sleep"
      },
      "source": [
        "#### Finetuning\n",
        "\n",
        "**Question 2: Load the pretrained model for finetuning**\n",
        "\n",
        "In the code below modify the ``main_hate_speech`` function from earlier to import the model we just trained, and finetune it on our ``OlidDataset`` train sets.\n",
        "\n",
        "**Note**: Your pre-trained model is saved as checkpoint files in your ``output_dir`` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "metropolitan-cliff",
      "metadata": {
        "id": "metropolitan-cliff"
      },
      "outputs": [],
      "source": [
        "def main_hate_speech():\n",
        "\n",
        "    ## Question 2 ##\n",
        "\n",
        "    model = BERT_hate_speech.from_pretrained(\"./models/ht_bert_pretrained/\")\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./experiment/hate_speech',\n",
        "        learning_rate = 0.0001,\n",
        "        logging_steps= 500,\n",
        "        per_device_train_batch_size=32,\n",
        "        num_train_epochs = 1\n",
        "    )\n",
        "    trainer = Trainer_hate_speech(\n",
        "        model=model,                         \n",
        "        args=training_args,                 \n",
        "        train_dataset=train_dataset,        \n",
        "        eval_dataset=test_dataset,             \n",
        "        data_collator=train_dataset.collate_fn\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model('./models/ht_bert_pretrained_finetuned/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "simplified-exemption",
      "metadata": {
        "id": "simplified-exemption"
      },
      "outputs": [],
      "source": [
        "main_hate_speech()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eqtu-ECSaUO",
      "metadata": {
        "id": "8eqtu-ECSaUO"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qCC3iV37M7np",
      "metadata": {
        "id": "qCC3iV37M7np"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "#your saved model name here\n",
        "model_name = './models/ht_bert_pretrained_finetuned/' \n",
        "model = BERT_hate_speech.from_pretrained(model_name)\n",
        "\n",
        "test_loader = DataLoader(test_dataset)\n",
        "\n",
        "report = evaluate(model, tokenizer, test_loader)\n",
        "print(report)\n",
        "print(report['accuracy'])\n",
        "print(report['Not offensive']['f1-score'])\n",
        "print(report['Offensive']['f1-score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "existing-courtesy",
      "metadata": {
        "id": "existing-courtesy"
      },
      "source": [
        "## Multi-task Hate Speech Classification\n",
        "\n",
        "It's time to add the two other tasks to our implementation of ``BERT_hate_speech()``.\n",
        "\n",
        "**Question 3: Add multi-heads (task b, task c) for multi-task hatespeech classification**\n",
        "\n",
        "Fill in the missing code from the following classes:\n",
        "\n",
        "* ``BERT_hate_speech_multitask()``\n",
        "* `` Trainer_hate_speech_multitask()``\n",
        "* ``main_hate_speech_multitask()``"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ogHc8w4Kyew",
      "metadata": {
        "id": "9ogHc8w4Kyew"
      },
      "source": [
        "### Multi-task Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "personalized-teddy",
      "metadata": {
        "id": "personalized-teddy"
      },
      "outputs": [],
      "source": [
        "\n",
        "class BERT_hate_speech_multitask(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        \n",
        "        # BERT Model\n",
        "        self.bert = BertModel(config)\n",
        "        \n",
        "        # Task A\n",
        "        self.projection_a = torch.nn.Sequential(torch.nn.Dropout(0.2),\n",
        "                                                torch.nn.Linear(config.hidden_size, 2))\n",
        "        \n",
        "        ##  Question 3 ##\n",
        "\n",
        "        # Task B\n",
        "        self.projection_b = torch.nn.Sequential(torch.nn.Dropout(0.2),\n",
        "                                                torch.nn.Linear(config.hidden_size, 3))\n",
        "\n",
        "        # Task C\n",
        "        self.projection_c = torch.nn.Sequential(torch.nn.Dropout(0.2),\n",
        "                                                torch.nn.Linear(config.hidden_size, 4))\n",
        "        \n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # Task A\n",
        "        logits_a = self.projection_a(outputs[1])\n",
        "        \n",
        "        ##  Question 3 ##\n",
        "        \n",
        "        # Task B\n",
        "        logits_b = self.projection_b(outputs[1])\n",
        "      \n",
        "        # Task C \n",
        "        logits_c = self.projection_c(outputs[1])\n",
        "\n",
        "        return (logits_a, logits_b, logits_c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lwCad-uo1pnP",
      "metadata": {
        "id": "lwCad-uo1pnP"
      },
      "outputs": [],
      "source": [
        "class Trainer_hate_speech_multitask(Trainer):\n",
        "    def compute_loss(self, model, inputs):\n",
        "        labels = {}\n",
        "        labels['label_a'] = inputs.pop('label_a')\n",
        "        labels['label_b'] = inputs.pop('label_b')\n",
        "        labels['label_c'] = inputs.pop('label_c')\n",
        "\n",
        "        (out_a, out_b, out_c) = model(**inputs)\n",
        "\n",
        "        # LOSS A\n",
        "        loss_task_a = nn.CrossEntropyLoss()\n",
        "        labels_a = labels['label_a']\n",
        "        loss_a = loss_task_a(out_a.view(-1, 2), labels_a.view(-1))\n",
        "\n",
        "        ## QUESTION 3 ##        \n",
        "        # LOSS B\n",
        "        loss_task_b = nn.CrossEntropyLoss()\n",
        "        labels_b = labels['label_b']\n",
        "        loss_b = loss_task_b(out_b.view(-1, 3), labels_b.view(-1))\n",
        "\n",
        "        # LOSS C\n",
        "        loss_task_c = nn.CrossEntropyLoss()\n",
        "        labels_c = labels['label_c']\n",
        "        loss_c = loss_task_c(out_c.view(-1, 4), labels_c.view(-1))\n",
        "\n",
        "        loss = loss_a + loss_b + loss_c\n",
        "        \n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LSA-HdbWjpAl",
      "metadata": {
        "id": "LSA-HdbWjpAl"
      },
      "source": [
        "Just as in the finetuning task, instantiate a ``BERT_hate_speech_multitask`` model from an pre-trained model and finetune it on our ``train_dataset``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "satisfied-short",
      "metadata": {
        "id": "satisfied-short"
      },
      "outputs": [],
      "source": [
        "def main_hate_speech_multitask():\n",
        "    ##  Question 3 ##\n",
        "\n",
        "    model = BERT_hate_speech_multitask.from_pretrained(\"bert-base-cased\")\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./experiment/hate_speech_multitask',\n",
        "        learning_rate = 0.0001,\n",
        "        logging_steps= 100,\n",
        "        num_train_epochs = 3,\n",
        "        per_device_train_batch_size=64,\n",
        "        remove_unused_columns=False\n",
        "    )\n",
        "    trainer = Trainer_hate_speech_multitask(\n",
        "        model=model,                         \n",
        "        args=training_args,                 \n",
        "        train_dataset=train_dataset,                 \n",
        "        data_collator=train_dataset.collate_fn\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model('./models/ht_bert_multi_finetuned/')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-P1ekYHaqPwR",
      "metadata": {
        "id": "-P1ekYHaqPwR"
      },
      "source": [
        "Running the code below should take ~10 min for 3 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wrapped-trading",
      "metadata": {
        "id": "wrapped-trading"
      },
      "outputs": [],
      "source": [
        "main_hate_speech_multitask()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p2EFyb3rKtVB",
      "metadata": {
        "id": "p2EFyb3rKtVB"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8EtaoYYGJ6J2",
      "metadata": {
        "id": "8EtaoYYGJ6J2"
      },
      "outputs": [],
      "source": [
        "def predict_hatespeech_multitask(input, tokenizer, model): \n",
        "  model.eval()\n",
        "  encodings = tokenizer(input, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "  \n",
        "  (out1, out2, out3) = model(**encodings)\n",
        "  \n",
        "  preds_a = torch.max(out1, 1)\n",
        "  preds_b = torch.max(out2, 1)\n",
        "  preds_c = torch.max(out3, 1)\n",
        "\n",
        "  preds = (preds_a[1], preds_b[1], preds_c[1])\n",
        "  scores = (preds_a[0], preds_b[0], preds_c[0])\n",
        "\n",
        "  return {'predictions':preds, 'confidences':scores}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "z5pIqvWoLbDy",
      "metadata": {
        "id": "z5pIqvWoLbDy"
      },
      "outputs": [],
      "source": [
        "def evaluate_multitask(model, tokenizer, data_loader):\n",
        "\n",
        "  task_num = 3\n",
        "  total_count = 0\n",
        "  correct_count = [0] * task_num  \n",
        "  accuracies = [0] * task_num\n",
        "\n",
        "  batch_size = data_loader.batch_size\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for data in tqdm(data_loader): \n",
        "\n",
        "      labels = {}\n",
        "      labels['label_a'] = data['label_a']\n",
        "      labels['label_b'] = data['label_b']\n",
        "      labels['label_c'] = data['label_c']\n",
        "\n",
        "      tweets = data['text']\n",
        "\n",
        "      pred = predict_hatespeech_multitask(tweets, tokenizer, model)\n",
        "\n",
        "      preds = pred['predictions'] \n",
        "\n",
        "      for i, label in enumerate(labels):\n",
        "        correct_count[i]+= torch.mean((preds[i] == labels[label]).float())\n",
        "\n",
        "      total_count += np.float(batch_size)\n",
        "\n",
        "    for i, label in enumerate(labels):\n",
        "      accuracies[i] = (correct_count[i]/total_count)\n",
        "\n",
        " \n",
        "  return accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "P24QVnAULiMd",
      "metadata": {
        "id": "P24QVnAULiMd"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = BERT_hate_speech_multitask.from_pretrained(\"./models/ht_bert_multi_finetuned/\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "test_loader = DataLoader(test_dataset)\n",
        "\n",
        "accuracies = evaluate_multitask(model, tokenizer, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-h8pw9i8L8NJ",
      "metadata": {
        "id": "-h8pw9i8L8NJ"
      },
      "outputs": [],
      "source": [
        "for i in range(3):\n",
        "    print('Task %d accuracy: %2.2f %%' % (i, 100.0*accuracies[i]))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yA_vw43f-UX4",
      "metadata": {
        "id": "yA_vw43f-UX4"
      },
      "outputs": [],
      "source": [
        "print(predict_hatespeech_multitask(\"I go see pinguins at the zoo.\", tokenizer, model)['predictions'])\n",
        "print(predict_hatespeech_multitask(\"Bananas are so stupid \", tokenizer, model)['predictions'])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "lab06_solutions.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.16 64-bit ('py38_pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "ff27035c8dc0a26468b79942def09685664b2e815f4bca7e9395dcaceb48c986"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
