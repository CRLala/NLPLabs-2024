{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc8hZvQ2u-Nb"
      },
      "source": [
        "# Lab 7: Part of Speech Tagging\n",
        "\n",
        "In this tutorial, we look at the structured prediction task of [Part Of Speech Tagging](https://en.wikipedia.org/wiki/Part-of-speech_tagging). This task consists of tagging/assigning the tokens in a sentence with Part of Speech (POS) tags/labels like Noun, Verb, Adjective, Adverb, Determiner, Pronoun, etcetera. \n",
        "\n",
        "We will,\n",
        "\n",
        "1.   Download and explore a POS tagged corpus (dataset).\n",
        "2.   Implement a couple of naive approaches.\n",
        "3.   Understand Hidden Markov Model\n",
        "4.   Understand Viterbi algorithm with an example.\n",
        "5.   Compute transition probabilities and emission probabilities.\n",
        "6.   Implement Viterbi algorithm.\n",
        "7.   (Optional) Implement a RNN-based Tagging model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FdWej774F1Cs"
      },
      "source": [
        "## The Corpus\n",
        "\n",
        "We will work with simple versions of the [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus) and [Penn Treebank Corpus](https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html) which can be accessed using [NLTK](https://www.nltk.org/). \n",
        "\n",
        "**Let's begin by downloading the corpuses**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGb_dEPlzKia"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "# Downloading the corpuses\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('brown')\n",
        "nltk.download('treebank')\n",
        "\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "brown_corpus = list(brown.tagged_sents(tagset='universal'))\n",
        "treebank_corpus = list(treebank.tagged_sents(tagset='universal'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20mksE2cB8PI"
      },
      "source": [
        "**Exploring the corpuses** \n",
        "\n",
        "The downloaded corpuses are a list of sentences. Each sentence is a list of tagged tokens. The tagged tokens are tuples of the form ```(token, tag)```. Each token and each tag is a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLu7_w7Uj_Wb"
      },
      "outputs": [],
      "source": [
        "print('Size of brown_corpus is', len(brown_corpus), 'sentences\\n')\n",
        "print('Size of treebank_corpus is', len(treebank_corpus), 'sentences\\n')\n",
        "print('A sample from the brown_corpus:\\n', brown_corpus[2])\n",
        "print('\\nA sample from the treebank_corpus:\\n', treebank_corpus[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxhJbLhgM4t4"
      },
      "source": [
        "Note: The tokens in the corpuses are not pre-processed and we will continue for now without pre-processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czVkO_--xNWW"
      },
      "source": [
        "**Exploring POS tags** \n",
        "\n",
        "**Q1:** How many unique POS tags, also called the tagset, do we have in these corpuses? Save the unique POS tags in a list called ```tagset```.\n",
        "\n",
        "**Q2:** Enumerate all the POS tags and its frequencies/counts. Save it in a python dictionary ```count_tags_in_brown``` for the brown corpus and a python dictionary ```count_tags_in_treebank``` for the treebank corpus. (Sanity check - ```count_tags_in_brown['NUM']``` should be ```14874```) \n",
        "\n",
        "**Q3:** Which POS tag occurs most frequently and which POS tag occurs least frequently in both the corpuses?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7KgaiN_yT6A"
      },
      "outputs": [],
      "source": [
        "import operator\n",
        "\n",
        "##########\n",
        "# For brown, computing count_tags_in_brown\n",
        "count_tags_in_brown = {}\n",
        "## ENTER CODE: Populate this dictionary in here\n",
        "\n",
        "##########\n",
        "\n",
        "# Tagset\n",
        "tagset = count_tags_in_brown.keys()\n",
        "\n",
        "##########\n",
        "# For treebank, computing count_tags_in_treebank\n",
        "count_tags_in_treebank = {}\n",
        "## ENTER CODE: Populate this dictionary in here\n",
        "\n",
        "##########\n",
        "\n",
        "##########\n",
        "## ENTER CODE: Find the most frequent and the least frequent POS Tags in here\n",
        "\n",
        "###########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0e2PCGl0aF7"
      },
      "source": [
        "**Splitting into Training and Test sets**\n",
        "\n",
        "We randomly sample 4000 sentences from the brown corpus as the ```test_set```. The rest of the brown corpus forms the ```train_set```. We will use the treebank corpus as the ```external_test_set```. \n",
        "\n",
        "**Note:** We could have tried other kinds of train-test splits too like 80-20 splits or mix the external test set with the brown test set, etcetera.\n",
        "\n",
        "Also, for convenience, we separate the tokens and the tags out from the tuples ```(token, tag)``` to create two separate sets of ```train_set_X``` and ```train_set_Y``` (X for tokens only, Y for POS tags only). We do the same with the ```test_set``` and the ```external_test_set```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6qbnouxkbOW"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# For consistency of results everytime you run this cell\n",
        "copy_of_brown = brown_corpus.copy() \n",
        "random.seed(20)                \n",
        "\n",
        "# Splitting into train and test and external_test sets\n",
        "random.shuffle(copy_of_brown)\n",
        "train_set = copy_of_brown[4000:]\n",
        "test_set = copy_of_brown[:4000]  # 4000 samples in the test set\n",
        "external_test_set = treebank_corpus\n",
        "\n",
        "# Separating tokens and tags\n",
        "def _split_into_tokens_and_tags(given_set):\n",
        "  \"\"\" Separating tokens and tags out from a list of list of (token, tag) tuples\n",
        "\n",
        "  Arguments:\n",
        "    given_set = a list of list of (token, tag) tuples. token and tag are strings\n",
        "\n",
        "  Returns:\n",
        "    token_set = a list of list of tokens only. order and shape is maintained.  \n",
        "    tag_set = a list of list of tags only. order and shape is maintained.\n",
        "  \"\"\"\n",
        "  token_set = given_set.copy() \n",
        "  tag_set = given_set.copy() \n",
        "\n",
        "  for i in range(len(given_set)):\n",
        "    (token_only, tag_only) = zip(*given_set[i])\n",
        "    token_set[i] = list(token_only)  # 0th element of tuple is token \n",
        "    tag_set[i] = list(tag_only)      # 1st element of tuple is POS tag\n",
        "  \n",
        "  return (token_set, tag_set)\n",
        "\n",
        "# Creating x (sentences) and Y (tags) splits\n",
        "(train_set_x, train_set_Y) = _split_into_tokens_and_tags(train_set)\n",
        "(test_set_x, test_set_Y) = _split_into_tokens_and_tags(test_set)\n",
        "(external_test_set_x, external_test_set_Y) \\\n",
        "= _split_into_tokens_and_tags(external_test_set)\n",
        "\n",
        "# Example of how *_x and *_Y look like\n",
        "# * could be train_set or test_set or external_test_set\n",
        "print(test_set_x) # List of sentences where each sentence is a list of tokens\n",
        "print(test_set_Y) # List of list of tags of the same shape as test_set_x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r5mKQmJb-3hq"
      },
      "source": [
        "## Naive approaches to POS tagging\n",
        "\n",
        "One naive approach could be to tag all words/tokens with the most frequent POS tag found in the entire training set. We call this the ```_most_freq_tag_model_1```. In other words, if ```'NUM'``` is the most frequent POS tag in the training set then we tag every word/token in the test set to ```'NUM'```.  \n",
        "\n",
        "A better approach could be to tag each word/token to its most frequent POS tag as seen in the training set. We call this the ```_most_freq_tag_model_2```. In other words, for a word like ```'the'```, its most frequent POS tag as seen in the training set is ```'DET'```, so the model will tag ```'the'``` to ```'DET'``` whenever it sees it in the test set.\n",
        "\n",
        "In the following exercises we will build models for these approaches and also build a function to evaluate them.\n",
        "\n",
        "**We begin with approach 1: _most_freq_tag_model_1**\n",
        "\n",
        "In Q3, we had found the most frequent POS tag across the entire brown corpus (which included both the train and test sets).\n",
        "\n",
        "**Q4:** Check if the most frequent POS tag found in Q3 remains the most frequent POS tag in the ```train_set``` as well. We do this to ensure our naive approach is not informed by the ```test_set```. Save this tag to a variable called ```most_freq_tag```. Also, similar to Q2, save all the counts of tags in a python dictionary called ```count_tag```. Sanity check: ```count_tag['VERB']``` should be 170196)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B10kJHPD6RzY"
      },
      "outputs": [],
      "source": [
        "##########\n",
        "count_tag = {}\n",
        "##ENTER CODE: Populate this dictionary in here\n",
        "\n",
        "##########\n",
        "\n",
        "sorted_count_tag = sorted(count_tag.items(), key=operator.itemgetter(1))\n",
        "\n",
        "most_freq_tag = sorted_count_tag[-1][0]\n",
        "print('The most frequent tag is', most_freq_tag)\n",
        "print(count_tag['VERB'])  # Sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD-PU6b9jOEE"
      },
      "outputs": [],
      "source": [
        "sorted_count_tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA4UwwcdV2xq"
      },
      "source": [
        "**Q5:** Implement a naive model as a function ```_most_freq_tag_model_1(x)``` which tags every token in a given test set ```x``` with the most frequent POS tag found in Q4. Ensure the output of this function is of the same shape and size as the input so that it is easy to evaluate it later on. For example, the call ```_most_freq_tag_model_1(test_set_x)``` should return a list of tuples of most frequent POS tag of the same size and shape as ```test_set_Y```. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW4p_ZP_VuFE"
      },
      "outputs": [],
      "source": [
        "def _most_freq_tag_model_1(x):\n",
        "  \"\"\"Tags every token with the most frequent POS tag\"\"\"\n",
        "  ## ENTER CODE HERE\n",
        "  return Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSrD17ksbOR8"
      },
      "source": [
        "**Q6:** Write a function ```_evaluate``` to evaluate the naive approach by measuring the accuracy of its predictions. Very simply, the function will take two list of lists of tags (of same shape) as arguments and count the number of times the tags are the same and divide this number with total number of tags. Sanity check: When you evaluate the naive approach ```_evaluate(_most_freq_tag_model_1(test_set_x), test_set_Y)``` you should get 23.8% accuracy. What is the accuracy of this model on the external test set?\n",
        "\n",
        "**Q6.1 (Optional):** You can extend the evaluation function to compute *precision* and *recall* and *F-measure* for each POS tag as a way to practice and understand these concepts. What is the precision, recall and F-measure for ```'NOUN'``` and any other tag?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TezC3duha6OS"
      },
      "outputs": [],
      "source": [
        "##########\n",
        "def _evaluate(predicted_Y, true_Y):\n",
        "  correct_tags = 0\n",
        "  total_tags = 0\n",
        "  ## ENTER CODE IN HERE TO UPDATE THE ABOVE VARIABLES\n",
        "  \n",
        "  Accuracy = float(correct_tags) / total_tags\n",
        "  print('Accuracy =', Accuracy)\n",
        "##########\n",
        "\n",
        "_evaluate(_most_freq_tag_model_1(test_set_x), test_set_Y) # Sanity check\n",
        "\n",
        "print('\\nExternal test set:')\n",
        "_evaluate(_most_freq_tag_model_1(external_test_set_x), external_test_set_Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXeuSAP-l8BD"
      },
      "source": [
        "**Now we build a model for approach 2: _most_freq_tag_model_2**\n",
        "\n",
        "For each word/token in the vocabulary, we need to retrieve the most frequent POS tag of that word as seen in the training set. Hence, we need to keep a count of every unique (token, tag) pair in the training set. We can do this using dictionaries in python. This will also prove to be useful later on in the exercise.\n",
        "\n",
        "**Q7:** Create a dictionary ```count_token_tag``` that stores the counts of every unique (token, tag) pair as seen in the ```train_set```. Sanity check: ```count_token_tag['the']['DET']``` should be 58337 and ```count_token_tag['the']['VERB']``` should return an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db6-XG5MkHJc"
      },
      "outputs": [],
      "source": [
        "##########\n",
        "count_token_tag = {}\n",
        "for i in range(len(train_set)):\n",
        "  for j in range(len(train_set[i])):\n",
        "    ## ENTER CODE IN HERE\n",
        "    \n",
        "##########\n",
        "\n",
        "# Sanity check\n",
        "print(count_token_tag['the']['DET'])\n",
        "print(count_token_tag['the']['VERB'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYESHIXy5buL"
      },
      "source": [
        "**Q8:** Create a function ```_most_freq_tag``` which retrieves the most frequent POS tag for a given token as seen in the ```count_token_tag```. Sanity check: ```_most_freq_tag('the')``` is ```'DET'``` and ```_most_freq_tag('play')``` is ```'VERB'```. What about unseen words? What about ```_most_freq_tag('onomatopoeia')```? What can we do about it? One simple solution is to tag it with the ```most_freq_tag``` found in Q4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJdcCQgu4yvh"
      },
      "outputs": [],
      "source": [
        "def _most_freq_tag(_token):\n",
        "  \"\"\"Returns the most frequent POS tag (mfPOSt) of a _token as seen in count_token_tag\"\"\"\n",
        "  if _token in count_token_tag.keys():\n",
        "    ## ENTER CODE HERE\n",
        "\n",
        "    return mfPOSt\n",
        "  else:\n",
        "    return most_freq_tag  # this is a global constant. See Q4. Can you see why we did it this way?\n",
        "\n",
        "print(_most_freq_tag('the'))\n",
        "print(_most_freq_tag('play'))\n",
        "print(_most_freq_tag('onomatopoeia'))  # for unseen words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uQ71dYK9p1p"
      },
      "source": [
        "**Q9:** Implement a naive model as a function ```_most_freq_tag_model_2(x)``` which tags every token in a test set ```x``` with its most frequent POS tag found in Q8. Ensure the output is of the same shape as ```x``` so that it is easy to evaluate. Evaluate the output of this model. Sanity check: ```_evaluate(_most_freq_tag_model_2(test_set_x), test_set_Y)``` should be 94.795%. Evaluate on the external test set too. Does the accuracy differ? Can you explain why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp1xTvmy9NuO"
      },
      "outputs": [],
      "source": [
        "def _most_freq_tag_model_2(x):\n",
        "  \"\"\"Tags every token with its most frequent POS tag\"\"\"\n",
        "  ## ENTER CODE HERE: use _most_freq_tag() from Q8\n",
        "\n",
        "  return Y\n",
        "\n",
        "_evaluate(_most_freq_tag_model_2(test_set_x), test_set_Y)\n",
        "_evaluate(_most_freq_tag_model_2(external_test_set_x), external_test_set_Y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgivqc8PC7gW"
      },
      "source": [
        "## Hidden Markov Model (HMM)\n",
        "\n",
        "Let $W = (w_1, w_2, ..., w_n)$ be a sequence of observed words/tokens.\n",
        "\n",
        "Let $T = (t_1, t_2, ..., t_n)$ be a sequence of tags and $\\mathbb{T}_n$ be the set of all possible such sequences of tags of length $n$.\n",
        "\n",
        "**Q10:** How many such sequences of tags exist? In other words, whats the size of the set $\\mathbb{T}_n$?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uV87-ori8ls"
      },
      "outputs": [],
      "source": [
        "## ENTER YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCgWpRE8jQMK"
      },
      "source": [
        "Our aim is to find the most probable sequence of tags $\\tilde{T}$ for the observed sequence of words $W$ such that the probability $P(\\tilde{T}|W) \\geq P(T|W)$ for all possible $T \\in \\mathbb{T}_n$.\n",
        "\n",
        "In other words, $\\tilde{T} = \\underset{T \\in \\mathbb{T}_n}{\\operatorname{argmax}}P(T|W)$\n",
        "\n",
        "Now, to estimate $P(T|W)$ we use the [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) followed by the [Chain Rule](https://en.wikipedia.org/wiki/Chain_rule_(probability)) followed by the Markov Assumption ($P(t_m|t_1, t_2, ..., t_{m-1}) \\approx P(t_m|t_{m-1})$) to get the following formulation for the HMM: \n",
        "\n",
        "$\\tilde{T} = \\underset{(t_1, t_2, ..., t_n) \\in \\mathbb{T}_n}{\\operatorname{argmax}}P(t_1)P(w_1|t_1)P(t_2|t_1)P(w_2|t_2)...P(t_{m}|t_{m-1})P(w_{m}|t_{m})...P(t_n|t_{n-1})P(w_n|t_n)$\n",
        "\n",
        "**Q11:** How do you interpret $P(t_m|t_{m-1})$ and $P(w_m|t_m)$ (frequentist interpretation)?. How will you estimate these probabilities from the ```train_set```? How do you interpret $P(t_1)$ and estimate it from the ```train_set```?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqDA4FhysJZ-"
      },
      "outputs": [],
      "source": [
        "## ENTER YOUR ANSWER HERE"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TCxu56HN03GU"
      },
      "source": [
        "## Preliminaries: Viterbi algorithm (manual pen-paper example)\n",
        "Recall from Q10, the number of possible tag sequences (size of $\\mathbb{T}_n$) is enormous, especially for larger n and larger POS tagsets, which makes the brute force algorithm extremely inefficient. To find the solution $\\tilde{T}$ quickly, we use a dynamic programming algorithm called [Viterbi](https://en.wikipedia.org/wiki/Viterbi_algorithm).\n",
        "\n",
        "In this section, we will use the Viterbi algorithm to tag the following sentence manually:\n",
        "\n",
        "***time flies like an arrow***\n",
        "\n",
        "We assume some pre-computed transition probabilities and emission probabilities as follows:\n",
        "\n",
        "Initial probabilities:\n",
        "*   $P(NOUN|start) = 0.5$\n",
        "*   $P(DET|start) = 0.4$\n",
        "*   $P(VERB|start) = 0.1$\n",
        "\n",
        "Transition probabilities:\n",
        "*   $P(NOUN|DET) = 1.0$\n",
        "*   $P(NOUN|NOUN) = 0.2$\n",
        "*   $P(VERB|NOUN) = 0.7$\n",
        "*   $P(PREP|NOUN) = 0.1$\n",
        "*   $P(DET|VERB) = 0.4$\n",
        "*   $P(NOUN|VERB) = 0.4$\n",
        "*   $P(PREP|VERB) = 0.1$\n",
        "*   $P(VERB|VERB) = 0.1$\n",
        "*   $P(DET|PREP) = 0.6$\n",
        "*   $P(NOUN|PREP) = 0.4$\n",
        "\n",
        "Emission probabilities:\n",
        "*   $P(time|NOUN) = 0.7$\n",
        "*   $P(time|VERB) = 0.1$\n",
        "*   $P(flies|NOUN) = 0.4$\n",
        "*   $P(flies|VERB) = 0.4$\n",
        "*   $P(like|VERB) = 0.1$\n",
        "*   $P(like|PREP) = 0.3$\n",
        "*   $P(an|DET) = 0.6$\n",
        "*   $P(arrow|NOUN) = 0.4$\n",
        "\n",
        "Next, we draw two tables called **viterbi** and **backpointer** of size (4, 5). (There are 4 unique POS tags in this tagset and there are 5 words in the observed sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8zEApuwFAng"
      },
      "source": [
        "**viterbi:**\n",
        "\n",
        "Tag\\Sent | ***time*** | ***flies*** | ***like*** | ***an*** | ***arrow***\n",
        "--- | --- | --- | --- | --- | ---\n",
        "DET | | | | | \n",
        "NOUN | | | | | \n",
        "VERB | | | | | \n",
        "PREP | | | | | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5OFep45Gvol"
      },
      "source": [
        "**backpointer:**\n",
        "\n",
        "Tag\\Sent | ***time*** | ***flies*** | ***like*** | ***an*** | ***arrow***\n",
        "--- | --- | --- | --- | --- | ---\n",
        "DET | | | | | \n",
        "NOUN | | | | | \n",
        "VERB | | | | | \n",
        "PREP | | | | | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNHPDwxgHGev"
      },
      "source": [
        "A cell, say corresponding to (***like***, VERB), in the ***viterbi*** table stores the maximum probability of all the sequences of tags until that point from left to right. For example,\n",
        "\n",
        "**viterbi**(***like***, VERB) $= \\underset{(t_1, t_2, \\text{VERB}) \\in \\mathbb{T}_n}{\\operatorname{max}}P(t_1)P($***time***$|t_1)P(t_2|t_1)P($***flies***$|t_2)P(VERB|t_2)P(like|VERB)$\n",
        "\n",
        "The same cell in the **backpointer** table stores the tag of the previous word from which the maximum viterbi score was arrived at. For example, if\n",
        "\n",
        "($\\tilde{t_1}$, $\\tilde{t_2}$, VERB) $= \\underset{(t_1, t_2, \\text{VERB}) \\in \\mathbb{T}_n}{\\operatorname{argmax}}P(t_1)P($***time***$|t_1)P(t_2|t_1)P($***flies***$|t_2)P(VERB|t_2)P(like|VERB)$\n",
        "\n",
        "then, **backpointer**(***like***, VERB) = $\\tilde{t_2}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTUUGYCCUFmV"
      },
      "source": [
        "**Q12: Let's begin by filling the first column of these tables.**\n",
        "\n",
        "As an example, **viterbi**(***time***, NOUN) = $\\underset{(\\text{NOUN}) \\in \\mathbb{T}_1}{\\operatorname{max}}P(NOUN)P($***time***$|NOUN) =  P(NOUN|start)P($***time***$|NOUN) = 0.5 \\times 0.7$\n",
        "\n",
        "What is **viterbi**(***time***, VERB)?\n",
        "What is the first column of the **backpointer** table? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKg_V6rZUd_z"
      },
      "source": [
        "**viterbi:**\n",
        "\n",
        "Tag\\Sent | ***time*** | ***flies*** | ***like*** | ***an*** | ***arrow***\n",
        "--- | --- | --- | --- | --- | ---\n",
        "DET | ? | | | | \n",
        "NOUN |0.35 | | | | \n",
        "VERB | ? | | | | \n",
        "PREP | ? | | | | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_Gs06yeUcbt"
      },
      "source": [
        "**backpointer:**\n",
        "\n",
        "Tag\\Sent | ***time*** | ***flies*** | ***like*** | ***an*** | ***arrow***\n",
        "--- | --- | --- | --- | --- | ---\n",
        "DET | - | | | | \n",
        "NOUN | start | | | | \n",
        "VERB | start | | | | \n",
        "PREP | - | | | | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpTqLDliVsvN"
      },
      "source": [
        "**Now we fill the second column.**\n",
        "As an example, \n",
        "\n",
        "**viterbi**(***flies***, NOUN) = $\\underset{(t_1, \\text{NOUN}) \\in \\mathbb{T}_1}{\\operatorname{max}}P(t_1)P($***time***$|t_1)P(NOUN|t_1)P($***flies***$|NOUN) = \\underset{(t_1, \\text{NOUN}) \\in \\mathbb{T}_1}{\\operatorname{max}}$**viterbi**(***time***, $t_1$)$P(NOUN|t_1)P($***flies***$|NOUN)$\n",
        "\n",
        "So all you need is the first column of the viterbi table and transition probabilites and the emission probabilities to compute the entries of the second column.\n",
        "\n",
        "**viterbi**(***time***, NOUN)$P(NOUN|NOUN)P($***flies***$|NOUN) = 0.35 \\times 0.2 \\times 0.4 = 0.028$\n",
        "\n",
        "**Q13:** Compute **viterbi**(***time***, VERB)$P(NOUN|VERB)P($***flies***$|NOUN)$. Is this greater than $0.028$? If yes, then enter the new computed value in the cell **viterbi**(***flies***, NOUN) and enter VERB in **backpointer**(***flies***, NOUN). If no, then enter $0.028$ in **viterbi**(***flies***, NOUN) and enter NOUN in **backpointer**(***flies***, NOUN). Do this for every cell in the second column.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JD7eSPEfyGT"
      },
      "source": [
        "**viterbi:**\n",
        "\n",
        "Tag\\Sent | ***time*** | ***flies*** | ***like*** | ***an*** | ***arrow***\n",
        "--- | --- | --- | --- | --- | ---\n",
        "DET | ? | | | | \n",
        "NOUN |0.35 | 0.028 or ? | | | \n",
        "VERB | ? | | | | \n",
        "PREP | ? | | | | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx2ISuidgIMI"
      },
      "source": [
        "**backpointer:**\n",
        "\n",
        "Tag\\Sent | ***time*** | ***flies*** | ***like*** | ***an*** | ***arrow***\n",
        "--- | --- | --- | --- | --- | ---\n",
        "DET | - | | | | \n",
        "NOUN | start | NOUN or VERB?| | | \n",
        "VERB | start | | | | \n",
        "PREP | - | | | | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkNRtexcgZTs"
      },
      "source": [
        "**Q14:** Now fill the third column using entries in the second column and the transition probabilities and emission probabilites provided.\n",
        "\n",
        "For your reference, the formulation is as follows:\n",
        "\n",
        "**viterbi**(***like***, VERB) = $ \\underset{(t_1, t_2, \\text{VERB}) \\in \\mathbb{T}_1}{\\operatorname{max}}$**viterbi**(***flies***$, t_2)P(VERB|t_2)P($***like***$|NOUN)$\n",
        "\n",
        "And the **backpointer**(***like***, VERB) will be that tag $t_2$ which resulted in the maximum viterbi score of **viterbi**(***like***, VERB)\n",
        "\n",
        "**Q15:** Column by column fill the entire **viterbi** and **backpointer** tables below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZKPlfJginWp"
      },
      "source": [
        "**viterbi:**\n",
        "\n",
        "Tag\\Sent | ***time*** | ***flies*** | ***like*** | ***an*** | ***arrow***\n",
        "--- | --- | --- | --- | --- | ---\n",
        "DET | ? | ?| ? | ? | ? \n",
        "NOUN | ? | ? | ? | ? | ?\n",
        "VERB | ? | ? | ? | ? | ?\n",
        "PREP | ? | ? | ? | ? | ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bMnSAIfjANw"
      },
      "source": [
        "**backpointer:**\n",
        "\n",
        "Tag\\Sent | ***time*** | ***flies*** | ***like*** | ***an*** | ***arrow***\n",
        "--- | --- | --- | --- | --- | ---\n",
        "DET | ? | ? | ?| ?| ?\n",
        "NOUN | ? | ?| ?| ?| ?\n",
        "VERB | ? | ?| ?| ?| ?\n",
        "PREP | ? | ?| ?| ?| ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk5W1x3ajR0J"
      },
      "source": [
        "**Q16:** In the last column, **viterbi**(***arrow***, tag), find the tag $\\tilde{t}$ which has the greatest value. Select the corresponding cell in the other table **backpointer**(***arrow***, $\\tilde{t}$) and trace back the path to get the solution. What is the solution $\\tilde{T}$ sequence of tags according to the provided HMM model? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8YX5cAOlCrz"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M9fIRnp0raw"
      },
      "source": [
        "**Optional:** What is the time-complexity of this Viterbi algorithm in terms of length of observed sentence and size of the tagset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi8JDUFK1PsT"
      },
      "outputs": [],
      "source": [
        "# ENTER YOUR ANSWER HERE"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7UU2XfLbDFQe"
      },
      "source": [
        "## Computing transition probabilities and emission probabilities \n",
        "\n",
        "We will now compute the following from the training set,\n",
        "\n",
        "1.   Initial state probabilities ```p_initial```\n",
        "2.   Last state probabilities ```p_last``` (optional)\n",
        "3.   State transition probabilities ```p_transition```\n",
        "4.   Emission probabilities ```p_emission```\n",
        "\n",
        "Recall from Q11, how can we estimate these probabilities using counts from the training set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7PV93uuFq0P"
      },
      "source": [
        "**Q17:** We will first compute the initial state probabilities for all tags and store it in ```p_initial``` dictionary. In other words, the probability of the first state/POS_tag is a ```'NOUN'``` is stored in ```p_initial['NOUN']```. The simple formula we use is:\n",
        "\n",
        "$$P(NOUN|start) = \\frac{count(\\text{NOUN in first position in training set })}{\\text{size of training set}}$$\n",
        "\n",
        "What is the most probable and the least probable POS Tag of the first/initial state? (Sanity check: ```p_initial['PRON']``` should be 15.9%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLXC_2aXAVxw"
      },
      "outputs": [],
      "source": [
        "##########\n",
        "p_initial = {}\n",
        "## ENTER CODE: populate the above dictionary\n",
        "\n",
        "##########\n",
        "\n",
        "sorted_p_initial = sorted(p_initial.items(), key=operator.itemgetter(1))\n",
        "\n",
        "print(p_initial['PRON'])  # Sanity check\n",
        "print('Initial probabilities are', p_initial)\n",
        "print('The most probable tag is', sorted_p_initial[-1])\n",
        "print('The least probable tag is', sorted_p_initial[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8tQQO45K47k"
      },
      "source": [
        "**Q18 (optional):** Similarly, we can also compute the last state probabilities and store it in ```p_last``` dictionary. In other words, the probability of the last state/POS_tag is a ```'NOUN'``` is stored in ```p_last['NOUN']```.\n",
        "\n",
        "What is the most probable and the least probable POS tag of the last state? (Sanity check: ```p_initial['PRON']``` should be $9.3738 \\times 10^{-5}$)\n",
        "\n",
        "**Note:** In the current HMM formulation, we don't require ```p_last```. However, we could use it in another HMM formulation where it is an additional term. The same viterbi algorithm could be applied to find the tags of an observed sequence of words/tokens in that case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3FivzRaJxUn"
      },
      "outputs": [],
      "source": [
        "##########\n",
        "p_last = {}\n",
        "## ENTER CODE: populate the above dictionary\n",
        "\n",
        "##########\n",
        "\n",
        "sorted_p_last = sorted(p_last.items(), key=operator.itemgetter(1))\n",
        "\n",
        "print(p_last['PRON'])  # Sanity check\n",
        "print('Last state probabilities are', p_last)\n",
        "print('The most probable tag is', sorted_p_last[-1])\n",
        "print('The least probable tag is', sorted_p_last[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvEKqbuxcf-W"
      },
      "source": [
        "**Q19:** To compute state transition probabilities, we first need to count the transitions and store it in a ```count_tag_tag``` dictionary of dictionaries. In other words, how often do we see a transition from ```NOUN``` to ```VERB``` in the training set will be stored in ```count_tag_tag['VERB']['NOUN']```. ***Should be read as count of Verb given Noun.***\n",
        "\n",
        "Count the transition counts in ```count_tag_tag``` dictionary of dictionaries. (Sanity check - ```count_tag_tag['VERB']['NOUN']``` should be 40760 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_5fpnI1M66N"
      },
      "outputs": [],
      "source": [
        "##########\n",
        "count_tag_tag = {}\n",
        "## ENTER CODE HERE\n",
        "\n",
        "##########\n",
        "\n",
        "print(count_tag_tag['VERB']['NOUN']) # Sanity check: count of transition from NOUN to VERB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJbqYgBKkc4i"
      },
      "source": [
        "Now we have all the ingredients we need to compute the state transition probabilities ```p_transition``` and the emission probabilities ```p_emission```.\n",
        "\n",
        "As an example,\n",
        "\n",
        "```p_transition['VERB']['NOUN'] = count_tag_tag['VERB']['NOUN'] / count_tag['NOUN']``` \n",
        "\n",
        "Should be read as probability of Verb given Noun, i.e. $P(VERB|NOUN)$\n",
        "\n",
        "**Q20:** Compute the state transition probabilities and store in ```p_transition``` dictionary of dictionaries which is of the same shape as ```count_tag_tag``` dictionary of dictionaries. (Sanity check - ```p_transition['VERB']['NOUN']``` should be 0.1588)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vy2KNaxhVkL"
      },
      "outputs": [],
      "source": [
        "p_transition = {}\n",
        "for end_tag in count_tag_tag.keys():\n",
        "  for start_tag in count_tag_tag[end_tag].keys():\n",
        "    numerator = ## ENTER HERE\n",
        "    denominator = ## ENTER HERE\n",
        "    probability = float(numerator) / denominator\n",
        "    if end_tag not in p_transition.keys():\n",
        "      p_transition[end_tag] = {start_tag: probability}\n",
        "    else:\n",
        "      p_transition[end_tag][start_tag] = probability\n",
        "\n",
        "print(p_transition['VERB']['NOUN'])\n",
        "print(count_tag_tag['VERB']['NOUN'])\n",
        "print(count_tag['VERB'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jR8InrhsRim"
      },
      "source": [
        "**Q21:** Similarly, compute the emission probabilities and store in ```p_emission``` dictionary of dictionaries which is of the same shape as ```count_token_tag``` from Q7. (Sanity check - ```p_emission['the']['DET']```, which is probability of 'the' given DET, should be 0.4574)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YzrpCwunpn5"
      },
      "outputs": [],
      "source": [
        "p_emission = {}\n",
        "for _token in count_token_tag.keys():\n",
        "  for _tag in count_token_tag[_token].keys():\n",
        "    numerator = ## ENTER HERE\n",
        "    denominator = ## ENTER HERE\n",
        "    probability = float(numerator) / denominator\n",
        "    if _token not in p_emission.keys():\n",
        "      p_emission[_token] = {_tag: probability}\n",
        "    else:\n",
        "      p_emission[_token][_tag] = probability\n",
        "\n",
        "print(p_emission['the']['DET'])\n",
        "print(count_token_tag['the']['DET'])\n",
        "print(count_tag['DET'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M8_xFWak0Zwf"
      },
      "source": [
        "## Viterbi algorithm\n",
        "\n",
        "Pseudo-code examples of the [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) are available in the lecture slides and online.\n",
        "\n",
        "We will create a function ```_HMMtagger(sentence)``` which takes an input sequence/tuple of words/tokens ```sentence``` and returns an output sequence/tuple of POS tags of the same size. This output is the most probable tag sequence of the input sentence according to our HMM model. We will implement the Viterbi algorithm to find the most probable tag sequence and use the global variable python dictionaries ```p_initial```, ```p_transition```, and ```p_emission``` from the previous section 5 to do so.\n",
        "\n",
        "**Q22:** Complete the code below at specified places. Look out for ```## ENTER YOUR ANSWER HERE```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdVKXWIztzRp"
      },
      "outputs": [],
      "source": [
        "import numpy as np # for logarithms\n",
        "\n",
        "def _HMMtagger(sentence):\n",
        "  \"\"\"Returns the most probable tag sequece of a sentence as per HMM probabilites\"\"\"\n",
        "  # Creating viterbi and backpointer matrices. \n",
        "  # For simplicity, we create a list of dictionaries. Dictionaries appended later.  \n",
        "  viterbi = [] \n",
        "  backpointer = []\n",
        "\n",
        "  # Initialize \n",
        "  # Filling the first column which is the the first dictionary in the above lists\n",
        "  viterbi.append({})\n",
        "  backpointer.append({})\n",
        "  first_word = sentence[0]\n",
        "  if first_word in p_emission.keys():\n",
        "    for tag in p_emission[first_word].keys():\n",
        "      if tag in p_initial.keys():\n",
        "        viterbi[0][tag] = np.log(p_initial[tag]) + np.log(p_emission[first_word][tag])  \n",
        "        # we work with log to handle small numbers. multiplication becomes addition in log scale.\n",
        "        backpointer[0][tag] = '<s>'\n",
        "  else:\n",
        "    # If first_word not in p_emission.keys() then its because its an unseen word.\n",
        "    # For unseen words we assume it is equally likely to be emitted by any tag.\n",
        "    # We could have included an UNKNOWN token in the training set \n",
        "    # but that is suppose to be done during pre-processing step.  \n",
        "    # Hence, for now, we will ignore p_emission[first_word][tag] \n",
        "    # when computing viterbi[0][tag]\n",
        "    for tag in p_initial.keys():\n",
        "      viterbi[0][tag] = ## ENTER YOUR ANSWER HERE  \n",
        "      backpointer[0][tag] = '<s>' \n",
        "  \n",
        "  # Filling other columns\n",
        "  for i in range(len(sentence)-1):\n",
        "    time_step = i+1\n",
        "    viterbi.append({})\n",
        "    backpointer.append({})\n",
        "    word = sentence[time_step]\n",
        "    if word in p_emission.keys():\n",
        "      for tag in p_emission[word].keys():\n",
        "        viterbi[time_step][tag] = float(\"-inf\")    # to be updated in the following loop\n",
        "        backpointer[time_step][tag] = 'placeholder'  # to be updated in following loop\n",
        "        for previous_tag in viterbi[time_step-1].keys():\n",
        "          if previous_tag in p_transition[tag].keys():\n",
        "            new_viterbi = ## ENTER YOUR ANSWER HERE\n",
        "            if new_viterbi > viterbi[time_step][tag]:\n",
        "              viterbi[time_step][tag] = new_viterbi\n",
        "              backpointer[time_step][tag] = ## ENTER YOUR ANSWER HERE\n",
        "    else:\n",
        "      # Just like before, the word is not seen in the training set\n",
        "      # So we assume it is equally likely to be emitted by any tag\n",
        "      for tag in tagset:\n",
        "        # tagset from Q1\n",
        "        viterbi[time_step][tag] = float(\"-inf\")    # to be updated in the following loop\n",
        "        backpointer[time_step][tag] = 'placeholder'  # to be updated in following loop\n",
        "        for previous_tag in viterbi[time_step-1].keys():\n",
        "          if previous_tag in p_transition[tag].keys():\n",
        "            new_viterbi = ## ENTER YOUR ANSWER HERE. ignore p_emission[word][tag]\n",
        "            if new_viterbi > viterbi[time_step][tag]:\n",
        "              viterbi[time_step][tag] = new_viterbi\n",
        "              backpointer[time_step][tag] = ## ENTER YOUR ANSWER HERE\n",
        "  \n",
        "  # Retracing the most probable path and returning it\n",
        "  solution = []\n",
        "  final_time_step = len(sentence)-1\n",
        "  final_probability = float(\"-inf\")  # to be updated in the following loop\n",
        "  final_tag = 'placeholder'  # to be updated in the following loop\n",
        "  for tag in viterbi[final_time_step].keys():\n",
        "    if viterbi[final_time_step][tag] > final_probability:\n",
        "      final_probability = viterbi[final_time_step][tag]\n",
        "      final_tag = tag\n",
        "  \n",
        "  while final_tag != '<s>':\n",
        "    solution.append(final_tag)\n",
        "    final_tag = ## ENTER YOUR ANSWER HERE \n",
        "    final_time_step = final_time_step - 1\n",
        "  \n",
        "  solution.reverse()\n",
        "\n",
        "  return solution\n",
        "\n",
        "print(_HMMtagger(['This', 'is', 'just', 'an', 'example', 'to', 'check', 'if', 'the', 'tagger', 'works', 'okay', 'or', 'not']), 'Sanity check')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cfvpto7QoWn"
      },
      "source": [
        "**Q23:** Evaluate the HMM tagger against ```test_set``` and ```external_test_set```. How does it compare to the baseline naive models? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4bIo0x6Rjcq"
      },
      "outputs": [],
      "source": [
        "def _HMM(x):\n",
        "  \"\"\"Tags every sentence in x with the _HMMtagger\"\"\"\n",
        "  Y = x.copy()\n",
        "  for i in range(len(x)):\n",
        "    Y[i] = _HMMtagger(x[i])\n",
        "  return Y\n",
        "\n",
        "_evaluate(_HMM(test_set_x), test_set_Y)\n",
        "_evaluate(_HMM(external_test_set_x), external_test_set_Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** \n",
        "\n",
        "To solve the previous pen-and-paper example with the above Viterbi algorithm code:\n",
        "\n",
        "*   change the function definition to: ```def _HMMtagger(sentence, p_initial, p_transition, p_emission):```, or change the previous ```p_initial, p_transition, p_emission``` to the ones used in the pen and paper exercise.\n",
        "\n",
        "*   initialise probability tables as follows and run the ```_HMMtagger``` function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # initialise probability tables \n",
        "tmp_initial = {'NOUN': 0.5, 'DET': 0.4, 'VERB': 0.1}\n",
        "tmp_transition = {'NOUN':{'DET': 1.0, 'NOUN': 0.2, 'VERB': 0.4, 'PREP': 0.4}, 'VERB': {'NOUN': 0.7, 'VERB': 0.1}, 'PREP': {'NOUN': 0.1, 'VERB': 0.1}, 'DET': {'VERB': 0.4, 'PREP': 0.6}}\n",
        "tmp_emission = {'time':{'NOUN': 0.7, 'VERB': 0.1}, 'flies':{'NOUN': 0.4, 'VERB': 0.4}, 'like':{'VERB': 0.1, 'PREP': 0.3}, 'an':{'DET': 0.6}, 'arrow':{'NOUN': 0.4}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p_initial, p_transition, p_emission = tmp_initial, tmp_transition, tmp_emission\n",
        "print(_HMMtagger(['time', 'flies', 'like', 'an', 'arrow']))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-rMdBb9vzo-W"
      },
      "source": [
        "## RNN-based Tagging model (optional)\n",
        "\n",
        "In previous lab sessions we explored a simple RNN-based Language Model which could also be trained to maximize the likelihood of the next word given the history of previous words in the sentence. Pictorially, the model looks as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiwL2p9R1KqS"
      },
      "source": [
        "![RNNLM](https://raw.githubusercontent.com/torch/torch.github.io/master/blog/_posts/images/rnnlm.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4vokrZX1gqw"
      },
      "source": [
        "**Q24:** If we replace the target words (see the above picture) with the POS tags and then train a RNN model then we get an RNN-based POS Tagger. Can you implement such a tagger? How much does the accuracy change?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Lab07: POStagging",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.16 64-bit ('py38_pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "ff27035c8dc0a26468b79942def09685664b2e815f4bca7e9395dcaceb48c986"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
