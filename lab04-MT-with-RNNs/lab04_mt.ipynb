{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJMQLZ_uf6tI"
   },
   "source": [
    "# Lab 4: Machine Translation with Recurrent Neural Networks\n",
    "\n",
    "In this lab session, you will be improving your practical understanding of neural machine translation (NMT). For this, you will experiment with state-of-the-art recurrent neural machine translation (NMT) models. For training, you will be using the [Multi30k](https://github.com/multi30k/dataset) dataset, which contains around 30,000 image descriptions in English and their translations in German, French and Czech. Although Multi30k is often used for multimodal machine translation, it is a good candidate for quick MT experiments due to its smaller size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkbmDTO_SAUh"
   },
   "source": [
    "## 1. Downloading the corpus\n",
    "\n",
    "Let's start by downloading the corpus. We'll also install `sacreBLEU` for BLEU computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eLd2J2B1i2u"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# install sacreBLEU\n",
    "pip install sacrebleu==1.5.0\n",
    "echo\n",
    "\n",
    "# Download the corpus\n",
    "URL=\"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok\"\n",
    "\n",
    "for split in \"train\" \"val\" \"test_2016_flickr\"; do\n",
    "  for lang in en de fr; do\n",
    "    fname=\"${split}.lc.norm.tok.${lang}\"\n",
    "    if [ ! -f $fname ]; then\n",
    "      echo \"Downloading $fname\"\n",
    "      wget -q \"${URL}/$fname\" -O \"${split/_2016_flickr/}.${lang}\"\n",
    "    fi\n",
    "  done\n",
    "done\n",
    "echo \n",
    "\n",
    "# Print the first 10 lines with line numbers of \n",
    "# the English and French training data\n",
    "cat -n train.en | head -n10\n",
    "echo\n",
    "\n",
    "cat -n train.fr | head -n10\n",
    "echo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzLYCAuD1-Ij"
   },
   "source": [
    "## 2. Setting up the environment\n",
    "\n",
    "The next step is to set up the environment for `pytorch` and define some helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSFcmlFdf6tK"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence, pack_sequence\n",
    "\n",
    "import sacrebleu\n",
    "\n",
    "###############\n",
    "# Torch setup #\n",
    "###############\n",
    "print('Torch version: {}, CUDA: {}'.format(torch.__version__, torch.version.cuda))\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "  print('WARNING: You may want to change the runtime to GPU for faster training!')\n",
    "  DEVICE = 'cpu'\n",
    "else:\n",
    "  DEVICE = 'cuda:0'\n",
    "\n",
    "#######################\n",
    "# Some helper functions\n",
    "#######################\n",
    "def fix_seed(seed=None):\n",
    "  \"\"\"Sets the seeds of random number generators.\"\"\"\n",
    "  if seed is None:\n",
    "    # Take a random seed\n",
    "    seed = time.time()\n",
    "  seed = int(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  return seed\n",
    "\n",
    "def readable_size(n):\n",
    "  \"\"\"Returns a readable size string for model parameters count.\"\"\"\n",
    "  sizes = ['K', 'M', 'G']\n",
    "  fmt = ''\n",
    "  size = n\n",
    "  for i, s in enumerate(sizes):\n",
    "    nn = n / (1000 ** (i + 1))\n",
    "    if nn >= 1:\n",
    "      size = nn\n",
    "      fmt = sizes[i]\n",
    "    else:\n",
    "      break\n",
    "  return '%.2f%s' % (size, fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWMhsDZOf6tS"
   },
   "source": [
    "## 3. Representing the vocabulary\n",
    "\n",
    "The `Vocabulary` class, which is defined below, encapsulates the **word-to-idx** and **idx-to-word** mapping that you should now be familiar with from the previous lab sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlxMOBzPf6tT"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "  \"\"\"Data structure representing the vocabulary of a corpus.\"\"\"\n",
    "  def __init__(self):\n",
    "    # Mapping from tokens to integers\n",
    "    self._word2idx = {}\n",
    "\n",
    "    # Reverse-mapping from integers to tokens\n",
    "    self.idx2word = []\n",
    "\n",
    "    # 0-padding token\n",
    "    self.add_word('<pad>')\n",
    "    # sentence start\n",
    "    self.add_word('<s>')\n",
    "    # sentence end\n",
    "    self.add_word('</s>')\n",
    "    # Unknown words\n",
    "    self.add_word('<unk>')\n",
    "\n",
    "    self._pad_idx = self._word2idx['<pad>']\n",
    "    self._bos_idx = self._word2idx['<s>']\n",
    "    self._eos_idx = self._word2idx['</s>']\n",
    "    self._unk_idx = self._word2idx['<unk>']\n",
    "\n",
    "  def word2idx(self, word):\n",
    "    \"\"\"Returns the integer ID of the word or <unk> if not found.\"\"\"\n",
    "    return self._word2idx.get(word, self._unk_idx)\n",
    "\n",
    "  def add_word(self, word):\n",
    "    \"\"\"Adds the `word` into the vocabulary.\"\"\"\n",
    "    if word not in self._word2idx:\n",
    "      self.idx2word.append(word)\n",
    "      self._word2idx[word] = len(self.idx2word) - 1\n",
    "\n",
    "  def build_from_file(self, fname):\n",
    "    \"\"\"Builds a vocabulary from a given corpus file.\"\"\"\n",
    "    with open(fname) as f:\n",
    "      for line in f:\n",
    "        words = line.strip().split()\n",
    "        for word in words:\n",
    "          self.add_word(word)\n",
    "\n",
    "  def convert_idxs_to_words(self, idxs, until_eos=False):\n",
    "    \"\"\"Converts a list of indices to words.\"\"\"\n",
    "    if until_eos:\n",
    "      try:\n",
    "        idxs = idxs[:idxs.index(self.word2idx('</s>'))]\n",
    "      except ValueError:\n",
    "        pass\n",
    "\n",
    "    return ' '.join(self.idx2word[idx] for idx in idxs)\n",
    "\n",
    "  def convert_words_to_idxs(self, words, add_bos=False, add_eos=False):\n",
    "    \"\"\"Converts a list of words to a list of indices.\"\"\"\n",
    "    idxs = [self.word2idx(w) for w in words]\n",
    "    if add_bos:\n",
    "      idxs.insert(0, self.word2idx('<s>'))\n",
    "    if add_eos:\n",
    "      idxs.append(self.word2idx('</s>'))\n",
    "    return idxs\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"Returns the size of the vocabulary.\"\"\"\n",
    "    return len(self.idx2word)\n",
    "\n",
    "  def __repr__(self):\n",
    "    return \"Vocabulary with {} items\".format(self.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-4PQ9_8f6tV"
   },
   "source": [
    "## 4. Representing the corpus\n",
    "\n",
    "Below we define a `Multi30K` class that encapsulates the vocabularies and the source/target sides of train/dev/test splits. Two important methods are `read_sentences()` and `get_batches()` which you may want to read in detail. \n",
    "\n",
    "***Q1: Explain why `pad_sequence` is used in the `get_batches()` method***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8GxcoBoETut"
   },
   "outputs": [],
   "source": [
    "class Multi30K:\n",
    "  \"\"\"A dataset wrapper for Multi30K.\"\"\"\n",
    "  def __init__(self, src_lang='en', trg_lang='fr'):\n",
    "    self.src_lang = src_lang\n",
    "    self.trg_lang = trg_lang\n",
    "\n",
    "    # Create vocabularies\n",
    "    self.src_vocab = Vocabulary()\n",
    "    self.src_vocab.build_from_file(f'train.{src_lang}')\n",
    "    self.trg_vocab = Vocabulary()\n",
    "    self.trg_vocab.build_from_file(f'train.{trg_lang}')\n",
    "\n",
    "    self.n_src_vocab = len(self.src_vocab)\n",
    "    self.n_trg_vocab = len(self.trg_vocab)\n",
    "\n",
    "    self._data = {}\n",
    "    for split in ('train', 'val', 'test'):\n",
    "      # Read sentences and map to indices using the vocabularies\n",
    "      print(f'Reading {split} set')\n",
    "      self._data[split] = self.read_sentences(split)\n",
    "\n",
    "  def read_sentences(self, split):\n",
    "    src_sents = []\n",
    "    trg_sents = []\n",
    "\n",
    "    # Read source side\n",
    "    with open(f'{split}.{self.src_lang}') as f:\n",
    "      for line in f:\n",
    "        line = line.strip()\n",
    "        assert line, \"Empty line found, please fix this!\"\n",
    "        idxs = self.src_vocab.convert_words_to_idxs(line.split(), add_eos=True)\n",
    "        src_sents.append(idxs)\n",
    "    # Read source side\n",
    "    with open(f'{split}.{self.trg_lang}') as f:\n",
    "      for line in f:\n",
    "        line = line.strip()\n",
    "        assert line, \"Empty line found, please fix this!\"\n",
    "        idxs = self.trg_vocab.convert_words_to_idxs(line.split(), add_bos=True, add_eos=True)\n",
    "        trg_sents.append(idxs)\n",
    "\n",
    "    assert len(src_sents) == len(trg_sents), \"Files are not aligned!\"\n",
    "    return src_sents, trg_sents\n",
    "\n",
    "  def get_batch(self, idxs, split='train'):\n",
    "    \"\"\"Returns padded torch tensors for source and target sample indices.\"\"\"\n",
    "    src_idxs = [torch.LongTensor(self._data[split][0][i]) for i in idxs]\n",
    "    trg_idxs = [torch.LongTensor(self._data[split][1][i]) for i in idxs]\n",
    "\n",
    "    ###################################\n",
    "    # Pad sequences to longest sequence\n",
    "    ###################################\n",
    "    padded_src_idxs = pad_sequence(src_idxs, padding_value=self.src_vocab._pad_idx)\n",
    "    padded_trg_idxs = pad_sequence(trg_idxs, padding_value=self.trg_vocab._pad_idx)\n",
    "    return padded_src_idxs.to(DEVICE), padded_trg_idxs.to(DEVICE)\n",
    "\n",
    "  def __repr__(self):\n",
    "    s = f\"Multi30K {self.src_lang} (# {self.n_src_vocab}) -> {self.trg_lang} (# {self.n_trg_vocab})\\n\"\n",
    "    s += f\" train: {len(self._data['train'][0])} sentences\\n\"\n",
    "    s += f\"   val: {len(self._data['val'][0])} sentences\\n\"\n",
    "    s += f\"  test: {len(self._data['test'][0])} sentences\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqXZCUkjgdp8"
   },
   "source": [
    "## 5. Create the dataset\n",
    "\n",
    "We now create a dataset instance to perform translation from French (FR) to English (EN). For demonstration purposes, we request a toy batch with 5 sentences and convert indices back to word forms.\n",
    "\n",
    "Notice how each column represents sentences and the shorter sequences are **zero-padded** at right.\n",
    "\n",
    "**NOTE**: Any code that deal with sequence-to-sequence paradigms should do its best to ignore these positions correctly, especially when using RNNs and Transformers with attention layers. Some support may come from the underlying toolkit that is used, through arguments related to **padding masks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90mVETUYzN6B"
   },
   "outputs": [],
   "source": [
    "dataset = Multi30K(src_lang='fr', trg_lang='en')\n",
    "print(dataset)\n",
    "\n",
    "# Get a batch and see inside\n",
    "src_idxs, trg_idxs = dataset.get_batch([0, 10, 234, 12, 7], split='val')\n",
    "print(src_idxs)\n",
    "for i in range(src_idxs.shape[1]):\n",
    "  print('SRC: ', dataset.src_vocab.convert_idxs_to_words(src_idxs[:, i]))\n",
    "  print('TRG: ', dataset.trg_vocab.convert_idxs_to_words(trg_idxs[:, i]))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmP0ZWbvB1aa"
   },
   "source": [
    "## 6. Encoder-Decoder Model\n",
    "\n",
    "In this section, you will implement an encoder-decoder model for neural machine translation (NMT). For a given source sentence $X = \\{x_1, \\dots, x_S\\}$ and the corresponding translation $Y = \\{y_1, \\dots, y_T\\}$, the model tries to predict the probability of words in the translation $Y$ using a **vectorial representation** $v$ of the source sentence.\n",
    "\n",
    "- The encoder is straight-forward: it receives the embeddings of the words in $X$ and produces contextualised representations for each position in the form of a matrix $H = \\{h_1, \\dots, h_S\\}$. The encoder can be uni- or bi-directional.\n",
    "\n",
    "- Several approaches exist to compress the whole source sentence into a single vector $v$:\n",
    "  - **Max pooling:** Perform a max pooling over $H$ to select the highest activations through time for each dimension.\n",
    "  - **Average pooling:** Take the average state as the representation by setting $v = \\frac{1}{S}\\sum_{i=1}^S h_i$.\n",
    "  - **Last hidden state:** Set $v = h_S$ assuming that the last hidden state of the RNN encoder would have collected as much linguistic information as possible from the source sentence.\n",
    "\n",
    "- The decoder is exactly a neural LM which is **additionally conditioned** over the representation vector $v$. The simplest way of achieving this is to set the  initial hidden state $d_0$ of the decoder to the (non-) linear projection of $v$.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2: Follow the `EncDecNMT` class below and fill in the `<TODO>` items:**\n",
    "\n",
    "- **Line ~49:** Create the GRU encoder\n",
    "- **Line ~59:** Handle encoders' output dimension in bi-directional case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZaoFR025lP3"
   },
   "outputs": [],
   "source": [
    "class EncDecNMT(nn.Module):\n",
    "  \"\"\"Encoder-decoder NMT without attention.\"\"\"\n",
    "  def __init__(self, dataset, emb_dim, enc_dim, dec_dim,\n",
    "               enc_bidirectional=False,\n",
    "               init_dec='max',\n",
    "               dropout=0.3, clip_gradient_norm=1.0, tie_weights=True,\n",
    "               batch_size=64):\n",
    "    # Call parent's __init__ first\n",
    "    super(EncDecNMT, self).__init__()\n",
    "\n",
    "    # Store arguments\n",
    "    self.dataset = dataset\n",
    "    self.emb_dim = emb_dim\n",
    "    self.enc_dim = enc_dim\n",
    "    self.enc_bidirectional = enc_bidirectional\n",
    "    self.dec_dim = dec_dim\n",
    "    self.init_dec = init_dec\n",
    "    self.clip_gradient_norm = clip_gradient_norm\n",
    "    self.p_dropout = dropout\n",
    "    self.tie_weights = tie_weights\n",
    "    self.batch_size = batch_size\n",
    "\n",
    "    assert self.init_dec in ('max', 'avg'), \\\n",
    "      \"init_dec argument contains unknown value!\"\n",
    "\n",
    "    # Since target sequences are <pad>'ded, we want to ignore the loss\n",
    "    # values for those positions\n",
    "    self.loss = nn.CrossEntropyLoss(\n",
    "        reduction='none', ignore_index=self.dataset.trg_vocab._pad_idx)\n",
    "\n",
    "    # Create the dropout\n",
    "    self.drop = nn.Dropout(p=self.p_dropout)\n",
    "\n",
    "    ###############################################\n",
    "    # Create the source and target embedding layers\n",
    "    ###############################################\n",
    "    self.src_emb = nn.Embedding(\n",
    "      num_embeddings=self.dataset.n_src_vocab, embedding_dim=self.emb_dim,\n",
    "      padding_idx=self.dataset.src_vocab._pad_idx)\n",
    "    self.trg_emb = nn.Embedding(\n",
    "      num_embeddings=self.dataset.n_trg_vocab, embedding_dim=self.emb_dim,\n",
    "      padding_idx=self.dataset.trg_vocab._pad_idx)\n",
    "\n",
    "    ###########################################\n",
    "    # QUESTION\n",
    "    ###########################################\n",
    "    # Create the encoder by using the arguments\n",
    "    ###########################################\n",
    "    self.enc = nn.GRU(\"<TODO>\")\n",
    "\n",
    "    ##################################################################\n",
    "    # QUESTION\n",
    "    ##################################################################\n",
    "    # Compute encoder's output dim which may be different than enc_dim\n",
    "    # because of bidirectionality\n",
    "    ##################################################################\n",
    "    self.enc_out_dim = self.enc_dim\n",
    "    if self.enc_bidirectional:\n",
    "      \"<TODO>\"\n",
    "\n",
    "    ###############################################################\n",
    "    # Let's use the `GRUCell` for decoder, which is designed to run for\n",
    "    # single timesteps through an explicit for loop. This will make\n",
    "    # it easier to understand the concepts shown at the lecture.\n",
    "    ###############################################################\n",
    "    self.dec = nn.GRUCell(input_size=self.emb_dim, hidden_size=self.dec_dim)\n",
    "\n",
    "    ##############################################################\n",
    "    # Add a non-linear layer for decoder initialisation projection\n",
    "    ##############################################################\n",
    "    self.ff_dec_init = nn.Sequential(\n",
    "        nn.Linear(self.enc_out_dim, self.dec_dim),\n",
    "        nn.Tanh(),\n",
    "    )\n",
    "\n",
    "    ####################################################################\n",
    "    # Bottleneck layer maps decoder's hidden state to emb size for tying\n",
    "    ####################################################################\n",
    "    self.bneck = nn.Linear(self.dec_dim, self.emb_dim)\n",
    "\n",
    "    ####################\n",
    "    # Final output layer\n",
    "    ####################\n",
    "    self.out = nn.Linear(self.emb_dim, self.dataset.n_trg_vocab)\n",
    "\n",
    "    ########################################################################\n",
    "    # This cuts the number of parameters by sharing the same embedding layer\n",
    "    # for both the inputs and outputs to/from the decoder GRU\n",
    "    ########################################################################\n",
    "    if self.tie_weights:\n",
    "      self.out.weight = self.trg_emb.weight\n",
    "\n",
    "    # Reset padding embeddings to all 0s for sanity\n",
    "    with torch.no_grad():\n",
    "        self.src_emb.weight.data[0].fill_(0)\n",
    "        self.trg_emb.weight.data[0].fill_(0)\n",
    "\n",
    "  def __repr__(self):\n",
    "    \"\"\"String representation for pretty-printing.\"\"\"\n",
    "    n_params = 0\n",
    "    for param in self.parameters():\n",
    "      n_params += np.cumprod(param.data.size())[-1]\n",
    "    n_params = readable_size(n_params)\n",
    "\n",
    "    s = super(EncDecNMT, self).__repr__()\n",
    "    return f\"{s}\\n# of parameters: {n_params} -- Decoder init: '{self.init_dec}'\"\n",
    "\n",
    "  def get_batch_indices(self, split='train', shuffle=True):\n",
    "    \"\"\"Returns the list of sample indices for a whole epoch.\"\"\"\n",
    "    # Get number of samples\n",
    "    n_samples = len(self.dataset._data[split][0])\n",
    "\n",
    "    # Get sample indices and batch them\n",
    "    if shuffle:\n",
    "      order = torch.randperm(n_samples)\n",
    "    else:\n",
    "      order = torch.arange(n_samples)\n",
    "\n",
    "    start_offsets = range(0, n_samples, self.batch_size)\n",
    "    return [order[i: i + self.batch_size] for i in start_offsets]\n",
    "\n",
    "  ###################################\n",
    "  # Decoder initialisation approaches\n",
    "  ###################################\n",
    "  def get_encoder_max_state(self, enc_states, input_mask):\n",
    "    \"\"\"Computes h_0 for the decoder based on the max-pooled encoding state.\"\"\"\n",
    "    # we fill the padded position values with small numbers so that\n",
    "    # max-pooling is not able to return wrong max values\n",
    "    masked_states = enc_states.masked_fill(\n",
    "        input_mask.unsqueeze(-1).bool().logical_not(), -100000)\n",
    "    max_state = masked_states.max(0)[0]\n",
    "    return max_state\n",
    "\n",
    "  def get_encoder_avg_state(self, enc_states, input_mask):\n",
    "    \"\"\"Returns the average of encoder states taking care of padding.\"\"\"\n",
    "    ##############################################################\n",
    "    # QUESTION\n",
    "    ##############################################################\n",
    "    # Compute the average encoder states by taking into account\n",
    "    # sentence length information through `input_mask`\n",
    "    ##############################################################\n",
    "    raise RuntimeError('Not implemented yet!')\n",
    "\n",
    "  def compute_decoder_state(self, enc_states, input_mask):\n",
    "    \"\"\"Calls the appropriate `init_dec` method, projects the `v`.\"\"\"\n",
    "    func = getattr(self, f'get_encoder_{self.init_dec}_state')\n",
    "    # Get the vector `v`\n",
    "    h_0 = func(enc_states, input_mask)\n",
    "    # Project it with the FF\n",
    "    return self.ff_dec_init(h_0)\n",
    "\n",
    "  ####################################\n",
    "  # Encodes a batch of input sentences\n",
    "  ####################################\n",
    "  def encode(self, x):\n",
    "    \"\"\"Encode tokens `x` to obtain the encoder states.\"\"\"\n",
    "    # Compute the mask to detect <pad>'s in the further parts\n",
    "    # x is a padded tensor of shape (seq_len, batch_size)\n",
    "    # mask: (seq_len, batch_size)\n",
    "    self.mask = x.ne(self.dataset.src_vocab._pad_idx).long()\n",
    "\n",
    "    # src_embs: (seq_len, batch_size, emb_dim)\n",
    "    embs = self.drop(self.src_emb(x))\n",
    "\n",
    "    # Pack the tensor so that RNN correctly computes the hidden\n",
    "    # states by ignoring padded positions\n",
    "    packed_inputs = pack_padded_sequence(\n",
    "        embs, lengths=self.mask.sum(0).long().cpu(), enforce_sorted=False)\n",
    "\n",
    "    # Encode -> unpack to obtain an ordinary tensor of hidden states\n",
    "    # padded positions will now have explicit 0's in their hidden states\n",
    "    # all_hids: (seq_len, batch_size, self.enc_out_dim)\n",
    "    self.all_hids = pad_packed_sequence(self.enc(packed_inputs)[0])[0]\n",
    "    return self.all_hids, self.mask\n",
    "\n",
    "  def compute_loss_from_logits(self, logits, y):\n",
    "    \"\"\"Returns the scalar losses for every element in the batch.\"\"\"\n",
    "    return self.loss(logits, y)\n",
    "\n",
    "  def compute_decoder_logits(self, dec_hid_state, y):\n",
    "    # *Cell() functions return (batch_size, hidden_size) i.e.\n",
    "    # a tensor containing the computed hidden state for each element in the batch\n",
    "    dec_hid_state = self.dec(self.trg_emb(y), dec_hid_state)\n",
    "    logits = self.out(self.bneck(self.drop(dec_hid_state)))\n",
    "    return logits, dec_hid_state\n",
    "\n",
    "  def forward(self, x, y):\n",
    "    \"\"\"Forward-pass of the model for training/ppl evaluation only.\"\"\"\n",
    "    # Encode source sentence and get encoder states\n",
    "    enc_states, input_mask = self.encode(x)\n",
    "\n",
    "    # Compute decoder's initial state h_0 for each sentence\n",
    "    dec_hid_state = self.compute_decoder_state(enc_states, input_mask)\n",
    "\n",
    "    # Manually compute the loss for each decoding timestep\n",
    "    # We skip the last item since we don't want to input </s>\n",
    "    loss = 0.0\n",
    "    n_trg_tokens = 0\n",
    "    for t in range(y.size(0) - 1):\n",
    "      y_prev, y_next = y[t], y[t + 1]\n",
    "\n",
    "      # Do a recurrence step\n",
    "      logits, dec_hid_state = self.compute_decoder_logits(dec_hid_state, y_prev)\n",
    "\n",
    "      # Accumulate the sequence loss\n",
    "      loss += self.compute_loss_from_logits(logits, y_next).sum()\n",
    "\n",
    "      # Compute number of valid positions by ignoring <pad>'s\n",
    "      n_trg_tokens += y_next.ne(self.dataset.trg_vocab._pad_idx).sum()\n",
    "\n",
    "    return loss, float(n_trg_tokens.item())\n",
    "\n",
    "  def train_model(self, optim, n_epochs=5):\n",
    "    \"\"\"Trains the model.\"\"\"\n",
    "    train_ppls, val_ppls = [], []\n",
    "\n",
    "    for eidx in range(1, n_epochs + 1):\n",
    "      start_time = time.time()\n",
    "      epoch_loss = 0\n",
    "      epoch_items = 0\n",
    "\n",
    "      # Enable training mode\n",
    "      self.train()\n",
    "\n",
    "      # Start training (will shuffle at each epoch)\n",
    "      for iter_count, idxs in enumerate(self.get_batch_indices('train')):\n",
    "        # Get x's and y's\n",
    "        x, y = self.dataset.get_batch(idxs)\n",
    "\n",
    "        # Clear the gradients\n",
    "        optim.zero_grad()\n",
    "\n",
    "        total_loss, n_items = self.forward(x, y)\n",
    "\n",
    "        # Backprop the average loss and update parameters\n",
    "        total_loss.div(n_items).backward()\n",
    "\n",
    "        # Clip the gradients to avoid exploding gradients\n",
    "        if self.clip_gradient_norm > 0:\n",
    "          torch.nn.utils.clip_grad_norm_(self.parameters(), self.clip_gradient_norm)\n",
    "\n",
    "        # Update parameters\n",
    "        optim.step()\n",
    "\n",
    "        # sum the loss for reporting, along with the denominator\n",
    "        epoch_loss += total_loss.item()\n",
    "        epoch_items += n_items\n",
    "\n",
    "        # Overall epoch loss and ppl\n",
    "        loss_per_token = epoch_loss / epoch_items\n",
    "        ppl = math.exp(loss_per_token)\n",
    "\n",
    "        if (iter_count + 1) % 100 == 0:\n",
    "          # Print progress\n",
    "          print(f'[Epoch {eidx:<3}] loss: {loss_per_token:6.2f}, perplexity: {ppl:6.2f}')\n",
    "\n",
    "      time_spent = time.time() - start_time\n",
    "\n",
    "      print(f'[Epoch {eidx:<3}] ended with train_loss: {loss_per_token:6.2f}, train_ppl: {ppl:6.2f}')\n",
    "      train_ppls.append(ppl)\n",
    "\n",
    "      # Evaluate on valid set\n",
    "      valid_loss, valid_ppl = self.evaluate('val')\n",
    "      val_ppls.append(valid_ppl)\n",
    "      print(f'[Epoch {eidx:<3}] ended with valid_loss: {valid_loss:6.2f}, valid_ppl: {valid_ppl:6.2f}')\n",
    "      print(f'[Epoch {eidx:<3}] completed in {time_spent:.2f} seconds')\n",
    "      print(f'[Epoch {eidx:<3}] validation BLEU with greedy search:')\n",
    "\n",
    "      bleu = self.greedy_search('val')\n",
    "      print(f'[Epoch {eidx:<3}] {bleu}\\n')\n",
    "\n",
    "    ######################################\n",
    "    # Evaluate the final model on test set\n",
    "    ######################################\n",
    "    test_loss, test_ppl = self.evaluate('test')\n",
    "    print(f' ---> Final test set performance: {test_loss:6.2f}, test_ppl: {test_ppl:6.2f}')\n",
    "\n",
    "  def evaluate(self, split):\n",
    "    # Switch to eval mode\n",
    "    self.eval()\n",
    "\n",
    "    eval_loss = 0.\n",
    "    eval_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for iter_count, idxs in enumerate(self.get_batch_indices(split, shuffle=False)):\n",
    "        # Get x's and y's\n",
    "        x, y = self.dataset.get_batch(idxs, split=split)\n",
    "\n",
    "        total_loss, n_items = self.forward(x, y)\n",
    "        eval_loss += total_loss.item()\n",
    "        eval_tokens += n_items\n",
    "    eval_loss /= eval_tokens\n",
    "\n",
    "    return eval_loss, math.exp(eval_loss)\n",
    "\n",
    "  def greedy_search(self, split, max_len=60):\n",
    "    \"\"\"Performs a greedy search, dumps the translations and computes BLEU.\"\"\"\n",
    "    # Switch to eval mode\n",
    "    self.eval()\n",
    "\n",
    "    bos = self.dataset.trg_vocab._bos_idx\n",
    "    eos = self.dataset.trg_vocab._eos_idx\n",
    "\n",
    "    # We keep the hypotheses for a batch in a tensor for efficiency\n",
    "    # Although there's a hard-limit for decoding timesteps `max_len`,\n",
    "    # the hypotheses will likely to produce </s> before reaching `max_len`.\n",
    "    batch_hyps = torch.zeros(\n",
    "      (max_len, self.batch_size), dtype=torch.long,\n",
    "      device=self.trg_emb.weight.device)\n",
    "\n",
    "    # Resulting sentences in dataset split order\n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for iter_count, idxs in enumerate(self.get_batch_indices(split, shuffle=False)):\n",
    "        # We don't care about `y` for translation decoding\n",
    "        # i.e. we rely on model's own predictions rather than ground-truths\n",
    "        x, _ = self.dataset.get_batch(idxs, split=split)\n",
    "\n",
    "        # Clear batch hypotheses tensor\n",
    "        batch_hyps.zero_()\n",
    "\n",
    "        # Get encoder hidden states\n",
    "        enc_states, input_mask = self.encode(x)\n",
    "\n",
    "        # Compute decoder's initial state h_0 for each sentence\n",
    "        h = self.compute_decoder_state(enc_states, input_mask)\n",
    "\n",
    "        # last batch could be smaller than the requested batch size\n",
    "        cur_batch_size = h.size(0)\n",
    "\n",
    "        # Start all sentences with <s>\n",
    "        next_word_idxs = torch.full(\n",
    "            (cur_batch_size, ), bos, dtype=torch.long, device=h.device)\n",
    "\n",
    "        # Track sentences who already produced </s>\n",
    "        track_fini = torch.zeros((cur_batch_size, ), device=h.device).bool()\n",
    "\n",
    "        # A maximum of `max_len` decoding steps\n",
    "        for t in range(max_len):\n",
    "          if track_fini.all():\n",
    "            # All hypotheses produced </s>, early stop!\n",
    "            break\n",
    "\n",
    "          # Get logits from the decoder\n",
    "          logits, h = self.compute_decoder_logits(h, next_word_idxs)\n",
    "\n",
    "          # Get next probabilities and argmax them for every sentence\n",
    "          next_word_idxs = nn.functional.softmax(logits, dim=-1).argmax(dim=-1)\n",
    "\n",
    "          # Update finished sentence tracker\n",
    "          track_fini.add_(next_word_idxs.eq(eos))\n",
    "\n",
    "          # Insert most probable words for timestep `t` into tensor\n",
    "          batch_hyps[t, :cur_batch_size] = next_word_idxs\n",
    "\n",
    "        # All finished, convert translations to python lists on CPU\n",
    "        results.extend(batch_hyps[:, :cur_batch_size].t().cpu().tolist())\n",
    "\n",
    "    # post-process results to convert them to actual sentences\n",
    "    out_fname = f'{split}_translations.{self.dataset.trg_lang}'\n",
    "    ref_fname = f'{split}.{self.dataset.trg_lang}'\n",
    "    hyps = []\n",
    "    refs = []\n",
    "\n",
    "    # read references\n",
    "    with open(ref_fname) as f:\n",
    "      for line in f:\n",
    "        refs.append(line.strip())\n",
    "\n",
    "    # Write hyps\n",
    "    with open(out_fname, 'w') as f:\n",
    "      for sent in results:\n",
    "        sent_str = self.dataset.trg_vocab.convert_idxs_to_words(sent, True)\n",
    "        hyps.append(sent_str)\n",
    "        f.write(sent_str + '\\n')\n",
    "\n",
    "    # Compute BLEU\n",
    "    bleu = sacrebleu.corpus_bleu(hyps, [refs], tokenize='none')\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0eHpYiRUHDe"
   },
   "source": [
    "## 7. Training the model\n",
    "\n",
    "The following function trains an encoder-decoder NMT with the base hyper-parameters that you can override through the keyword arguments. Proceed to the next code block for the actual training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kzOvvY6f6tx"
   },
   "outputs": [],
   "source": [
    "def train_encdec_model(n_epochs=5, init_lr=0.0005, **kwargs):\n",
    "  # Set the seed for reproducible results\n",
    "  fix_seed(30494)\n",
    "\n",
    "  base_params = {\n",
    "    'dataset':dataset,\n",
    "    'emb_dim':200,              # word embedding dim\n",
    "    'enc_dim':200,              # hidden layer dim for the encoder\n",
    "    'enc_bidirectional':False,  # True makes the encoder bidirectional\n",
    "    'dec_dim':300,              # hidden layer dim for the decoder\n",
    "    'clip_gradient_norm':1.0,   # gradient clip threshold\n",
    "    'dropout':0.3,              # dropout probability\n",
    "    'tie_weights':True,         # Weight typing for decoder inputs/outputs\n",
    "    'batch_size':64,            # Batch size\n",
    "    'init_dec':'max',          # Initialize the decoder with max or avg state\n",
    "  }\n",
    "\n",
    "  # Override with given arguments\n",
    "  base_params.update(kwargs)\n",
    "\n",
    "  # Create the Pytorch model\n",
    "  model = EncDecNMT(**base_params)\n",
    "\n",
    "  # move to device\n",
    "  model.to(DEVICE)\n",
    "\n",
    "  # Create the optimizer\n",
    "  opt = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "  print(model)\n",
    "\n",
    "  # Returns train, val and test perplexities\n",
    "  model.train_model(opt, n_epochs=n_epochs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJ1vPygDI3h"
   },
   "source": [
    "Below, you can add different configurations to `param_set` and they will be trained sequentially. At the end, the final model parameters will be used to decode the translations for the test set and BLEU scores will be computed and printed for each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9VPp4wqkknL"
   },
   "outputs": [],
   "source": [
    "# add more configurations to here to train other systems\n",
    "# related to the experiments for Q3\n",
    "param_set = [\n",
    "  {'enc_bidirectional': False, 'init_dec': 'max'},\n",
    "]\n",
    "\n",
    "bleu_scores = []\n",
    "\n",
    "for params in param_set:\n",
    "  header_msg = f'Training {params}'\n",
    "  print('*' * len(header_msg) + '\\n' + header_msg + '\\n' + '*' * len(header_msg))\n",
    "\n",
    "  # Train for 5 epochs\n",
    "  model = train_encdec_model(n_epochs=5, init_lr=0.0005, **params)\n",
    "\n",
    "  # Translate the test set and get the bleu scores\n",
    "  bleu = model.greedy_search('test', max_len=60)\n",
    "  bleu_scores.append(bleu)\n",
    "  \n",
    "print()\n",
    "\n",
    "for config, score in zip(param_set, bleu_scores):\n",
    "  print(f'{config}\\n  {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsYXRU68SWF7"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "\n",
    "At the end of 5 epochs, the default settings with max-pooled decoder initialisation and uni-directional encoder, should give you a BLEU score of $\\sim$**20** on the test set. Now here are some exercises that you can proceed with:\n",
    "\n",
    "---\n",
    "\n",
    "**Q3: Implement the `avg` initialisation (`get_encoder_average_state()` method) and fill in the table below for uni-directional and bi-directional encoder by training more configurations. What are your conclusions? In the end, which initialisation scheme yields the best BLEU?**\n",
    "\n",
    "| $v$| Unidir.   | Bidir.|\n",
    "|----|-----------|-------|\n",
    "| max|           |       |\n",
    "|avg |           |       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTLiSpQPqBv5"
   },
   "source": [
    "## 8. Incorporating `DOT` attention\n",
    "\n",
    "In the following, we derive our attentive NMT model from the `EncDecNMT` and only modify the necessary parts to incorporate the attention mechanism. Specifically, three `Linear` layers are added for decoder's state (query) projection, encoder state (key) projection, and a final projection from encoder's states to output layer dimensionality.\n",
    "\n",
    "---\n",
    "**Q4: Follow the code below and fill two `<TODO>` items in `__init__()` and four more in `compute_decoder_logits()` for the dot attention. (The MLP attention parts are extra exercises that you can come back to, after finishing the other questions - see Extra Questions at the end of the notebook.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUXfLx9Vk-aa"
   },
   "outputs": [],
   "source": [
    "class AttentionNMT(EncDecNMT):\n",
    "  \"\"\"Encoder-decoder NMT with attention.\"\"\"\n",
    "  def __init__(self, **kwargs):\n",
    "    # The internal dimension for the dot product i.e.\n",
    "    # the common dimension that d_t and h_i's should be projected\n",
    "    self.att_dim = kwargs.pop('att_dim')\n",
    "\n",
    "    # The attention type to compute the similarity scores\n",
    "    self.att_type = kwargs.pop('att_type')\n",
    "    assert self.att_type in ('dot', 'mlp'), \"att_type unknown.\"\n",
    "\n",
    "    # Call parent's __init__ with the remaining arguments\n",
    "    super(AttentionNMT, self).__init__(**kwargs)\n",
    "\n",
    "    ############################################\n",
    "    # QUESTION\n",
    "    ############################################\n",
    "    # Add decoder state (query) projection layer\n",
    "    self.ff_q_proj = nn.Linear(\"<TODO>\")\n",
    "\n",
    "    ############################################\n",
    "    # QUESTION\n",
    "    ############################################\n",
    "    # Add encoder states projection layer for similarity computation\n",
    "    ############################################\n",
    "    self.ff_k_proj = nn.Linear(\"<TODO>\")\n",
    "\n",
    "    ####################################################\n",
    "    # Adaptor so that the output of attention can be fed\n",
    "    # directly to the `self.bneck` layer\n",
    "    ####################################################\n",
    "    self.ff_enc2bneck = nn.Linear(self.enc_out_dim, self.dec_dim)\n",
    "\n",
    "    if self.att_type == 'mlp':\n",
    "      ###############################################################\n",
    "      # [EXTRA QUESTION]\n",
    "      # the only parameter you would add is linear layer (no bias)\n",
    "      # representing the `a` vector in the lecture slides.\n",
    "      ###############################################################\n",
    "      self.mlp_att = \"<TODO>\"\n",
    "\n",
    "  def encode(self, x):\n",
    "    # Let's first call the EncDec's encode()\n",
    "    all_hids, mask = super(AttentionNMT, self).encode(x)\n",
    "\n",
    "    # This is to avoid projection of encoder states at each decoding step\n",
    "    # since they can be precomputed at once\n",
    "    self.e_proj = self.ff_k_proj(all_hids)\n",
    "\n",
    "    return all_hids, mask\n",
    "\n",
    "  def compute_decoder_logits(self, dec_hid_state, y):\n",
    "    ###########################################################\n",
    "    # This step is the same as encoder-decoder, we feed the embedding\n",
    "    # and get `d_t` (query for attention) i.e. the hidden state of the decoder\n",
    "    ###########################################################\n",
    "    dec_hid_state = self.dec(self.trg_emb(y), dec_hid_state)\n",
    "\n",
    "    # Below you'll have to do a lot of permute(), t(), squeeze(), unsqueeze()\n",
    "    # operations to make dimensions compatible. Check PyTorch documents\n",
    "    # if you are not familiar with these operations\n",
    "\n",
    "    ###########################################################\n",
    "    # QUESTION\n",
    "    ###########################################################\n",
    "    # Project `dec_hid_state to attention dim with `ff_q_proj` layer\n",
    "    # Expected shape: (batch_size, att_dim, 1)\n",
    "    ###########################################################\n",
    "    proj_q = \"<TODO>\"\n",
    "\n",
    "    # Permuting the dimensions of already cached `self.e_proj`\n",
    "    # so that the shape becomes: (batch_size, seq_len, att_dim)\n",
    "    proj_e = self.e_proj.permute(1, 0, 2)\n",
    "\n",
    "    ###########################################################\n",
    "    # QUESTION (Dot attention)\n",
    "    ###########################################################\n",
    "    # Now that you have the queries for all the batch (proj_q)\n",
    "    # and encoder states for all source positions in the batch (proj_e)\n",
    "    # you can use `torch.bmm()` to compute all similarity scores at once.\n",
    "    # `bmm` stands for \"Batch matrix multiplication\". If you have two 3D\n",
    "    # tensors where first dimension represents the `batch_size`, `bmm`\n",
    "    # computes the products for each element in the batch.\n",
    "    ##########\n",
    "    # Example:\n",
    "    ##########\n",
    "    # torch.bmm(\"tensor of size B x S x A\" , \"tensor of size B x A x 1\")\n",
    "    #    --> produces a tensor of \"B x S x 1\"\n",
    "    ##########\n",
    "    # Use this to obtain the similarity scores and use squeeze() and t()\n",
    "    # to make it look like (seq_len, batch_size)\n",
    "    if self.att_type == 'dot':\n",
    "      scores = \"<TODO>\"\n",
    "    elif self.att_type == 'mlp':\n",
    "      ###########################################################\n",
    "      # [EXTRA QUESTION]\n",
    "      ###########################################################\n",
    "      scores = \"<TODO>\"\n",
    "\n",
    "    #########################################################\n",
    "    # we fill the padded positions with small numbers so that\n",
    "    # softmax() does not assign probabilities to them.\n",
    "    #########################################################\n",
    "    scores.masked_fill_(self.mask.logical_not(), -1e8)\n",
    "\n",
    "    #############################################\n",
    "    # QUESTION\n",
    "    #############################################\n",
    "    # Use softmax() on `scores` to obtain alpha's / probabilities\n",
    "    # expected shape: (seq_len, batch_size)\n",
    "    alpha = \"<TODO>\"\n",
    "\n",
    "    #############################################\n",
    "    # QUESTION\n",
    "    #############################################\n",
    "    # Weigh the bidirectional encoder states `self.all_hids`\n",
    "    # with `alpha`\n",
    "    # expected shape: (batch_size, self.enc_out_dim)\n",
    "    ctx = \"<TODO>\"\n",
    "\n",
    "    # Project the computed weighted context to `dec_dim` so that\n",
    "    # the output layer works as espected\n",
    "    # shape: (batch_size, dec_dim)\n",
    "    c_t = self.ff_enc2bneck(ctx)\n",
    "\n",
    "    ##################################################################\n",
    "    # We sum the decoder's state `d_t` and the computed `c_t` together\n",
    "    ##################################################################\n",
    "    logits = self.out(self.bneck(self.drop(c_t + dec_hid_state)))\n",
    "    return logits, dec_hid_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wv_3z30aRuCr"
   },
   "source": [
    "## 9. Training the model\n",
    "\n",
    "The following function trains an attentive NMT with the base hyper-parameters that you can override through the keyword arguments. Proceed to the next code block for the actual training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxv89r81F3Sx"
   },
   "outputs": [],
   "source": [
    "def train_attention_model(n_epochs=5, init_lr=0.0005, **kwargs):\n",
    "  # Set the seed for reproducible results\n",
    "  fix_seed(30494)\n",
    "\n",
    "  base_params = {\n",
    "    'dataset':dataset,\n",
    "    'emb_dim':200,              # word embedding dim\n",
    "    'enc_dim':200,              # hidden layer dim for the encoder\n",
    "    'enc_bidirectional':True,   # True makes the encoder bidirectional\n",
    "    'dec_dim':300,              # hidden layer dim for the decoder\n",
    "    'clip_gradient_norm':1.0,   # gradient clip threshold\n",
    "    'dropout':0.3,              # dropout probability\n",
    "    'tie_weights':True,         # Weight typing for decoder inputs/outputs\n",
    "    'batch_size':64,            # Batch size\n",
    "    'init_dec':'avg',           # Initialize the decoder with max or avg state\n",
    "    'att_dim': 200,             # Dot product's inner dimension\n",
    "    'att_type': 'dot',          # att_type dot/mlp\n",
    "  }\n",
    "\n",
    "  # Override with given arguments\n",
    "  base_params.update(kwargs)\n",
    "\n",
    "  # Create the Pytorch model\n",
    "  model = AttentionNMT(**base_params)\n",
    "\n",
    "  # move to device\n",
    "  model.to(DEVICE)\n",
    "\n",
    "  # Create the optimizer\n",
    "  opt = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "  print(model)\n",
    "\n",
    "  # Returns train, val and test perplexities\n",
    "  model.train_model(opt, n_epochs=n_epochs)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbJIeWDYC6kN"
   },
   "source": [
    "Below, you can add different configurations to `param_set` and they will be trained sequentially. At the end, the final model parameters will be used to decode the translations for the test set and BLEU scores will be computed and printed for each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A36iH_9wHG5I"
   },
   "outputs": [],
   "source": [
    "# add more configurations to here to train other systems\n",
    "param_set = [\n",
    "  {'enc_bidirectional': True, 'init_dec': 'avg', 'att_dim': 200, 'att_type': 'dot'},\n",
    "  {'enc_bidirectional': True, 'init_dec': 'avg', 'att_dim': 200, 'att_type': 'mlp'},\n",
    "]\n",
    "\n",
    "bleu_scores = []\n",
    "\n",
    "for params in param_set:\n",
    "  header_msg = f'Training {params}'\n",
    "  print('*' * len(header_msg) + '\\n' + header_msg + '\\n' + '*' * len(header_msg))\n",
    "\n",
    "  # Train for 5 epochs\n",
    "  model = train_attention_model(n_epochs=5, init_lr=0.0005, **params)\n",
    "\n",
    "  # Translate the test set and get the bleu scores\n",
    "  bleu = model.greedy_search('test', max_len=60)\n",
    "  bleu_scores.append(bleu)\n",
    "\n",
    "print()\n",
    "\n",
    "for config, score in zip(param_set, bleu_scores):\n",
    "  print(f'{config}\\n  {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d1-IXRH6R7fq"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "After completing the model and training the provided configuration, you should see a test BLEU of around **42** after 5 epochs! It's almost twice the performance that we observed with the decoder initialisation method ($\\sim$ 20).\n",
    "\n",
    "You can now play with the hyper-parameters to discover a better model in terms of the BLEU score or proceed with the extra questions below!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7m4z7y2F_5Hz"
   },
   "source": [
    "## Extra Questions\n",
    "\n",
    "**Q5:** Modify `EncDecNMT` to add a new initalisation method called `each`. In this mode, instead of initialising the decoder's initial state with the average encoder state, you'll add the average encoder state to each `y_prev` i.e. to each input of the decoder. This way, the same information will be much more accessible throughout decoding, instead of just being used at $t=0$. How does the performance compare to the `avg` method? Do you think initialising the decoder's hidden state is crucial to pass along source sentence representation or are you satisfied with this approach as well?\n",
    "\n",
    "**NOTE:** Make sure that you do no longer initialise your decoder's hidden state with the same information and leave it `None` i.e. a 0-vector initialisation. You can do this by adding a new initialisation method `zero` to your class and implementing the rest accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "**Q6: (Attentive NMT)** Experiment and try to come up with a better hyper-parameters settings that surpasses the default setting in terms of test BLEU. For example, you can try increasing the number of layers in the encoder to see what happens.\n",
    "\n",
    "---\n",
    "\n",
    "**Q7: (Attentive NMT)** Fill in the missing parts to implement the MLP attention. Compare the performance to `dot` attention by keeping all other hyper-parameters the same.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab04_mt.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.8.16 64-bit ('py38_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff27035c8dc0a26468b79942def09685664b2e815f4bca7e9395dcaceb48c986"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
