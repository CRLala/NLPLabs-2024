{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJMQLZ_uf6tI"
      },
      "source": [
        "# Lab 3, part 2: Neural Language Models\n",
        "\n",
        "In the second part of this session, you will experiment with feed-forward neural language models (FFLM) and recurrent language models (RNNLM) using [PyTorch](https://www.pytorch.org). To train the models, you will be using the [WikiText-2](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) corpus, which is a popular LM dataset introduced in 2016:\n",
        "\n",
        "> The WikiText language modeling dataset is a collection of texts extracted from Good and Featured articles on Wikipedia. The dataset is available under the Creative Commons Attribution-ShareAlike License. Compared to the preprocessed version of Penn Treebank (PTB), `WikiText-2` is over 2 times larger. The dataset also features a far larger vocabulary and retains the original case, punctuation and numbers - all of which are removed in PTB. As it is composed of full articles, the dataset is well suited for models that can take advantage of long term dependencies.\n",
        "\n",
        "**NOTE:** Training on the whole corpus is time consuming on CPU. Make sure that you switch to a GPU runtime in Colab or use the `train_small` corpus which is a subset of the WikiText-2 dataset.\n",
        "\n",
        "**Let's start by downloading the corpus:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "1eLd2J2B1i2u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading train.txt\n",
            "Downloading valid.txt\n",
            "Downloading test.txt\n",
            "     1\t Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
            "     2\t The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \n",
            "     3\t It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \n",
            "     4\t As with previous <unk> Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through <unk> text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely <unk> through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main <unk> , although they take a very minor role . \n",
            "     5\t The game 's battle system , the <unk> system , is carried over directly from <unk> Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters ' turns . Each character has a field and distance of movement limited by their Action <unk> . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant <unk> to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special <unk> that grant them temporary <unk> on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without <unk> his Action Point gauge , the character <unk> can shift into her \" Valkyria Form \" and become <unk> , while Imca can target multiple enemy units with her heavy weapon . \n",
            "     6\t Troops are divided into five classes : Scouts , <unk> , Engineers , <unk> and Armored Soldier . <unk> can switch classes by changing their assigned weapon . Changing class does not greatly affect the stats gained while in a previous class . With victory in battle , experience points are awarded to the squad , which are distributed into five different attributes shared by the entire squad , a feature differing from early games ' method of distributing to different unit types . \n",
            "     7\t The game takes place during the Second Europan War . Gallian Army Squad 422 , also known as \" The Nameless \" , are a penal military unit composed of criminals , foreign <unk> , and military offenders whose real names are erased from the records and <unk> officially referred to by numbers . <unk> by the Gallian military to perform the most dangerous missions that the Regular Army and Militia will not do , they are nevertheless up to the task , exemplified by their motto , <unk> <unk> , meaning \" Always Ready . \" The three main characters are <unk> Kurt Irving , an army officer falsely accused of treason who wishes to redeem himself ; Ace <unk> Imca , a female Darcsen heavy weapons specialist who seeks revenge against the Valkyria who destroyed her home ; and <unk> Riela <unk> , a seemingly <unk> young woman who is unknowingly a descendant of the Valkyria . Together with their fellow squad members , these three are tasked to fight against a mysterious Imperial unit known as Calamity Raven , consisting of mostly Darcsen soldiers . \n",
            "     8\t As the Nameless officially do not exist , the upper echelons of the Gallian Army exploit the concept of plausible <unk> in order to send them on missions that would otherwise make Gallia lose face in the war . While at times this works to their advantage , such as a successful incursion into Imperial territory , other orders cause certain members of the 422nd great distress . One such member , <unk> , becomes so enraged that he abandons his post and defects into the ranks of Calamity Raven , attached to the ideal of Darcsen independence proposed by their leader , Dahau . At the same time , elements within Gallian Army Command move to erase the Nameless in order to protect their own interests . <unk> by both allies and enemies , and combined with the presence of a traitor within their ranks , the 422nd desperately move to keep themselves alive while at the same time fight to help the Gallian war effort . This continues until the Nameless 's commanding officer , Ramsey Crowe , who had been kept under house arrest , is escorted to the capital city of <unk> in order to present evidence <unk> the weary soldiers and expose the real traitor , the Gallian General that had accused Kurt of Treason . \n",
            "     9\t <unk> due to these events , and partly due to the major losses in manpower Gallia suffers towards the end of the war with the Empire , the Nameless are offered a formal position as a squad in the Gallian Army rather than serve as an anonymous shadow force . This is short @-@ lived , however , as following Maximilian 's defeat , Dahau and Calamity Raven move to activate an ancient <unk> super weapon within the Empire , kept secret by their benefactor . Without the support of Maximilian or the chance to prove themselves in the war with Gallia , it is Dahau 's last <unk> card in creating a new Darcsen nation . As an armed Gallian force invading the Empire just following the two nations ' cease @-@ fire would certainly wreck their newfound peace , Kurt decides to once again make his squad the Nameless , asking Crowe to list himself and all under his command as killed @-@ in @-@ action . Now owing allegiance to none other than themselves , the 422nd confronts Dahau and destroys the <unk> weapon . Each member then goes their separate ways in order to begin their lives <unk> . \n",
            "    10\t Concept work for Valkyria Chronicles III began after development finished on Valkyria Chronicles II in early 2010 , with full development beginning shortly after this . The director of Valkyria Chronicles II , Takeshi Ozawa , returned to that role for Valkyria Chronicles III . Development work took approximately one year . After the release of Valkyria Chronicles II , the staff took a look at both the popular response for the game and what they wanted to do next for the series . Like its predecessor , Valkyria Chronicles III was developed for PlayStation Portable : this was due to the team wanting to refine the mechanics created for Valkyria Chronicles II , and they had not come up with the \" revolutionary \" idea that would warrant a new entry for the PlayStation 3 . Speaking in an interview , it was stated that the development team considered Valkyria Chronicles III to be the series ' first true sequel : while Valkyria Chronicles II had required a large amount of trial and error during development due to the platform move , the third game gave them a chance to improve upon the best parts of Valkyria Chronicles II due to being on the same platform . In addition to Sega staff from the previous games , development work was also handled by <unk> The original scenario was written <unk> <unk> , while the script was written by Hiroyuki <unk> , <unk> <unk> , <unk> <unk> , <unk> <unk> and <unk> <unk> . Its story was darker and more somber than that of its predecessor . \n",
            "\n",
            "\n",
            "   Line,   word,   character counts\n",
            "    2185   235854  1233015 test.txt\n",
            "    5000   558695  2954850 train_small.txt\n",
            "   17556  2007146 10596891 train.txt\n",
            "    1841   209338  1101534 valid.txt\n",
            "   26582  3011033 15886290 total\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "URL=\"https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2\"\n",
        "\n",
        "for split in \"train\" \"valid\" \"test\"; do\n",
        "  if [ ! -f \"${split}.txt\" ]; then\n",
        "    echo \"Downloading ${split}.txt\"\n",
        "    wget -q \"${URL}/${split}.txt\"\n",
        "    # Remove empty lines\n",
        "    sed -i '/^ *$/d' \"${split}.txt\"\n",
        "    # Remove article titles starting with = and ending with =\n",
        "    sed -i '/^ *= .* = $/d' \"${split}\".txt\n",
        "  fi\n",
        "done\n",
        "\n",
        "# Prepare smaller version for fast training neural LMs\n",
        "head -n 5000 < train.txt > train_small.txt\n",
        "\n",
        "# Print the first 10 lines with line numbers\n",
        "cat -n train.txt | head -n10\n",
        "echo\n",
        "\n",
        "# Print some statistics\n",
        "echo -e \"\\n   Line,   word,   character counts\"\n",
        "wc *.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzLYCAuD1-Ij"
      },
      "source": [
        "## Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "T9ZkAldEIJ3S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUBLAS_WORKSPACE_CONFIG=:4096:8\n"
          ]
        }
      ],
      "source": [
        "# in order to allow deterministic behaviour, that is, make results reproducible\n",
        "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "GSFcmlFdf6tK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch version: 1.12.1, CUDA: 11.3\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import time\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Fancy progress bar\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "###############\n",
        "# Torch setup #\n",
        "###############\n",
        "print('Torch version: {}, CUDA: {}'.format(torch.__version__, torch.version.cuda))\n",
        "cuda_available = torch.cuda.is_available()\n",
        "if not torch.cuda.is_available():\n",
        "  print('WARNING: You may want to change the runtime to GPU for Neural LM experiments!')\n",
        "  DEVICE = 'cpu'\n",
        "else:\n",
        "  DEVICE = 'cuda:0'\n",
        "\n",
        "#######################\n",
        "# Some helper functions\n",
        "#######################\n",
        "def fix_seed(seed=None):\n",
        "  \"\"\"Sets the seeds of random number generators.\"\"\"\n",
        "  torch.use_deterministic_algorithms(True)\n",
        "  if seed is None:\n",
        "    # Take a random seed\n",
        "    seed = time.time()\n",
        "  seed = int(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  return seed\n",
        "\n",
        "def readable_size(n):\n",
        "  \"\"\"Returns a readable size string for model parameters count.\"\"\"\n",
        "  sizes = ['K', 'M', 'G']\n",
        "  fmt = ''\n",
        "  size = n\n",
        "  for i, s in enumerate(sizes):\n",
        "    nn = n / (1000 ** (i + 1))\n",
        "    if nn >= 1:\n",
        "      size = nn\n",
        "      fmt = sizes[i]\n",
        "    else:\n",
        "      break\n",
        "  return '%.2f%s' % (size, fmt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmP0ZWbvB1aa"
      },
      "source": [
        "## Feed-forward Language Models (FFLM)\n",
        "\n",
        "FFLMs are similar to $n$-gram language models in the sense that the choice of $n$ is a hyperparameter for the network architecture. A basic FFLM constructs a  $C=n\\mathrm{-1}$ length context window before the word to be predicted. If the word embedding size is $E$, the feature vector for the context window becomes a vector of size $E\\times C$, resulting from the **concatenation** of individual word embeddings of context words. Hence, the choice of $C$ for FFLMs, affects the number of final learnable parameters in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWMhsDZOf6tS"
      },
      "source": [
        "### Representing the vocabulary\n",
        "\n",
        "The `Vocabulary` class below encapsulates the **word-to-idx** and **idx-to-word** mapping that you should now be familiar with from the previous lab sessions. Read it to understand how the vocabulary is constructed from a plain text file, within the `build_from_file()` method. Special `<.>` markers are also included in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "xlxMOBzPf6tT"
      },
      "outputs": [],
      "source": [
        "class Vocabulary(object):\n",
        "  \"\"\"Data structure representing the vocabulary of a corpus.\"\"\"\n",
        "  def __init__(self):\n",
        "    # Mapping from tokens to integers\n",
        "    self._word2idx = {}\n",
        "\n",
        "    # Reverse-mapping from integers to tokens\n",
        "    self.idx2word = []\n",
        "\n",
        "    # 0-padding token\n",
        "    self.add_word('<pad>')\n",
        "    # sentence start\n",
        "    self.add_word('<s>')\n",
        "    # sentence end\n",
        "    self.add_word('</s>')\n",
        "    # Unknown words\n",
        "    self.add_word('<unk>')\n",
        "\n",
        "    self._unk_idx = self._word2idx['<unk>']\n",
        "\n",
        "  def word2idx(self, word):\n",
        "    \"\"\"Returns the integer ID of the word or <unk> if not found.\"\"\"\n",
        "    return self._word2idx.get(word, self._unk_idx)\n",
        "\n",
        "  def add_word(self, word):\n",
        "    \"\"\"Adds the `word` into the vocabulary.\"\"\"\n",
        "    if word not in self._word2idx:\n",
        "      self.idx2word.append(word)\n",
        "      self._word2idx[word] = len(self.idx2word) - 1\n",
        "\n",
        "  def build_from_file(self, fname):\n",
        "    \"\"\"Builds a vocabulary from a given corpus file.\"\"\"\n",
        "    with open(fname) as f:\n",
        "      for line in f:\n",
        "        words = line.strip().split()\n",
        "        for word in words:\n",
        "          self.add_word(word)\n",
        "\n",
        "  def convert_idxs_to_words(self, idxs):\n",
        "    \"\"\"Converts a list of indices to words.\"\"\"\n",
        "    return ' '.join(self.idx2word[idx] for idx in idxs)\n",
        "\n",
        "  def convert_words_to_idxs(self, words):\n",
        "    \"\"\"Converts a list of words to a list of indices.\"\"\"\n",
        "    return [self.word2idx(w) for w in words]\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"Returns the size of the vocabulary.\"\"\"\n",
        "    return len(self.idx2word)\n",
        "  \n",
        "  def __repr__(self):\n",
        "    return \"Vocabulary with {} items\".format(self.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hE1Lr6j_oYB"
      },
      "source": [
        "Let's construct the vocabulary for the training set and analyse the token indices for a sentence with an unknown word.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Why do we map unknown tokens to a special `<unk>` token? Do you think the network will learn a useful embedding for that? If not, how can you let the network to learn an embedding for it?**\n",
        "\n",
        "**A:** *First of all, this is to alleviate the case of unknown tokens at test time. Obviously, there are no unknown tokens in the training set here, since we construct a full vocabulary out of it. So the embedding for the `<unk>` token will never be used during training and it will keep its randomly initialized content. You can let the network learn an embedding for it by constructing a smaller vocabulary (usually referred to as a short-list), say with only the tokens occurring at least 5 times. This way, the rare tokens will be mapped to `<unk>` during training as well. In short, this is really a design choice.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "QcNMhXqB5wwT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary with 33233 items\n",
            "[17, 5837, 4735, 94, 28, 3]\n"
          ]
        }
      ],
      "source": [
        "vocab = Vocabulary()\n",
        "vocab.build_from_file('train.txt')\n",
        "print(vocab)\n",
        "\n",
        "# Convert sentence to list of indices, note how the last word is mapped to 3 (<unk>)\n",
        "print(vocab.convert_words_to_idxs('the cat sat on a probably_an_unknown_word'.split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-4PQ9_8f6tV"
      },
      "source": [
        "### Representing the corpus\n",
        "\n",
        "Let's process the corpus for PyTorch: all splits will end up being a large, 1D token sequences. Note that, in `corpus_to_tensor()`, every line is wrapped between `<s> .. </s>` tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "M5s9MMt06YsL"
      },
      "outputs": [],
      "source": [
        "def corpus_to_tensor(_vocab, filename):\n",
        "  # Final token indices\n",
        "  idxs = []\n",
        "  \n",
        "  with open(filename) as data:\n",
        "    for line in tqdm(data, ncols=80, unit=' line', desc=f'Reading {filename} '):\n",
        "      line = line.strip()\n",
        "      # Skip empty lines if any\n",
        "      if line:\n",
        "        # Each line is considered as a long sentence for WikiText-2\n",
        "        line = f\"<s> {line} </s>\"\n",
        "        # Split from whitespace and add sentence markers\n",
        "        idxs.extend(_vocab.convert_words_to_idxs(line.split()))\n",
        "  return torch.LongTensor(idxs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Hl6xuwZS9uO1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reading train.txt : 17556 line [00:00, 24680.99 line/s]\n",
            "Reading train_small.txt : 5000 line [00:00, 23442.68 line/s]\n",
            "Reading valid.txt : 1841 line [00:00, 25285.92 line/s]\n",
            "Reading test.txt : 2185 line [00:00, 28198.98 line/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Small training size in tokens: 568.70K\n",
            "Training size in tokens: 2.04M\n",
            "Validation size in tokens: 213.02K\n",
            "Test size in tokens: 240.22K\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Read the files, prepare the small one as well\n",
        "train = corpus_to_tensor(vocab, 'train.txt')\n",
        "train_small = corpus_to_tensor(vocab, 'train_small.txt')\n",
        "\n",
        "valid = corpus_to_tensor(vocab, 'valid.txt')\n",
        "test = corpus_to_tensor(vocab, 'test.txt')\n",
        "print('\\n')\n",
        "\n",
        "print(f'Small training size in tokens: {readable_size(len(train_small))}')\n",
        "print(f'Training size in tokens: {readable_size(len(train))}')\n",
        "print(f'Validation size in tokens: {readable_size(len(valid))}')\n",
        "print(f'Test size in tokens: {readable_size(len(test))}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNxNrdhPECML"
      },
      "source": [
        "**Q: Print the first 20 token indices from the training set. And then print the sentence in actual words corresponding to these 20 tokens by using one of the provided methods in the `Vocabulary` class.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "cXobO9_n-Giz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 1,  4,  5,  6,  7,  8,  3,  9, 10, 11,  8, 12, 13, 14, 15,  6, 16, 17,\n",
            "        18,  7])\n",
            "<s> Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3\n"
          ]
        }
      ],
      "source": [
        "##############\n",
        "# Answer to Q2\n",
        "##############\n",
        "print(train[:20])\n",
        "print(vocab.convert_idxs_to_words(train[:20]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiroR-NszhGY"
      },
      "source": [
        "### Model definition\n",
        "\n",
        "Now that we are done with data loading and vocabulary construction, we can define the actual FFLM model in PyTorch. Recall from the lectures that this model requires a pre-defined context window size $C$ which will affect the way you set up some of the linear layers. **Note that**, in contrast to the model depicted in the lecture, this model has an additional layer `ff_ctx`, which projects the context vector $c_k$ to hidden dimension $H$. This ensures that the number of parameters in the output layer does not depend on the context size, i.e. it is always $H\\times V$ instead of $CE\\times V$.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Follow the comments in `__init__()` and `forward()` to fill in the missing parts with some actual code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "X9ttYW2IC_UV"
      },
      "outputs": [],
      "source": [
        "class FFLM(nn.Module):\n",
        "  def __init__(self, vocab_size, emb_dim, hid_dim, context_size, dropout=0.5):\n",
        "    # Call parent's __init__ first\n",
        "    super(FFLM, self).__init__()\n",
        "    \n",
        "    # Store arguments\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_dim = emb_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.context_size = context_size\n",
        "\n",
        "    # Create the loss, don't sum or average, we'll take care of it\n",
        "    # in the training loop for logging purposes\n",
        "    self.loss = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "    # Create the non-linearity\n",
        "    self.nonlin = torch.nn.Tanh()\n",
        "\n",
        "    # Dropout regularizer\n",
        "    self.drop = nn.Dropout(p=dropout)\n",
        "\n",
        "    ##############################\n",
        "    # Fill the missing parts below\n",
        "    ##############################\n",
        "    # Q: Compute the dimension of the context vector\n",
        "    self.context_dim = self.context_size * self.emb_dim\n",
        "    \n",
        "    # Create the embedding layer (i.e. lookup table tokens->vectors)\n",
        "    self.emb = nn.Embedding(\n",
        "        num_embeddings=self.vocab_size, embedding_dim=self.emb_dim,\n",
        "        padding_idx=0)\n",
        " \n",
        "    # This cuts the number of parameters a bit\n",
        "    self.ff_ctx = nn.Linear(self.context_dim, self.hid_dim)\n",
        "\n",
        "    ############################################\n",
        "    # Output layer mapping from the output of `ff_ctx` to vocabulary size\n",
        "    # Q: Fill the dimensions of the output layer\n",
        "    ############################################\n",
        "    self.out = nn.Linear(self.hid_dim, self.vocab_size)\n",
        "\n",
        "    # Purely for informational purposes: compute # of total params\n",
        "    self.n_params = 0\n",
        "    for param in self.parameters():\n",
        "        self.n_params += np.cumprod(param.data.size())[-1]\n",
        "    self.n_params = readable_size(self.n_params)\n",
        "      \n",
        "  def forward(self, x, y):\n",
        "    \"\"\"Forward-pass of the module.\"\"\"\n",
        "    # Shape of x is (batch_size, context_size)\n",
        "\n",
        "    # Get the embeddings for the token indices in `x`\n",
        "    embs = self.emb(x)\n",
        "\n",
        "    ##########################################################\n",
        "    # Q: Concatenate the embeddings to form the context vector\n",
        "    ##########################################################\n",
        "    ctx = embs.view(embs.shape[0], -1)\n",
        "\n",
        "    #######################################################\n",
        "    # Q: Apply ff_ctx -> non-lin -> dropout -> output layer\n",
        "    # to obtain the logits i.e. unnormalized scores   \n",
        "    #######################################################\n",
        "    ctx = self.drop(self.nonlin(self.ff_ctx(ctx)))\n",
        "    logits = self.out(ctx)\n",
        "\n",
        "    ###########################################################\n",
        "    # Q: Use self.loss to compute the losses, return the losses\n",
        "    # (true labels are in `y`)\n",
        "    ###########################################################\n",
        "    return self.loss(logits, y)\n",
        "\n",
        "  def get_batches(self, data_tensor, batch_size=64):\n",
        "    \"\"\"Returns a tensor of size (n_batches, batch_size, context_size + 1).\"\"\"\n",
        "    # Split data into rows of n-grams followed by the (n+1)th true label\n",
        "    x_y = data_tensor.unfold(0, self.context_size + 1, step=1)\n",
        "\n",
        "    # Get the number of training n-grams\n",
        "    n_samples = x_y.size()[0]\n",
        "\n",
        "    # Hack: discard the last uneven batch for simplicity\n",
        "    n_batches = n_samples // batch_size\n",
        "    n_samples = n_batches * batch_size\n",
        "    # Split nicely into batches, i.e. (n_batches, batch_size, context_size + 1)\n",
        "    # The final element in each row is the ID of the true label to predict\n",
        "    x_y = x_y[:n_samples].view(n_batches, batch_size, -1)\n",
        "\n",
        "    # A particular batch for context_size=2 will now look like below in\n",
        "    # word format. Last element for every array is the next token to be predicted\n",
        "    #\n",
        "    # [[<s>, cat, sat],\n",
        "    #  [cat, sat, on],\n",
        "    #  [sat, on,  the],\n",
        "    #  [on,  the, mat],\n",
        "    #   ....\n",
        "    return x_y\n",
        "\n",
        "  def train_model(self, optim, train_tensor, valid_tensor, test_tensor, n_epochs=5,\n",
        "                 batch_size=64, shuffle=False):\n",
        "    \"\"\"Trains the model.\"\"\"\n",
        "    # Get batches for the training data\n",
        "    batches = self.get_batches(train_tensor, batch_size)\n",
        "    \n",
        "    print(f'Will do {batches.size(0)} batches for an epoch.')\n",
        "\n",
        "    for eidx in range(1, n_epochs + 1):\n",
        "      start_time = time.time()\n",
        "      epoch_loss = 0\n",
        "      epoch_items = 0\n",
        "\n",
        "      # Enable training mode\n",
        "      self.train()\n",
        "\n",
        "      # Shuffle the batch order or not\n",
        "      if shuffle:\n",
        "        batch_order = torch.randperm(batches.size(0))\n",
        "      else:\n",
        "        batch_order = torch.arange(batches.size(0))\n",
        "\n",
        "      # Start training\n",
        "      for iter_count, idx in enumerate(batch_order):\n",
        "        batch = batches[idx].to(DEVICE)\n",
        "\n",
        "        # split into inputs `x` and labels `y`\n",
        "        x, y = batch[:, :self.context_size], batch[:, -1]\n",
        "\n",
        "        # Clear the gradients\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # loss will be a vector of size (batch_size, ) with losses per every sample\n",
        "        loss = self.forward(x, y)\n",
        "\n",
        "        # Backprop the average loss and update parameters\n",
        "        loss.mean().backward()\n",
        "        optim.step()\n",
        "\n",
        "        # sum the loss for reporting, along with the denominator\n",
        "        epoch_loss += loss.detach().sum()\n",
        "        epoch_items += loss.numel()\n",
        "\n",
        "        if iter_count % 1000 == 0:\n",
        "          # Print progress\n",
        "          loss_per_token = epoch_loss / epoch_items\n",
        "          ppl = math.exp(loss_per_token)\n",
        "          print(f'[Epoch {eidx:<3}] loss: {loss_per_token:6.2f}, perplexity: {ppl:6.2f}')\n",
        "\n",
        "      time_spent = time.time() - start_time\n",
        "\n",
        "      print(f'\\n[Epoch {eidx:<3}] ended with train_loss: {loss_per_token:6.2f}, ppl: {ppl:6.2f}')\n",
        "      # Evaluate on valid set\n",
        "      valid_loss, valid_ppl = self.evaluate(test_set=valid_tensor)\n",
        "      print(f'[Epoch {eidx:<3}] ended with valid_loss: {valid_loss:6.2f}, valid_ppl: {valid_ppl:6.2f}')\n",
        "      print(f'[Epoch {eidx:<3}] completed in {time_spent:.2f} seconds\\n')\n",
        "\n",
        "    # Evaluate the final model on test set\n",
        "    test_loss, test_ppl = self.evaluate(test_set=test_tensor)\n",
        "    print(f' ---> Final test set performance: {test_loss:6.2f}, test_ppl: {test_ppl:6.2f}')\n",
        "\n",
        "  def evaluate(self, test_set, batch_size=32):\n",
        "    \"\"\"Evaluates and computes perplexity for the given test set.\"\"\"\n",
        "    loss = 0\n",
        "\n",
        "    # Get the batches\n",
        "    batches = self.get_batches(test_set, batch_size)\n",
        "\n",
        "    # Eval mode\n",
        "    self.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch in batches:\n",
        "        batch = batch.to(DEVICE)\n",
        "\n",
        "        # split into inputs `x` and labels `y`\n",
        "        x, y = batch[:, :self.context_size], batch[:, -1]\n",
        "\n",
        "        # loss will be a vector of size (batch_size, ) with losses per every sample\n",
        "        # sum the loss for reporting, along with the denominator\n",
        "        loss += self.forward(x, y).sum()\n",
        "    \n",
        "    # Normalize by the number of tokens in the test set\n",
        "    loss /= batches.size()[:2].numel()\n",
        "\n",
        "    # Switch back to training mode\n",
        "    self.train()\n",
        "\n",
        "    # return the perplexity and loss\n",
        "    return loss, math.exp(loss)\n",
        "\n",
        "  def __repr__(self):\n",
        "    \"\"\"String representation for pretty-printing.\"\"\"\n",
        "    s = super(FFLM, self).__repr__()\n",
        "    return f\"{s}\\n# of parameters: {self.n_params}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0eHpYiRUHDe"
      },
      "source": [
        "### Training\n",
        "\n",
        "We can now launch training using a set of sane hyper-parameters for our model. This is a 3-gram FFLM since the context size is set to 2. On a Colab GPU, a single epoch should take around 1 minute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "7rjwvEYYFjjE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FFLM(\n",
            "  (loss): CrossEntropyLoss()\n",
            "  (nonlin): Tanh()\n",
            "  (drop): Dropout(p=0.4, inplace=False)\n",
            "  (emb): Embedding(33233, 128, padding_idx=0)\n",
            "  (ff_ctx): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (out): Linear(in_features=128, out_features=33233, bias=True)\n",
            ")\n",
            "# of parameters: 8.57M\n",
            "Starting training!\n",
            "Will do 7977 batches for an epoch.\n",
            "[Epoch 1  ] loss:  10.46, perplexity: 35026.95\n",
            "[Epoch 1  ] loss:   7.33, perplexity: 1518.19\n",
            "[Epoch 1  ] loss:   7.04, perplexity: 1143.49\n",
            "[Epoch 1  ] loss:   6.90, perplexity: 992.75\n",
            "[Epoch 1  ] loss:   6.80, perplexity: 900.52\n",
            "[Epoch 1  ] loss:   6.73, perplexity: 837.53\n",
            "[Epoch 1  ] loss:   6.67, perplexity: 790.53\n",
            "[Epoch 1  ] loss:   6.62, perplexity: 751.63\n",
            "\n",
            "[Epoch 1  ] ended with train_loss:   6.62, ppl: 751.63\n",
            "[Epoch 1  ] ended with valid_loss:   5.89, valid_ppl: 360.35\n",
            "[Epoch 1  ] completed in 27.52 seconds\n",
            "\n",
            "[Epoch 2  ] loss:   6.38, perplexity: 588.38\n",
            "[Epoch 2  ] loss:   6.09, perplexity: 442.92\n",
            "[Epoch 2  ] loss:   6.09, perplexity: 441.10\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 437.62\n",
            "[Epoch 2  ] loss:   6.07, perplexity: 434.75\n",
            "[Epoch 2  ] loss:   6.07, perplexity: 432.16\n",
            "[Epoch 2  ] loss:   6.06, perplexity: 428.66\n",
            "[Epoch 2  ] loss:   6.05, perplexity: 426.04\n",
            "\n",
            "[Epoch 2  ] ended with train_loss:   6.05, ppl: 426.04\n",
            "[Epoch 2  ] ended with valid_loss:   5.73, valid_ppl: 307.51\n",
            "[Epoch 2  ] completed in 28.20 seconds\n",
            "\n",
            "[Epoch 3  ] loss:   6.18, perplexity: 480.67\n",
            "[Epoch 3  ] loss:   5.84, perplexity: 342.73\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 346.04\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 346.21\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 346.92\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 347.81\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 347.51\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 347.78\n",
            "\n",
            "[Epoch 3  ] ended with train_loss:   5.85, ppl: 347.78\n",
            "[Epoch 3  ] ended with valid_loss:   5.65, valid_ppl: 282.88\n",
            "[Epoch 3  ] completed in 28.55 seconds\n",
            "\n",
            "[Epoch 4  ] loss:   5.62, perplexity: 276.03\n",
            "[Epoch 4  ] loss:   5.67, perplexity: 290.48\n",
            "[Epoch 4  ] loss:   5.69, perplexity: 294.91\n",
            "[Epoch 4  ] loss:   5.70, perplexity: 298.69\n",
            "[Epoch 4  ] loss:   5.71, perplexity: 301.19\n",
            "[Epoch 4  ] loss:   5.71, perplexity: 302.50\n",
            "[Epoch 4  ] loss:   5.72, perplexity: 304.14\n",
            "[Epoch 4  ] loss:   5.72, perplexity: 305.23\n",
            "\n",
            "[Epoch 4  ] ended with train_loss:   5.72, ppl: 305.23\n",
            "[Epoch 4  ] ended with valid_loss:   5.59, valid_ppl: 267.40\n",
            "[Epoch 4  ] completed in 28.69 seconds\n",
            "\n",
            "[Epoch 5  ] loss:   5.28, perplexity: 197.33\n",
            "[Epoch 5  ] loss:   5.55, perplexity: 257.89\n",
            "[Epoch 5  ] loss:   5.58, perplexity: 266.18\n",
            "[Epoch 5  ] loss:   5.59, perplexity: 269.02\n",
            "[Epoch 5  ] loss:   5.61, perplexity: 272.50\n",
            "[Epoch 5  ] loss:   5.62, perplexity: 274.84\n",
            "[Epoch 5  ] loss:   5.62, perplexity: 276.13\n",
            "[Epoch 5  ] loss:   5.63, perplexity: 277.39\n",
            "\n",
            "[Epoch 5  ] ended with train_loss:   5.63, ppl: 277.39\n",
            "[Epoch 5  ] ended with valid_loss:   5.55, valid_ppl: 257.71\n",
            "[Epoch 5  ] completed in 28.86 seconds\n",
            "\n",
            " ---> Final test set performance:   5.47, test_ppl: 238.14\n"
          ]
        }
      ],
      "source": [
        "# Set the seed for reproducible results\n",
        "fix_seed(30494)\n",
        "\n",
        "fflm_model = FFLM(\n",
        "    len(vocab),       # vocabulary size\n",
        "    emb_dim=128,      # word embedding dim\n",
        "    hid_dim=128,      # hidden layer dim\n",
        "    context_size=2,   # C = (N-1) if you think in n-gram LM terminology\n",
        "    dropout=0.4,      # dropout probability\n",
        ")\n",
        "\n",
        "# move to device\n",
        "fflm_model.to(DEVICE)\n",
        "\n",
        "# Initial learning rate for the optimizer\n",
        "FFLM_INIT_LR = 0.001\n",
        "\n",
        "# Create the optimizer\n",
        "fflm_optimizer = torch.optim.Adam(fflm_model.parameters(), lr=FFLM_INIT_LR)\n",
        "print(fflm_model)\n",
        "\n",
        "print('Starting training!')\n",
        "# NOTE: If you happen to have memory errors, try decreasing the batch size\n",
        "# It will print progress every 1000 batches\n",
        "fflm_model.train_model(fflm_optimizer, train, valid, test, n_epochs=5, batch_size=256, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAHN-C-XHuVs"
      },
      "source": [
        "**Q: If everything goes well, you should see a loss of around ~10.4 printed as the first loss. This will still be the case if you change the random seed to some other number before model construction i.e. the culprit is not the exact values that they take. Can you come up with a simple mathematical formula which yields that value?**\n",
        "\n",
        "**A:** *At the very first batch in your training, i.e. before doing the first optimizer step, the weights of your network are purely random. Hence, the output of the softmax layer will not be biased towards any token in the vocabulary. In other words, every vocabulary item/class should receive a probability of $\\frac{1}{V}$ for this batch, acting like a uniform distribution. If you then compute the negative log-likelihood of the uniform probability with $V$ outcomes, you'll end up with ~10.4.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "RiG_vI1uJj1X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.411\n"
          ]
        }
      ],
      "source": [
        "##########################\n",
        "# Answer to question above\n",
        "##########################\n",
        "print('{:.3f}'.format(-np.log(1 / len(vocab))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsYXRU68SWF7"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "With the default settings above, you should end up with a validation perplexity of $\\sim257$ and a final test set perplexity of $\\sim238$ at the end of 5th epoch. Now here are some exercises that you can proceed with:\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Remove the `tanh()` non-linearity from the code so that the context is computed as a linear combination of its embeddings. How does the results compare to the initial one? Do you think non-linearity helps?**\n",
        "\n",
        "**A:** *Removing the non-linearity seems to slightly increase the perplexity, so yes it has some impact on the results. Apparently, it is not a mandatory component of this specific architecture as one would have expected :).*\n",
        "\n",
        "**Q: Compare the results by rerunning the training with unshuffled batches i.e. with `shuffle=False`. What do you notice in terms of results?**\n",
        "\n",
        "**A:** *Apparently, presenting n-grams to the model in a shuffled order performs better than the original order that they appear. Shuffling is a form of regularization to prevent the model from being lazy i.e. when the text is presented in order, the task becomes easier during training since model can overfit to a set of most probable tokens already, for a Wiki article which talks about a specific topic. Thus, it may not exploit the context in full power. Also, the final set of weights at the end of 5th epoch would probably be biased towards achieving a good result on the final article of the training set! Shuffling removes this exposure bias.*\n",
        "\n",
        "**Q: Play with hyper-parameters related to dimensions and dropout. Could you find a model with smaller perplexity?**\n",
        "\n",
        "**A:** *Hyper-parameter search is a key notion for obtaining good models. The set of defaults given in this lab are purely empirical, so you could indeed find better hyper-parameters which will yield lower perplexities. Feel free to share your findings with us if something interesting comes out :)*\n",
        "\n",
        "**Q: Try with different context sizes such as 3, 5, 7, etc. What is the best perplexity you can get?**\n",
        "\n",
        "**A:** Below are the perplexities for $C \\in \\{1, 2, 3, 5, 7, 11\\}$. **Note** that since the size of the intermediate projection layer increases with $C$, new sets of hyperparameters may be required for optimum performance. You may also have better results by increasing the rate of dropout. In any case, bear in mind that we are only training the models for 5 epochs.\n",
        "\n",
        "| C  | Valid PPL | Test PPL |\n",
        "|----|-----------|----------|\n",
        "| 1 | 302 | 238\n",
        "| 2 | 257 | 279\n",
        "| 3 | 247 | 228\n",
        "| 5 | **242** | **224**\n",
        "| 7 | 245 | 226\n",
        "| 11 | 261 | 238"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "jnTCpdu7N-oP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FFLM(\n",
            "  (loss): CrossEntropyLoss()\n",
            "  (nonlin): Tanh()\n",
            "  (drop): Dropout(p=0.4, inplace=False)\n",
            "  (emb): Embedding(33233, 128, padding_idx=0)\n",
            "  (ff_ctx): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (out): Linear(in_features=128, out_features=33233, bias=True)\n",
            ")\n",
            "# of parameters: 8.57M\n",
            "Starting training!\n",
            "Will do 7977 batches for an epoch.\n",
            "[Epoch 1  ] loss:  10.46, perplexity: 35026.95\n",
            "[Epoch 1  ] loss:   7.33, perplexity: 1518.19\n",
            "[Epoch 1  ] loss:   7.04, perplexity: 1143.49\n",
            "[Epoch 1  ] loss:   6.90, perplexity: 992.75\n",
            "[Epoch 1  ] loss:   6.80, perplexity: 900.52\n",
            "[Epoch 1  ] loss:   6.73, perplexity: 837.53\n",
            "[Epoch 1  ] loss:   6.67, perplexity: 790.53\n",
            "[Epoch 1  ] loss:   6.62, perplexity: 751.63\n",
            "\n",
            "[Epoch 1  ] ended with train_loss:   6.62, ppl: 751.63\n",
            "[Epoch 1  ] ended with valid_loss:   5.89, valid_ppl: 360.35\n",
            "[Epoch 1  ] completed in 28.85 seconds\n",
            "\n",
            "[Epoch 2  ] loss:   6.38, perplexity: 588.38\n",
            "[Epoch 2  ] loss:   6.09, perplexity: 442.92\n",
            "[Epoch 2  ] loss:   6.09, perplexity: 441.10\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 437.62\n",
            "[Epoch 2  ] loss:   6.07, perplexity: 434.75\n",
            "[Epoch 2  ] loss:   6.07, perplexity: 432.16\n",
            "[Epoch 2  ] loss:   6.06, perplexity: 428.66\n",
            "[Epoch 2  ] loss:   6.05, perplexity: 426.04\n",
            "\n",
            "[Epoch 2  ] ended with train_loss:   6.05, ppl: 426.04\n",
            "[Epoch 2  ] ended with valid_loss:   5.73, valid_ppl: 307.51\n",
            "[Epoch 2  ] completed in 28.94 seconds\n",
            "\n",
            "[Epoch 3  ] loss:   6.18, perplexity: 480.67\n",
            "[Epoch 3  ] loss:   5.84, perplexity: 342.73\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 346.04\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 346.21\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 346.92\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 347.81\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 347.51\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 347.78\n",
            "\n",
            "[Epoch 3  ] ended with train_loss:   5.85, ppl: 347.78\n",
            "[Epoch 3  ] ended with valid_loss:   5.65, valid_ppl: 282.88\n",
            "[Epoch 3  ] completed in 29.00 seconds\n",
            "\n",
            "[Epoch 4  ] loss:   5.62, perplexity: 276.03\n",
            "[Epoch 4  ] loss:   5.67, perplexity: 290.48\n",
            "[Epoch 4  ] loss:   5.69, perplexity: 294.91\n",
            "[Epoch 4  ] loss:   5.70, perplexity: 298.69\n",
            "[Epoch 4  ] loss:   5.71, perplexity: 301.19\n",
            "[Epoch 4  ] loss:   5.71, perplexity: 302.50\n",
            "[Epoch 4  ] loss:   5.72, perplexity: 304.14\n",
            "[Epoch 4  ] loss:   5.72, perplexity: 305.23\n",
            "\n",
            "[Epoch 4  ] ended with train_loss:   5.72, ppl: 305.23\n",
            "[Epoch 4  ] ended with valid_loss:   5.59, valid_ppl: 267.40\n",
            "[Epoch 4  ] completed in 29.08 seconds\n",
            "\n",
            "[Epoch 5  ] loss:   5.28, perplexity: 197.33\n",
            "[Epoch 5  ] loss:   5.55, perplexity: 257.89\n",
            "[Epoch 5  ] loss:   5.58, perplexity: 266.18\n",
            "[Epoch 5  ] loss:   5.59, perplexity: 269.02\n",
            "[Epoch 5  ] loss:   5.61, perplexity: 272.50\n",
            "[Epoch 5  ] loss:   5.62, perplexity: 274.84\n",
            "[Epoch 5  ] loss:   5.62, perplexity: 276.13\n",
            "[Epoch 5  ] loss:   5.63, perplexity: 277.39\n",
            "\n",
            "[Epoch 5  ] ended with train_loss:   5.63, ppl: 277.39\n",
            "[Epoch 5  ] ended with valid_loss:   5.55, valid_ppl: 257.71\n",
            "[Epoch 5  ] completed in 28.92 seconds\n",
            "\n",
            " ---> Final test set performance:   5.47, test_ppl: 238.14\n",
            "FFLM(\n",
            "  (loss): CrossEntropyLoss()\n",
            "  (nonlin): Tanh()\n",
            "  (drop): Dropout(p=0.4, inplace=False)\n",
            "  (emb): Embedding(33233, 128, padding_idx=0)\n",
            "  (ff_ctx): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (out): Linear(in_features=128, out_features=33233, bias=True)\n",
            ")\n",
            "# of parameters: 8.56M\n",
            "Starting training!\n",
            "Will do 7977 batches for an epoch.\n",
            "[Epoch 1  ] loss:  10.49, perplexity: 35917.75\n",
            "[Epoch 1  ] loss:   7.38, perplexity: 1597.13\n",
            "[Epoch 1  ] loss:   7.07, perplexity: 1177.58\n",
            "[Epoch 1  ] loss:   6.93, perplexity: 1021.14\n",
            "[Epoch 1  ] loss:   6.83, perplexity: 924.98\n",
            "[Epoch 1  ] loss:   6.76, perplexity: 858.48\n",
            "[Epoch 1  ] loss:   6.70, perplexity: 812.80\n",
            "[Epoch 1  ] loss:   6.65, perplexity: 775.71\n",
            "\n",
            "[Epoch 1  ] ended with train_loss:   6.65, ppl: 775.71\n",
            "[Epoch 1  ] ended with valid_loss:   5.98, valid_ppl: 395.37\n",
            "[Epoch 1  ] completed in 28.66 seconds\n",
            "\n",
            "[Epoch 2  ] loss:   6.28, perplexity: 536.19\n",
            "[Epoch 2  ] loss:   6.18, perplexity: 482.54\n",
            "[Epoch 2  ] loss:   6.16, perplexity: 473.81\n",
            "[Epoch 2  ] loss:   6.16, perplexity: 471.12\n",
            "[Epoch 2  ] loss:   6.15, perplexity: 469.50\n",
            "[Epoch 2  ] loss:   6.15, perplexity: 466.97\n",
            "[Epoch 2  ] loss:   6.14, perplexity: 464.16\n",
            "[Epoch 2  ] loss:   6.13, perplexity: 459.54\n",
            "\n",
            "[Epoch 2  ] ended with train_loss:   6.13, ppl: 459.54\n",
            "[Epoch 2  ] ended with valid_loss:   5.83, valid_ppl: 341.00\n",
            "[Epoch 2  ] completed in 28.69 seconds\n",
            "\n",
            "[Epoch 3  ] loss:   5.61, perplexity: 273.44\n",
            "[Epoch 3  ] loss:   5.97, perplexity: 391.01\n",
            "[Epoch 3  ] loss:   5.96, perplexity: 386.46\n",
            "[Epoch 3  ] loss:   5.95, perplexity: 385.51\n",
            "[Epoch 3  ] loss:   5.96, perplexity: 386.79\n",
            "[Epoch 3  ] loss:   5.96, perplexity: 388.53\n",
            "[Epoch 3  ] loss:   5.96, perplexity: 389.15\n",
            "[Epoch 3  ] loss:   5.96, perplexity: 388.81\n",
            "\n",
            "[Epoch 3  ] ended with train_loss:   5.96, ppl: 388.81\n",
            "[Epoch 3  ] ended with valid_loss:   5.77, valid_ppl: 320.30\n",
            "[Epoch 3  ] completed in 28.75 seconds\n",
            "\n",
            "[Epoch 4  ] loss:   5.87, perplexity: 355.89\n",
            "[Epoch 4  ] loss:   5.84, perplexity: 344.20\n",
            "[Epoch 4  ] loss:   5.85, perplexity: 346.40\n",
            "[Epoch 4  ] loss:   5.85, perplexity: 347.58\n",
            "[Epoch 4  ] loss:   5.86, perplexity: 349.31\n",
            "[Epoch 4  ] loss:   5.86, perplexity: 349.53\n",
            "[Epoch 4  ] loss:   5.86, perplexity: 350.68\n",
            "[Epoch 4  ] loss:   5.86, perplexity: 351.40\n",
            "\n",
            "[Epoch 4  ] ended with train_loss:   5.86, ppl: 351.40\n",
            "[Epoch 4  ] ended with valid_loss:   5.73, valid_ppl: 308.93\n",
            "[Epoch 4  ] completed in 28.77 seconds\n",
            "\n",
            "[Epoch 5  ] loss:   5.71, perplexity: 301.71\n",
            "[Epoch 5  ] loss:   5.74, perplexity: 312.27\n",
            "[Epoch 5  ] loss:   5.76, perplexity: 318.91\n",
            "[Epoch 5  ] loss:   5.77, perplexity: 321.85\n",
            "[Epoch 5  ] loss:   5.78, perplexity: 322.68\n",
            "[Epoch 5  ] loss:   5.78, perplexity: 324.76\n",
            "[Epoch 5  ] loss:   5.79, perplexity: 326.68\n",
            "[Epoch 5  ] loss:   5.79, perplexity: 328.07\n",
            "\n",
            "[Epoch 5  ] ended with train_loss:   5.79, ppl: 328.07\n",
            "[Epoch 5  ] ended with valid_loss:   5.71, valid_ppl: 302.57\n",
            "[Epoch 5  ] completed in 28.88 seconds\n",
            "\n",
            " ---> Final test set performance:   5.63, test_ppl: 279.78\n",
            "FFLM(\n",
            "  (loss): CrossEntropyLoss()\n",
            "  (nonlin): Tanh()\n",
            "  (drop): Dropout(p=0.4, inplace=False)\n",
            "  (emb): Embedding(33233, 128, padding_idx=0)\n",
            "  (ff_ctx): Linear(in_features=384, out_features=128, bias=True)\n",
            "  (out): Linear(in_features=128, out_features=33233, bias=True)\n",
            ")\n",
            "# of parameters: 8.59M\n",
            "Starting training!\n",
            "Will do 7977 batches for an epoch.\n",
            "[Epoch 1  ] loss:  10.49, perplexity: 35948.32\n",
            "[Epoch 1  ] loss:   7.32, perplexity: 1511.17\n",
            "[Epoch 1  ] loss:   7.04, perplexity: 1145.74\n",
            "[Epoch 1  ] loss:   6.90, perplexity: 994.61\n",
            "[Epoch 1  ] loss:   6.81, perplexity: 905.72\n",
            "[Epoch 1  ] loss:   6.74, perplexity: 845.56\n",
            "[Epoch 1  ] loss:   6.68, perplexity: 798.07\n",
            "[Epoch 1  ] loss:   6.63, perplexity: 758.47\n",
            "\n",
            "[Epoch 1  ] ended with train_loss:   6.63, ppl: 758.47\n",
            "[Epoch 1  ] ended with valid_loss:   5.87, valid_ppl: 355.05\n",
            "[Epoch 1  ] completed in 29.26 seconds\n",
            "\n",
            "[Epoch 2  ] loss:   5.82, perplexity: 336.19\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 438.15\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 438.38\n",
            "[Epoch 2  ] loss:   6.07, perplexity: 431.31\n",
            "[Epoch 2  ] loss:   6.06, perplexity: 430.27\n",
            "[Epoch 2  ] loss:   6.06, perplexity: 427.93\n",
            "[Epoch 2  ] loss:   6.05, perplexity: 425.03\n",
            "[Epoch 2  ] loss:   6.05, perplexity: 423.10\n",
            "\n",
            "[Epoch 2  ] ended with train_loss:   6.05, ppl: 423.10\n",
            "[Epoch 2  ] ended with valid_loss:   5.70, valid_ppl: 298.73\n",
            "[Epoch 2  ] completed in 29.14 seconds\n",
            "\n",
            "[Epoch 3  ] loss:   6.13, perplexity: 459.41\n",
            "[Epoch 3  ] loss:   5.83, perplexity: 339.63\n",
            "[Epoch 3  ] loss:   5.83, perplexity: 340.24\n",
            "[Epoch 3  ] loss:   5.84, perplexity: 342.53\n",
            "[Epoch 3  ] loss:   5.83, perplexity: 341.91\n",
            "[Epoch 3  ] loss:   5.84, perplexity: 343.34\n",
            "[Epoch 3  ] loss:   5.84, perplexity: 343.22\n",
            "[Epoch 3  ] loss:   5.84, perplexity: 343.55\n",
            "\n",
            "[Epoch 3  ] ended with train_loss:   5.84, ppl: 343.55\n",
            "[Epoch 3  ] ended with valid_loss:   5.61, valid_ppl: 272.78\n",
            "[Epoch 3  ] completed in 29.27 seconds\n",
            "\n",
            "[Epoch 4  ] loss:   5.89, perplexity: 360.31\n",
            "[Epoch 4  ] loss:   5.64, perplexity: 282.05\n",
            "[Epoch 4  ] loss:   5.66, perplexity: 288.22\n",
            "[Epoch 4  ] loss:   5.67, perplexity: 289.82\n",
            "[Epoch 4  ] loss:   5.68, perplexity: 292.42\n",
            "[Epoch 4  ] loss:   5.68, perplexity: 293.70\n",
            "[Epoch 4  ] loss:   5.69, perplexity: 295.05\n",
            "[Epoch 4  ] loss:   5.69, perplexity: 296.30\n",
            "\n",
            "[Epoch 4  ] ended with train_loss:   5.69, ppl: 296.30\n",
            "[Epoch 4  ] ended with valid_loss:   5.56, valid_ppl: 258.94\n",
            "[Epoch 4  ] completed in 29.13 seconds\n",
            "\n",
            "[Epoch 5  ] loss:   5.25, perplexity: 190.17\n",
            "[Epoch 5  ] loss:   5.54, perplexity: 253.59\n",
            "[Epoch 5  ] loss:   5.56, perplexity: 259.12\n",
            "[Epoch 5  ] loss:   5.56, perplexity: 260.23\n",
            "[Epoch 5  ] loss:   5.57, perplexity: 261.61\n",
            "[Epoch 5  ] loss:   5.58, perplexity: 264.61\n",
            "[Epoch 5  ] loss:   5.58, perplexity: 266.30\n",
            "[Epoch 5  ] loss:   5.59, perplexity: 268.28\n",
            "\n",
            "[Epoch 5  ] ended with train_loss:   5.59, ppl: 268.28\n",
            "[Epoch 5  ] ended with valid_loss:   5.51, valid_ppl: 247.10\n",
            "[Epoch 5  ] completed in 29.20 seconds\n",
            "\n",
            " ---> Final test set performance:   5.43, test_ppl: 228.70\n",
            "FFLM(\n",
            "  (loss): CrossEntropyLoss()\n",
            "  (nonlin): Tanh()\n",
            "  (drop): Dropout(p=0.4, inplace=False)\n",
            "  (emb): Embedding(33233, 128, padding_idx=0)\n",
            "  (ff_ctx): Linear(in_features=640, out_features=128, bias=True)\n",
            "  (out): Linear(in_features=128, out_features=33233, bias=True)\n",
            ")\n",
            "# of parameters: 8.62M\n",
            "Starting training!\n",
            "Will do 7977 batches for an epoch.\n",
            "[Epoch 1  ] loss:  10.50, perplexity: 36260.48\n",
            "[Epoch 1  ] loss:   7.31, perplexity: 1497.15\n",
            "[Epoch 1  ] loss:   7.07, perplexity: 1174.73\n",
            "[Epoch 1  ] loss:   6.93, perplexity: 1025.51\n",
            "[Epoch 1  ] loss:   6.84, perplexity: 934.69\n",
            "[Epoch 1  ] loss:   6.77, perplexity: 872.31\n",
            "[Epoch 1  ] loss:   6.71, perplexity: 823.99\n",
            "[Epoch 1  ] loss:   6.66, perplexity: 782.27\n",
            "\n",
            "[Epoch 1  ] ended with train_loss:   6.66, ppl: 782.27\n",
            "[Epoch 1  ] ended with valid_loss:   5.87, valid_ppl: 355.33\n",
            "[Epoch 1  ] completed in 29.34 seconds\n",
            "\n",
            "[Epoch 2  ] loss:   5.97, perplexity: 390.38\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 435.40\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 438.78\n",
            "[Epoch 2  ] loss:   6.09, perplexity: 441.29\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 438.33\n",
            "[Epoch 2  ] loss:   6.07, perplexity: 434.41\n",
            "[Epoch 2  ] loss:   6.06, perplexity: 430.23\n",
            "[Epoch 2  ] loss:   6.06, perplexity: 427.96\n",
            "\n",
            "[Epoch 2  ] ended with train_loss:   6.06, ppl: 427.96\n",
            "[Epoch 2  ] ended with valid_loss:   5.69, valid_ppl: 295.18\n",
            "[Epoch 2  ] completed in 29.39 seconds\n",
            "\n",
            "[Epoch 3  ] loss:   5.70, perplexity: 300.31\n",
            "[Epoch 3  ] loss:   5.82, perplexity: 338.39\n",
            "[Epoch 3  ] loss:   5.82, perplexity: 335.31\n",
            "[Epoch 3  ] loss:   5.82, perplexity: 337.11\n",
            "[Epoch 3  ] loss:   5.82, perplexity: 338.31\n",
            "[Epoch 3  ] loss:   5.83, perplexity: 340.71\n",
            "[Epoch 3  ] loss:   5.83, perplexity: 339.99\n",
            "[Epoch 3  ] loss:   5.83, perplexity: 340.33\n",
            "\n",
            "[Epoch 3  ] ended with train_loss:   5.83, ppl: 340.33\n",
            "[Epoch 3  ] ended with valid_loss:   5.59, valid_ppl: 267.89\n",
            "[Epoch 3  ] completed in 29.44 seconds\n",
            "\n",
            "[Epoch 4  ] loss:   6.26, perplexity: 524.22\n",
            "[Epoch 4  ] loss:   5.63, perplexity: 277.65\n",
            "[Epoch 4  ] loss:   5.64, perplexity: 281.97\n",
            "[Epoch 4  ] loss:   5.65, perplexity: 285.41\n",
            "[Epoch 4  ] loss:   5.66, perplexity: 287.41\n",
            "[Epoch 4  ] loss:   5.67, perplexity: 289.82\n",
            "[Epoch 4  ] loss:   5.68, perplexity: 292.19\n",
            "[Epoch 4  ] loss:   5.68, perplexity: 293.09\n",
            "\n",
            "[Epoch 4  ] ended with train_loss:   5.68, ppl: 293.09\n",
            "[Epoch 4  ] ended with valid_loss:   5.53, valid_ppl: 250.90\n",
            "[Epoch 4  ] completed in 29.40 seconds\n",
            "\n",
            "[Epoch 5  ] loss:   4.44, perplexity:  85.14\n",
            "[Epoch 5  ] loss:   5.50, perplexity: 244.43\n",
            "[Epoch 5  ] loss:   5.53, perplexity: 251.51\n",
            "[Epoch 5  ] loss:   5.53, perplexity: 252.72\n",
            "[Epoch 5  ] loss:   5.54, perplexity: 254.57\n",
            "[Epoch 5  ] loss:   5.55, perplexity: 256.95\n",
            "[Epoch 5  ] loss:   5.56, perplexity: 260.33\n",
            "[Epoch 5  ] loss:   5.57, perplexity: 261.84\n",
            "\n",
            "[Epoch 5  ] ended with train_loss:   5.57, ppl: 261.84\n",
            "[Epoch 5  ] ended with valid_loss:   5.49, valid_ppl: 242.04\n",
            "[Epoch 5  ] completed in 29.39 seconds\n",
            "\n",
            " ---> Final test set performance:   5.41, test_ppl: 224.19\n",
            "FFLM(\n",
            "  (loss): CrossEntropyLoss()\n",
            "  (nonlin): Tanh()\n",
            "  (drop): Dropout(p=0.4, inplace=False)\n",
            "  (emb): Embedding(33233, 128, padding_idx=0)\n",
            "  (ff_ctx): Linear(in_features=896, out_features=128, bias=True)\n",
            "  (out): Linear(in_features=128, out_features=33233, bias=True)\n",
            ")\n",
            "# of parameters: 8.66M\n",
            "Starting training!\n",
            "Will do 7977 batches for an epoch.\n",
            "[Epoch 1  ] loss:  10.44, perplexity: 34295.59\n",
            "[Epoch 1  ] loss:   7.26, perplexity: 1424.95\n",
            "[Epoch 1  ] loss:   7.06, perplexity: 1167.58\n",
            "[Epoch 1  ] loss:   6.95, perplexity: 1038.41\n",
            "[Epoch 1  ] loss:   6.86, perplexity: 957.73\n",
            "[Epoch 1  ] loss:   6.80, perplexity: 896.71\n",
            "[Epoch 1  ] loss:   6.74, perplexity: 848.00\n",
            "[Epoch 1  ] loss:   6.70, perplexity: 810.00\n",
            "\n",
            "[Epoch 1  ] ended with train_loss:   6.70, ppl: 810.00\n",
            "[Epoch 1  ] ended with valid_loss:   5.89, valid_ppl: 359.86\n",
            "[Epoch 1  ] completed in 29.60 seconds\n",
            "\n",
            "[Epoch 2  ] loss:   6.40, perplexity: 602.91\n",
            "[Epoch 2  ] loss:   6.13, perplexity: 460.34\n",
            "[Epoch 2  ] loss:   6.13, perplexity: 457.19\n",
            "[Epoch 2  ] loss:   6.13, perplexity: 457.43\n",
            "[Epoch 2  ] loss:   6.11, perplexity: 452.43\n",
            "[Epoch 2  ] loss:   6.11, perplexity: 449.66\n",
            "[Epoch 2  ] loss:   6.10, perplexity: 446.88\n",
            "[Epoch 2  ] loss:   6.10, perplexity: 444.80\n",
            "\n",
            "[Epoch 2  ] ended with train_loss:   6.10, ppl: 444.80\n",
            "[Epoch 2  ] ended with valid_loss:   5.69, valid_ppl: 296.99\n",
            "[Epoch 2  ] completed in 29.77 seconds\n",
            "\n",
            "[Epoch 3  ] loss:   6.54, perplexity: 689.88\n",
            "[Epoch 3  ] loss:   5.82, perplexity: 338.51\n",
            "[Epoch 3  ] loss:   5.84, perplexity: 343.71\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 347.04\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 347.29\n",
            "[Epoch 3  ] loss:   5.86, perplexity: 349.40\n",
            "[Epoch 3  ] loss:   5.86, perplexity: 349.54\n",
            "[Epoch 3  ] loss:   5.86, perplexity: 350.34\n",
            "\n",
            "[Epoch 3  ] ended with train_loss:   5.86, ppl: 350.34\n",
            "[Epoch 3  ] ended with valid_loss:   5.59, valid_ppl: 268.81\n",
            "[Epoch 3  ] completed in 29.73 seconds\n",
            "\n",
            "[Epoch 4  ] loss:   5.34, perplexity: 209.05\n",
            "[Epoch 4  ] loss:   5.63, perplexity: 279.49\n",
            "[Epoch 4  ] loss:   5.66, perplexity: 286.71\n",
            "[Epoch 4  ] loss:   5.66, perplexity: 288.48\n",
            "[Epoch 4  ] loss:   5.68, perplexity: 292.61\n",
            "[Epoch 4  ] loss:   5.69, perplexity: 295.61\n",
            "[Epoch 4  ] loss:   5.70, perplexity: 297.64\n",
            "[Epoch 4  ] loss:   5.70, perplexity: 299.61\n",
            "\n",
            "[Epoch 4  ] ended with train_loss:   5.70, ppl: 299.61\n",
            "[Epoch 4  ] ended with valid_loss:   5.53, valid_ppl: 252.98\n",
            "[Epoch 4  ] completed in 29.80 seconds\n",
            "\n",
            "[Epoch 5  ] loss:   5.13, perplexity: 169.17\n",
            "[Epoch 5  ] loss:   5.51, perplexity: 245.96\n",
            "[Epoch 5  ] loss:   5.53, perplexity: 251.66\n",
            "[Epoch 5  ] loss:   5.55, perplexity: 257.45\n",
            "[Epoch 5  ] loss:   5.57, perplexity: 261.25\n",
            "[Epoch 5  ] loss:   5.58, perplexity: 264.78\n",
            "[Epoch 5  ] loss:   5.59, perplexity: 266.41\n",
            "[Epoch 5  ] loss:   5.59, perplexity: 268.07\n",
            "\n",
            "[Epoch 5  ] ended with train_loss:   5.59, ppl: 268.07\n",
            "[Epoch 5  ] ended with valid_loss:   5.50, valid_ppl: 245.23\n",
            "[Epoch 5  ] completed in 29.77 seconds\n",
            "\n",
            " ---> Final test set performance:   5.42, test_ppl: 226.11\n",
            "FFLM(\n",
            "  (loss): CrossEntropyLoss()\n",
            "  (nonlin): Tanh()\n",
            "  (drop): Dropout(p=0.4, inplace=False)\n",
            "  (emb): Embedding(33233, 128, padding_idx=0)\n",
            "  (ff_ctx): Linear(in_features=1408, out_features=128, bias=True)\n",
            "  (out): Linear(in_features=128, out_features=33233, bias=True)\n",
            ")\n",
            "# of parameters: 8.72M\n",
            "Starting training!\n",
            "Will do 7977 batches for an epoch.\n",
            "[Epoch 1  ] loss:  10.43, perplexity: 33817.47\n",
            "[Epoch 1  ] loss:   7.26, perplexity: 1428.95\n",
            "[Epoch 1  ] loss:   7.08, perplexity: 1188.06\n",
            "[Epoch 1  ] loss:   6.98, perplexity: 1069.65\n",
            "[Epoch 1  ] loss:   6.90, perplexity: 991.84\n",
            "[Epoch 1  ] loss:   6.84, perplexity: 930.68\n",
            "[Epoch 1  ] loss:   6.79, perplexity: 884.67\n",
            "[Epoch 1  ] loss:   6.74, perplexity: 843.13\n",
            "\n",
            "[Epoch 1  ] ended with train_loss:   6.74, ppl: 843.13\n",
            "[Epoch 1  ] ended with valid_loss:   5.91, valid_ppl: 370.26\n",
            "[Epoch 1  ] completed in 30.06 seconds\n",
            "\n",
            "[Epoch 2  ] loss:   5.73, perplexity: 307.14\n",
            "[Epoch 2  ] loss:   6.19, perplexity: 487.49\n",
            "[Epoch 2  ] loss:   6.18, perplexity: 484.68\n",
            "[Epoch 2  ] loss:   6.18, perplexity: 483.11\n",
            "[Epoch 2  ] loss:   6.18, perplexity: 482.44\n",
            "[Epoch 2  ] loss:   6.17, perplexity: 480.51\n",
            "[Epoch 2  ] loss:   6.17, perplexity: 479.46\n",
            "[Epoch 2  ] loss:   6.17, perplexity: 475.87\n",
            "\n",
            "[Epoch 2  ] ended with train_loss:   6.17, ppl: 475.87\n",
            "[Epoch 2  ] ended with valid_loss:   5.74, valid_ppl: 310.92\n",
            "[Epoch 2  ] completed in 29.98 seconds\n",
            "\n",
            "[Epoch 3  ] loss:   5.93, perplexity: 375.53\n",
            "[Epoch 3  ] loss:   5.90, perplexity: 365.58\n",
            "[Epoch 3  ] loss:   5.91, perplexity: 368.15\n",
            "[Epoch 3  ] loss:   5.91, perplexity: 369.18\n",
            "[Epoch 3  ] loss:   5.92, perplexity: 372.80\n",
            "[Epoch 3  ] loss:   5.92, perplexity: 373.30\n",
            "[Epoch 3  ] loss:   5.93, perplexity: 375.44\n",
            "[Epoch 3  ] loss:   5.93, perplexity: 375.83\n",
            "\n",
            "[Epoch 3  ] ended with train_loss:   5.93, ppl: 375.83\n",
            "[Epoch 3  ] ended with valid_loss:   5.65, valid_ppl: 285.12\n",
            "[Epoch 3  ] completed in 30.25 seconds\n",
            "\n",
            "[Epoch 4  ] loss:   5.92, perplexity: 371.09\n",
            "[Epoch 4  ] loss:   5.71, perplexity: 301.17\n",
            "[Epoch 4  ] loss:   5.74, perplexity: 309.84\n",
            "[Epoch 4  ] loss:   5.75, perplexity: 314.59\n",
            "[Epoch 4  ] loss:   5.76, perplexity: 317.81\n",
            "[Epoch 4  ] loss:   5.77, perplexity: 319.74\n",
            "[Epoch 4  ] loss:   5.77, perplexity: 322.05\n",
            "[Epoch 4  ] loss:   5.78, perplexity: 324.17\n",
            "\n",
            "[Epoch 4  ] ended with train_loss:   5.78, ppl: 324.17\n",
            "[Epoch 4  ] ended with valid_loss:   5.59, valid_ppl: 268.57\n",
            "[Epoch 4  ] completed in 30.05 seconds\n",
            "\n",
            "[Epoch 5  ] loss:   5.51, perplexity: 248.34\n",
            "[Epoch 5  ] loss:   5.58, perplexity: 265.52\n",
            "[Epoch 5  ] loss:   5.60, perplexity: 271.77\n",
            "[Epoch 5  ] loss:   5.63, perplexity: 277.99\n",
            "[Epoch 5  ] loss:   5.65, perplexity: 283.66\n",
            "[Epoch 5  ] loss:   5.66, perplexity: 286.24\n",
            "[Epoch 5  ] loss:   5.66, perplexity: 288.42\n",
            "[Epoch 5  ] loss:   5.67, perplexity: 289.93\n",
            "\n",
            "[Epoch 5  ] ended with train_loss:   5.67, ppl: 289.93\n",
            "[Epoch 5  ] ended with valid_loss:   5.57, valid_ppl: 261.38\n",
            "[Epoch 5  ] completed in 30.24 seconds\n",
            "\n",
            " ---> Final test set performance:   5.48, test_ppl: 238.86\n",
            "C: 2 | valid_ppl: 257.7095441252696 | test_ppl: 238.13527120775404\n",
            "C: 1 | valid_ppl: 302.5698686445651 | test_ppl: 279.78428569222376\n",
            "C: 3 | valid_ppl: 247.10216253710217 | test_ppl: 228.70340188538876\n",
            "C: 5 | valid_ppl: 242.037191039878 | test_ppl: 224.19091708999622\n",
            "C: 7 | valid_ppl: 245.2260838660174 | test_ppl: 226.10964375308924\n",
            "C: 11 | valid_ppl: 261.3815946641577 | test_ppl: 238.85639247963198\n"
          ]
        }
      ],
      "source": [
        "fix_seed(30494)\n",
        "\n",
        "def run_with_context_size(C=2):\n",
        "  fflm_model = FFLM(\n",
        "      len(vocab),       # vocabulary size\n",
        "      emb_dim=128,      # word embedding dim\n",
        "      hid_dim=128,      # hidden layer dim\n",
        "      context_size=C,   # C = (N-1) if you think in n-gram LM terminology\n",
        "      dropout=0.4,      # dropout probability\n",
        "  )\n",
        "\n",
        "  # move to device\n",
        "  fflm_model.to(DEVICE)\n",
        "\n",
        "  # Initial learning rate for the optimizer\n",
        "  FFLM_INIT_LR = 0.001\n",
        "\n",
        "  # Create the optimizer\n",
        "  fflm_optimizer = torch.optim.Adam(fflm_model.parameters(), lr=FFLM_INIT_LR)\n",
        "  print(fflm_model)\n",
        "\n",
        "  print('Starting training!')\n",
        "  # NOTE: If you happen to have memory errors, try decreasing the batch size\n",
        "  # It will print progress every 1000 batches\n",
        "  fflm_model.train_model(fflm_optimizer, train, valid, test, n_epochs=5, batch_size=256, shuffle=True)\n",
        "  return fflm_model\n",
        "\n",
        "results = [] # (C, valid_ppl, test_ppl)\n",
        "for i in [2,1,3,5,7,11]:\n",
        "  # for 2 we can see above\n",
        "  model = run_with_context_size(i)\n",
        "  _, valid_ppl = model.evaluate(test_set=valid)\n",
        "  _, test_ppl = model.evaluate(test_set=test)\n",
        "  results.append((i, valid_ppl, test_ppl))\n",
        "\n",
        "for (i, valid_ppl, test_ppl) in results:\n",
        "  print(f\"C: {i} | valid_ppl: {valid_ppl} | test_ppl: {test_ppl}\")\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5cOyX2Kf6td"
      },
      "source": [
        "## Recurrent Language Models (RNNLM)\n",
        "\n",
        "It is now time to switch to more complex LMs, basically the recurrent ones which have access to large context windows. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG3k2NS8qFYk"
      },
      "source": [
        "### Model definition\n",
        "\n",
        "You will notice that apart from the `train_model()` and `get_batches()` methods, the remaining parts look similar to `FFLM`. Take your time to compare both models. Read `get_batches()` thoroughly as preparing the batches and chunking into further fragments for BPTT is pretty interesting and may be unintuitive at first look.\n",
        "\n",
        "---\n",
        "\n",
        "**Q: Follow the comments in the `RNNLM` class to fill in the missing parts with some actual code. You will come back here when you reach the question regarding the support for LSTM. So, skip LSTM-related TODO's for now.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "b7FcID_kf6te"
      },
      "outputs": [],
      "source": [
        "class RNNLM(nn.Module):\n",
        "  \"\"\"RNN-based LM module.\"\"\"\n",
        "  def __init__(self, vocab_size, emb_dim, hid_dim, rnn_type='RNN',\n",
        "               n_layers=1, dropout=0.5, clip_gradient_norm=1.0,\n",
        "               bptt_steps=35):\n",
        "    # Call parent's __init__ first\n",
        "    super(RNNLM, self).__init__()\n",
        "    \n",
        "    # Store arguments\n",
        "    self.vocab_size = vocab_size\n",
        "    self.emb_dim = emb_dim\n",
        "    self.hid_dim = hid_dim\n",
        "    self.clip_gradient_norm = clip_gradient_norm\n",
        "    self.bptt_steps = bptt_steps\n",
        "    self.n_layers = n_layers\n",
        "    self.rnn_type = rnn_type.upper()\n",
        "\n",
        "    # This will be used to store the detached histories for truncated BPTT\n",
        "    self.prev_histories = None\n",
        "  \n",
        "    # Create the loss, don't sum or average, we'll take care of it\n",
        "    # in the training loop for logging purposes\n",
        "    self.loss = nn.CrossEntropyLoss(reduction='none')\n",
        "    \n",
        "    # Create the dropout\n",
        "    self.drop = nn.Dropout(p=dropout)\n",
        "    \n",
        "    # Create the embedding layer as usual\n",
        "    self.emb = nn.Embedding(\n",
        "      num_embeddings=self.vocab_size, embedding_dim=self.emb_dim,\n",
        "      padding_idx=0)\n",
        "    \n",
        "    # Create the RNN layer\n",
        "    if self.rnn_type == 'RNN':\n",
        "      self.rnn = nn.RNN( \n",
        "          input_size=self.emb_dim, hidden_size=self.hid_dim,\n",
        "          num_layers=self.n_layers, nonlinearity='tanh')\n",
        "    elif self.rnn_type == 'GRU':\n",
        "      self.rnn = nn.GRU(\n",
        "          input_size=self.emb_dim, hidden_size=self.hid_dim,\n",
        "          num_layers=self.n_layers)\n",
        "    elif self.rnn_type == 'LSTM':\n",
        "      #####################################\n",
        "      # Q: Fill in to create the LSTM layer\n",
        "      #####################################\n",
        "      # raise NotImplementedError()\n",
        "      # self.rnn = \"<TODO>\"\n",
        "      self.rnn = nn.LSTM(\n",
        "          input_size=self.emb_dim, hidden_size=self.hid_dim,\n",
        "          num_layers=self.n_layers)\n",
        "    \n",
        "    # Create the output layer: maps the hidden state of the RNN to vocabulary\n",
        "    self.out = nn.Linear(self.hid_dim, self.vocab_size)\n",
        "\n",
        "    # Compute number of parameters for information\n",
        "    self.n_params = 0\n",
        "    for param in self.parameters():\n",
        "      self.n_params += np.cumprod(param.data.size())[-1]\n",
        "    self.n_params = readable_size(self.n_params)\n",
        "\n",
        "  def init_state(self, batch_size):\n",
        "    \"\"\"Returns the initial 0 states.\"\"\"\n",
        "    if self.rnn_type != 'LSTM':\n",
        "      # for every layer and every sample -> 0 hidden state vector\n",
        "      return torch.zeros(self.n_layers, batch_size, self.hid_dim, device=DEVICE)\n",
        "    else:\n",
        "      #################################################################\n",
        "      # Q: Adapt the above snippet to LSTM. Check PyTorch docs\n",
        "      # to understand what is the expectation of LSTM's forward() call\n",
        "      # in terms of initial states.\n",
        "      #################################################################\n",
        "      #return \"<TODO>\"\n",
        "      h_0 = torch.zeros(self.n_layers, batch_size, self.hid_dim,\n",
        "                          device=DEVICE)\n",
        "      c_0 = torch.zeros(self.n_layers, batch_size, self.hid_dim,\n",
        "                          device=DEVICE)\n",
        "      return (h_0, c_0)\n",
        "\n",
        "  def clear_hidden_states(self):\n",
        "    \"\"\"Set the relevant instance attribute to None.\"\"\"\n",
        "    self.prev_histories = None\n",
        "\n",
        "  def save_hidden_states(self, last_states):\n",
        "    \"\"\"Save the detached states into the model for the next batch. `last_states`\n",
        "    is the second return value of RNN/GRU/LSTM's forward() methods.\"\"\"\n",
        "    if isinstance(last_states, tuple):\n",
        "      # This is true for LSTM\n",
        "      self.prev_histories = tuple(r.detach() for r in last_states)\n",
        "    else:\n",
        "      self.prev_histories = last_states.detach()\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    \"\"\"Forward-pass of the module.\"\"\"\n",
        "    # Detached previous histories for a batch. If `None`, we assume\n",
        "    # start of an epoch or start of an evaluation and create 0\n",
        "    # vector(s) to start with.\n",
        "    if self.prev_histories is None:\n",
        "      self.prev_histories = self.init_state(x.shape[1])\n",
        "\n",
        "    # Tokens -> Embeddings -> Dropout\n",
        "    embs = self.drop(self.emb(x))\n",
        "\n",
        "    # an RNN in PyTorch returns two values:\n",
        "    # (1) All hidden states of the last RNN layer\n",
        "    #     Shape -> (bptt_steps, batch_size, hid_dim)\n",
        "    #     You'll plug the output layer on top of this to obtain\n",
        "    #     the logits for each prediction.\n",
        "    # (2) Hidden state h_t of last timestep for EVERY layer\n",
        "    #     Shape -> (self.n_layers, batch_size, hid_dim)\n",
        "    #     This is what we'll store as the previous history\n",
        "    #     (NOTE: this is a tuple for LSTM which contains h_t and c_t)\n",
        "    all_hids, last_hid = self.rnn(embs, self.prev_histories)\n",
        "\n",
        "    # Detach the computation graph since we are done with BPTT for this batch\n",
        "    self.save_hidden_states(last_hid)\n",
        "\n",
        "    ##########################################################\n",
        "    # Q: Apply dropout on all_hids and pass it to output layer\n",
        "    ##########################################################\n",
        "    #logits = \"<TODO>\"\n",
        "    logits = self.out(self.drop(all_hids))\n",
        "\n",
        "    # Return the losses per token/position\n",
        "    return self.loss(logits.view(-1, self.vocab_size), y)\n",
        "\n",
        "  def get_batches(self, data_tensor, batch_size):\n",
        "    # NOTE: There is absolutely no shuffling here, which\n",
        "    # will totally break the histories coming from previous steps.\n",
        "    # The document is evenly divided into independent `batch_size` portions.\n",
        "    # At every iteration, the BPTT window will slide over each of these\n",
        "    # portions, by keeping track of the previous h_t's as discussed\n",
        "    # in the lecture.\n",
        "    \n",
        "    # Imagine this as `batch_size` pointers running over the text, each\n",
        "    # processing its share in a continuous. Although the portions may have\n",
        "    # been splitted in a noisy way (one pointer can be starting from the\n",
        "    # middle of a sentence for example), this makes training faster.\n",
        "    # For instance, with the alphabet as the dataset and batch size 4, we'd get\n",
        "    # ┌ a g m s ┐\n",
        "    # │ b h n t │\n",
        "    # │ c i o u │\n",
        "    # │ d j p v │\n",
        "    # │ e k q w │\n",
        "    # └ f l r x ┘.\n",
        "    # These columns are treated as \"independent\" by the model, which means that\n",
        "    # the dependence of 'g' on 'f' can not be learned, but allows more efficient\n",
        "    # batch processing. The view above will further be splitted into chunks\n",
        "    # of size `bptt_steps` to apply truncated BPTT. For example, with\n",
        "    # `bptt_steps == 2`, we'll have the following `x` and `y` tensors. The\n",
        "    # first batch will be processing \"a, b\" to predict \"b, c\", \n",
        "    # the second batch will be processing \"g, h\" to predict \"h, i\", and so on.\n",
        "    #\n",
        "    #       X          Y\n",
        "    #   ----->>------\n",
        "    #   |           |\n",
        "    # ┌ a g m s ┐ ┌ b h n t ┐\n",
        "    # └ b h n t ┘ └ c i o u ┘\n",
        "    #   |           |\n",
        "    #   ----->>------\n",
        "\n",
        "    # Work out how cleanly we can divide the dataset into batch_size parts.\n",
        "    n_batches = data_tensor.size(0) // batch_size\n",
        "\n",
        "    # Trim off the remainder tokens to evenly split\n",
        "    # Evenly divide the data across the batches\n",
        "    data = data_tensor[:n_batches * batch_size].view(\n",
        "        batch_size, n_batches).t().contiguous()\n",
        "\n",
        "    batches = []\n",
        "\n",
        "    for i in range(0, data.size(0) - 1, self.bptt_steps):\n",
        "      # seq_len can be less than bptt_steps in the final parts of the data\n",
        "      seq_len = min(self.bptt_steps, len(data) - i - 1)\n",
        "\n",
        "      # x shape => (seq_len, batch_size)\n",
        "      x = data[i: i + seq_len]\n",
        "      # flatten the ground-truth labels (shifted inputs for LM)\n",
        "      y = data[i + 1: i + 1 + seq_len].view(-1)\n",
        "      batches.append((x, y))\n",
        "\n",
        "    return batches\n",
        " \n",
        "  def train_model(self, optim, train_tensor, valid_tensor, test_tensor, n_epochs=5,\n",
        "                 batch_size=64):\n",
        "    \"\"\"Trains the model.\"\"\"\n",
        "    # Get batches for all splits at once\n",
        "    train_batches = self.get_batches(train_tensor, batch_size)\n",
        "    valid_batches = self.get_batches(valid_tensor, batch_size)\n",
        "    test_batches = self.get_batches(test_tensor, batch_size)\n",
        "\n",
        "    for eidx in range(1, n_epochs + 1):\n",
        "      start_time = time.time()\n",
        "      epoch_loss = 0\n",
        "      epoch_items = 0\n",
        "\n",
        "      # Enable training mode\n",
        "      self.train()\n",
        "\n",
        "      # Start training\n",
        "      for iter_count, (x, y) in enumerate(train_batches):\n",
        "        # Clear the gradients\n",
        "        optim.zero_grad()\n",
        "\n",
        "        loss = self.forward(x.to(DEVICE), y.to(DEVICE))\n",
        "\n",
        "        # Backprop the average loss and update parameters\n",
        "        loss.mean().backward()\n",
        "\n",
        "        # Clip the gradients to avoid exploding gradients\n",
        "        if self.clip_gradient_norm > 0:\n",
        "          torch.nn.utils.clip_grad_norm_(self.parameters(), self.clip_gradient_norm)\n",
        "\n",
        "        # Update parameters\n",
        "        optim.step()\n",
        "\n",
        "        # sum the loss for reporting, along with the denominator\n",
        "        epoch_loss += loss.detach().sum()\n",
        "        epoch_items += loss.numel()\n",
        "\n",
        "        if iter_count % 500 == 0:\n",
        "          # Print progress\n",
        "          loss_per_token = epoch_loss / epoch_items\n",
        "          ppl = math.exp(loss_per_token)\n",
        "          print(f'[Epoch {eidx:<3}] loss: {loss_per_token:6.2f}, perplexity: {ppl:6.2f}')\n",
        "\n",
        "      time_spent = time.time() - start_time\n",
        "\n",
        "      # Clear stale h_t history before evaluation\n",
        "      self.clear_hidden_states()\n",
        "\n",
        "      print(f'\\n[Epoch {eidx:<3}] ended with train_loss: {loss_per_token:6.2f}, ppl: {ppl:6.2f}')\n",
        "      # Evaluate on valid set\n",
        "      valid_loss, valid_ppl = self.evaluate(valid_batches)\n",
        "      print(f'[Epoch {eidx:<3}] ended with valid_loss: {valid_loss:6.2f}, valid_ppl: {valid_ppl:6.2f}')\n",
        "      print(f'[Epoch {eidx:<3}] completed in {time_spent:.2f} seconds\\n')\n",
        "\n",
        "    # Evaluate the final model on test set\n",
        "    test_loss, test_ppl = self.evaluate(test_batches)\n",
        "    print(f' ---> Final test set performance: {test_loss:6.2f}, test_ppl: {test_ppl:6.2f}')\n",
        "\n",
        "  def evaluate(self, batches):\n",
        "    # Clear stale h_t history before evaluation\n",
        "    self.clear_hidden_states()\n",
        "\n",
        "    # Switch to eval mode\n",
        "    self.eval()\n",
        "\n",
        "    total_loss = 0.\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for iter_count, (x, y) in enumerate(batches):\n",
        "        loss = self.forward(x.to(DEVICE), y.to(DEVICE))\n",
        "        \n",
        "        total_loss += loss.sum().item()\n",
        "        total_tokens += loss.size(0)\n",
        "    total_loss /= total_tokens\n",
        "\n",
        "    self.clear_hidden_states()\n",
        "    return total_loss, math.exp(total_loss)\n",
        "\n",
        "  def __repr__(self):\n",
        "    \"\"\"String representation for pretty-printing.\"\"\"\n",
        "    s = super(RNNLM, self).__repr__()\n",
        "    return \"{}\\n# of parameters: {}\".format(s, self.n_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beEYKqQNf6tj"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "3kzOvvY6f6tx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNNLM(\n",
            "  (loss): CrossEntropyLoss()\n",
            "  (drop): Dropout(p=0.4, inplace=False)\n",
            "  (emb): Embedding(33233, 128, padding_idx=0)\n",
            "  (rnn): GRU(128, 128)\n",
            "  (out): Linear(in_features=128, out_features=33233, bias=True)\n",
            ")\n",
            "# of parameters: 8.64M\n",
            "Starting training!\n",
            "[Epoch 1  ] loss:  10.43, perplexity: 33835.70\n",
            "[Epoch 1  ] loss:   7.06, perplexity: 1160.65\n",
            "[Epoch 1  ] loss:   6.81, perplexity: 908.58\n",
            "[Epoch 1  ] loss:   6.68, perplexity: 798.14\n",
            "[Epoch 1  ] loss:   6.59, perplexity: 731.22\n",
            "[Epoch 1  ] loss:   6.52, perplexity: 680.76\n",
            "[Epoch 1  ] loss:   6.47, perplexity: 643.54\n",
            "[Epoch 1  ] loss:   6.42, perplexity: 611.26\n",
            "\n",
            "[Epoch 1  ] ended with train_loss:   6.42, ppl: 611.26\n",
            "[Epoch 1  ] ended with valid_loss:   5.70, valid_ppl: 299.29\n",
            "[Epoch 1  ] completed in 20.69 seconds\n",
            "\n",
            "[Epoch 2  ] loss:   6.48, perplexity: 648.90\n",
            "[Epoch 2  ] loss:   5.92, perplexity: 372.33\n",
            "[Epoch 2  ] loss:   5.90, perplexity: 363.57\n",
            "[Epoch 2  ] loss:   5.88, perplexity: 356.43\n",
            "[Epoch 2  ] loss:   5.86, perplexity: 350.99\n",
            "[Epoch 2  ] loss:   5.84, perplexity: 344.07\n",
            "[Epoch 2  ] loss:   5.83, perplexity: 339.93\n",
            "[Epoch 2  ] loss:   5.81, perplexity: 334.48\n",
            "\n",
            "[Epoch 2  ] ended with train_loss:   5.81, ppl: 334.48\n",
            "[Epoch 2  ] ended with valid_loss:   5.48, valid_ppl: 239.77\n",
            "[Epoch 2  ] completed in 21.10 seconds\n",
            "\n",
            "[Epoch 3  ] loss:   6.25, perplexity: 519.01\n",
            "[Epoch 3  ] loss:   5.66, perplexity: 286.59\n",
            "[Epoch 3  ] loss:   5.64, perplexity: 282.19\n",
            "[Epoch 3  ] loss:   5.63, perplexity: 278.11\n",
            "[Epoch 3  ] loss:   5.62, perplexity: 275.69\n",
            "[Epoch 3  ] loss:   5.60, perplexity: 271.02\n",
            "[Epoch 3  ] loss:   5.60, perplexity: 269.29\n",
            "[Epoch 3  ] loss:   5.59, perplexity: 266.45\n",
            "\n",
            "[Epoch 3  ] ended with train_loss:   5.59, ppl: 266.45\n",
            "[Epoch 3  ] ended with valid_loss:   5.38, valid_ppl: 216.61\n",
            "[Epoch 3  ] completed in 21.40 seconds\n",
            "\n",
            "[Epoch 4  ] loss:   6.06, perplexity: 430.22\n",
            "[Epoch 4  ] loss:   5.49, perplexity: 242.42\n",
            "[Epoch 4  ] loss:   5.48, perplexity: 239.07\n",
            "[Epoch 4  ] loss:   5.47, perplexity: 236.63\n",
            "[Epoch 4  ] loss:   5.46, perplexity: 235.84\n",
            "[Epoch 4  ] loss:   5.45, perplexity: 232.62\n",
            "[Epoch 4  ] loss:   5.45, perplexity: 232.03\n",
            "[Epoch 4  ] loss:   5.44, perplexity: 229.97\n",
            "\n",
            "[Epoch 4  ] ended with train_loss:   5.44, ppl: 229.97\n",
            "[Epoch 4  ] ended with valid_loss:   5.33, valid_ppl: 206.94\n",
            "[Epoch 4  ] completed in 21.47 seconds\n",
            "\n",
            "[Epoch 5  ] loss:   5.93, perplexity: 376.19\n",
            "[Epoch 5  ] loss:   5.37, perplexity: 215.50\n",
            "[Epoch 5  ] loss:   5.36, perplexity: 213.71\n",
            "[Epoch 5  ] loss:   5.36, perplexity: 212.09\n",
            "[Epoch 5  ] loss:   5.35, perplexity: 211.52\n",
            "[Epoch 5  ] loss:   5.34, perplexity: 208.64\n",
            "[Epoch 5  ] loss:   5.34, perplexity: 208.45\n",
            "[Epoch 5  ] loss:   5.33, perplexity: 206.96\n",
            "\n",
            "[Epoch 5  ] ended with train_loss:   5.33, ppl: 206.96\n",
            "[Epoch 5  ] ended with valid_loss:   5.29, valid_ppl: 198.77\n",
            "[Epoch 5  ] completed in 21.44 seconds\n",
            "\n",
            " ---> Final test set performance:   5.22, test_ppl: 184.91\n"
          ]
        }
      ],
      "source": [
        "# Set the seed for reproducible results\n",
        "fix_seed(30494)\n",
        "\n",
        "rnnlm_model = RNNLM(\n",
        "    vocab_size=len(vocab),  # vocabulary size\n",
        "    emb_dim=128,            # word embedding dim\n",
        "    hid_dim=128,            # hidden layer dim\n",
        "    rnn_type='GRU',         # RNN type\n",
        "    n_layers=1,             # Number of stacked RNN layers\n",
        "    clip_gradient_norm=1.0, # gradient clip threshold\n",
        "    bptt_steps=35,          # Truncated BPTT window size\n",
        "    dropout=0.4,            # dropout probability\n",
        ")\n",
        "\n",
        "# move to device\n",
        "rnnlm_model.to(DEVICE)\n",
        "\n",
        "# Initial learning rate for the optimizer\n",
        "RNNLM_INIT_LR = 0.002\n",
        "\n",
        "# Create the optimizer\n",
        "rnnlm_optimizer = torch.optim.Adam(rnnlm_model.parameters(), lr=RNNLM_INIT_LR)\n",
        "print(rnnlm_model)\n",
        "\n",
        "print('Starting training!')\n",
        "# NOTE: If you happen to have memory errors, try decreasing the batch size\n",
        "rnnlm_model.train_model(\n",
        "    rnnlm_optimizer, train, valid, test, n_epochs=5, batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIhbuAPkaMds"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "---\n",
        "\n",
        "**Q: How does the results compare to FFLMs? Feel free to play with different hyper-parameters here as well. Especially, try with different BPTT steps.**\n",
        "\n",
        "**A:** *With almost the same model size (~8.5M parameters), the default hyper-parameters achieve a validation and test set perplexity of $198$ and $184$, respectively. The improvement is significant compared to FFLMs. As for the `bptt_steps`, increasing it to $60$ yields around the same perplexity for the valid and test set. Decreasing it to 16, makes the performance worse i.e. $210$ and $191$ for valid and test sets, respectively. Keep in mind that increasing `bptt_steps` decrease the number of parameter updates happening in a single epoch since we are delaying the `backward() -> optim.step()`. So it is not totally fair to compare these models unless we let them run until convergence.*\n",
        "\n",
        "**Q: Play with hyper-parameters related to dimensions and dropout. Could you find a model with smaller perplexity?**\n",
        "\n",
        "**A:** *Hyper-parameter search is a key notion for obtaining good models. The set of defaults given in this lab are purely empirical, so you could indeed find better hyper-parameters which will yield lower perplexities. Feel free to share your findings with us if something interesting comes out :)*\n",
        "\n",
        "**Q: There are missing parts related to LSTM support in the implementation. Try filling those parts and train an LSTM-based model. How does the performance compare to vanilla RNN and GRU? Compare model sizes of three variants with the default set of hyper-parameters.**\n",
        "\n",
        "**A:** *With the default set of hyper-parameters, these are the approximate results that you should obtain with the given random seed. Once again, this does not mean that GRU is better than LSTM or vice-versa. We are just training with some pre-determined hyperparameters and for only 5 epochs!*\n",
        "\n",
        "| RNN     | Valid PPL | Test PPL |\n",
        "|---------|-----------|----------|\n",
        "| Vanilla | 263       | 241      |\n",
        "| GRU     | 198       | 184      |\n",
        "| LSTM    | 213       | 196      |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "JNKQOLoqZydr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNNLM(\n",
            "  (loss): CrossEntropyLoss()\n",
            "  (drop): Dropout(p=0.4, inplace=False)\n",
            "  (emb): Embedding(33233, 128, padding_idx=0)\n",
            "  (rnn): GRU(128, 128)\n",
            "  (out): Linear(in_features=128, out_features=33233, bias=True)\n",
            ")\n",
            "# of parameters: 8.64M\n",
            "Starting training!\n",
            "[Epoch 1  ] loss:  10.42, perplexity: 33600.99\n",
            "[Epoch 1  ] loss:   7.15, perplexity: 1277.47\n",
            "[Epoch 1  ] loss:   6.92, perplexity: 1014.68\n",
            "[Epoch 1  ] loss:   6.80, perplexity: 895.42\n",
            "[Epoch 1  ] loss:   6.73, perplexity: 833.28\n",
            "[Epoch 1  ] loss:   6.68, perplexity: 793.06\n",
            "[Epoch 1  ] loss:   6.63, perplexity: 756.44\n",
            "[Epoch 1  ] loss:   6.58, perplexity: 723.34\n",
            "[Epoch 1  ] loss:   6.56, perplexity: 704.62\n",
            "[Epoch 1  ] loss:   6.52, perplexity: 681.00\n",
            "[Epoch 1  ] loss:   6.50, perplexity: 664.62\n",
            "[Epoch 1  ] loss:   6.47, perplexity: 646.48\n",
            "[Epoch 1  ] loss:   6.45, perplexity: 634.48\n",
            "[Epoch 1  ] loss:   6.43, perplexity: 618.30\n",
            "[Epoch 1  ] loss:   6.41, perplexity: 606.38\n",
            "[Epoch 1  ] loss:   6.39, perplexity: 593.99\n",
            "\n",
            "[Epoch 1  ] ended with train_loss:   6.39, ppl: 593.99\n",
            "[Epoch 1  ] ended with valid_loss:   5.70, valid_ppl: 297.41\n",
            "[Epoch 1  ] completed in 32.90 seconds\n",
            "\n",
            "[Epoch 2  ] loss:   6.48, perplexity: 649.24\n",
            "[Epoch 2  ] loss:   5.93, perplexity: 374.36\n",
            "[Epoch 2  ] loss:   5.90, perplexity: 364.80\n",
            "[Epoch 2  ] loss:   5.89, perplexity: 359.86\n",
            "[Epoch 2  ] loss:   5.89, perplexity: 359.73\n",
            "[Epoch 2  ] loss:   5.89, perplexity: 361.42\n",
            "[Epoch 2  ] loss:   5.88, perplexity: 356.59\n",
            "[Epoch 2  ] loss:   5.87, perplexity: 353.31\n",
            "[Epoch 2  ] loss:   5.87, perplexity: 354.07\n",
            "[Epoch 2  ] loss:   5.86, perplexity: 350.59\n",
            "[Epoch 2  ] loss:   5.85, perplexity: 348.61\n",
            "[Epoch 2  ] loss:   5.85, perplexity: 346.31\n",
            "[Epoch 2  ] loss:   5.85, perplexity: 346.42\n",
            "[Epoch 2  ] loss:   5.84, perplexity: 343.50\n",
            "[Epoch 2  ] loss:   5.84, perplexity: 342.22\n",
            "[Epoch 2  ] loss:   5.83, perplexity: 340.01\n",
            "\n",
            "[Epoch 2  ] ended with train_loss:   5.83, ppl: 340.01\n",
            "[Epoch 2  ] ended with valid_loss:   5.53, valid_ppl: 252.55\n",
            "[Epoch 2  ] completed in 33.23 seconds\n",
            "\n",
            "[Epoch 3  ] loss:   6.28, perplexity: 531.69\n",
            "[Epoch 3  ] loss:   5.71, perplexity: 303.03\n",
            "[Epoch 3  ] loss:   5.69, perplexity: 296.61\n",
            "[Epoch 3  ] loss:   5.68, perplexity: 293.91\n",
            "[Epoch 3  ] loss:   5.69, perplexity: 294.42\n",
            "[Epoch 3  ] loss:   5.69, perplexity: 296.86\n",
            "[Epoch 3  ] loss:   5.68, perplexity: 293.00\n",
            "[Epoch 3  ] loss:   5.67, perplexity: 290.81\n",
            "[Epoch 3  ] loss:   5.68, perplexity: 291.82\n",
            "[Epoch 3  ] loss:   5.67, perplexity: 289.40\n",
            "[Epoch 3  ] loss:   5.66, perplexity: 288.15\n",
            "[Epoch 3  ] loss:   5.66, perplexity: 286.88\n",
            "[Epoch 3  ] loss:   5.66, perplexity: 287.54\n",
            "[Epoch 3  ] loss:   5.65, perplexity: 285.67\n",
            "[Epoch 3  ] loss:   5.65, perplexity: 284.95\n",
            "[Epoch 3  ] loss:   5.65, perplexity: 283.67\n",
            "\n",
            "[Epoch 3  ] ended with train_loss:   5.65, ppl: 283.67\n",
            "[Epoch 3  ] ended with valid_loss:   5.44, valid_ppl: 230.35\n",
            "[Epoch 3  ] completed in 32.90 seconds\n",
            "\n",
            "[Epoch 4  ] loss:   6.15, perplexity: 470.23\n",
            "[Epoch 4  ] loss:   5.59, perplexity: 266.79\n",
            "[Epoch 4  ] loss:   5.57, perplexity: 262.04\n",
            "[Epoch 4  ] loss:   5.56, perplexity: 260.64\n",
            "[Epoch 4  ] loss:   5.57, perplexity: 261.80\n",
            "[Epoch 4  ] loss:   5.58, perplexity: 264.21\n",
            "[Epoch 4  ] loss:   5.56, perplexity: 260.86\n",
            "[Epoch 4  ] loss:   5.56, perplexity: 259.48\n",
            "[Epoch 4  ] loss:   5.56, perplexity: 260.44\n",
            "[Epoch 4  ] loss:   5.56, perplexity: 258.72\n",
            "[Epoch 4  ] loss:   5.55, perplexity: 257.65\n",
            "[Epoch 4  ] loss:   5.55, perplexity: 256.94\n",
            "[Epoch 4  ] loss:   5.55, perplexity: 257.76\n",
            "[Epoch 4  ] loss:   5.55, perplexity: 256.52\n",
            "[Epoch 4  ] loss:   5.55, perplexity: 256.07\n",
            "[Epoch 4  ] loss:   5.54, perplexity: 255.14\n",
            "\n",
            "[Epoch 4  ] ended with train_loss:   5.54, ppl: 255.14\n",
            "[Epoch 4  ] ended with valid_loss:   5.39, valid_ppl: 218.61\n",
            "[Epoch 4  ] completed in 32.96 seconds\n",
            "\n",
            "[Epoch 5  ] loss:   6.02, perplexity: 412.28\n",
            "[Epoch 5  ] loss:   5.50, perplexity: 244.12\n",
            "[Epoch 5  ] loss:   5.49, perplexity: 241.10\n",
            "[Epoch 5  ] loss:   5.48, perplexity: 240.65\n",
            "[Epoch 5  ] loss:   5.49, perplexity: 241.50\n",
            "[Epoch 5  ] loss:   5.50, perplexity: 244.37\n",
            "[Epoch 5  ] loss:   5.49, perplexity: 241.44\n",
            "[Epoch 5  ] loss:   5.48, perplexity: 240.33\n",
            "[Epoch 5  ] loss:   5.49, perplexity: 241.41\n",
            "[Epoch 5  ] loss:   5.48, perplexity: 239.88\n",
            "[Epoch 5  ] loss:   5.48, perplexity: 238.87\n",
            "[Epoch 5  ] loss:   5.47, perplexity: 238.18\n",
            "[Epoch 5  ] loss:   5.48, perplexity: 239.16\n",
            "[Epoch 5  ] loss:   5.47, perplexity: 238.18\n",
            "[Epoch 5  ] loss:   5.47, perplexity: 237.77\n",
            "[Epoch 5  ] loss:   5.47, perplexity: 237.00\n",
            "\n",
            "[Epoch 5  ] ended with train_loss:   5.47, ppl: 237.00\n",
            "[Epoch 5  ] ended with valid_loss:   5.35, valid_ppl: 210.32\n",
            "[Epoch 5  ] completed in 32.93 seconds\n",
            "\n",
            " ---> Final test set performance:   5.25, test_ppl: 191.17\n",
            "RNNLM(\n",
            "  (loss): CrossEntropyLoss()\n",
            "  (drop): Dropout(p=0.4, inplace=False)\n",
            "  (emb): Embedding(33233, 128, padding_idx=0)\n",
            "  (rnn): GRU(128, 128)\n",
            "  (out): Linear(in_features=128, out_features=33233, bias=True)\n",
            ")\n",
            "# of parameters: 8.64M\n",
            "Starting training!\n",
            "[Epoch 1  ] loss:  10.43, perplexity: 33957.31\n",
            "[Epoch 1  ] loss:   6.98, perplexity: 1069.59\n",
            "[Epoch 1  ] loss:   6.73, perplexity: 836.40\n",
            "[Epoch 1  ] loss:   6.59, perplexity: 728.78\n",
            "[Epoch 1  ] loss:   6.49, perplexity: 657.32\n",
            "\n",
            "[Epoch 1  ] ended with train_loss:   6.49, ppl: 657.32\n",
            "[Epoch 1  ] ended with valid_loss:   5.74, valid_ppl: 312.48\n",
            "[Epoch 1  ] completed in 17.69 seconds\n",
            "\n",
            "[Epoch 2  ] loss:   6.42, perplexity: 615.03\n",
            "[Epoch 2  ] loss:   5.97, perplexity: 390.14\n",
            "[Epoch 2  ] loss:   5.93, perplexity: 376.50\n",
            "[Epoch 2  ] loss:   5.90, perplexity: 365.85\n",
            "[Epoch 2  ] loss:   5.87, perplexity: 354.85\n",
            "\n",
            "[Epoch 2  ] ended with train_loss:   5.87, ppl: 354.85\n",
            "[Epoch 2  ] ended with valid_loss:   5.53, valid_ppl: 251.52\n",
            "[Epoch 2  ] completed in 17.69 seconds\n",
            "\n",
            "[Epoch 3  ] loss:   6.15, perplexity: 470.22\n",
            "[Epoch 3  ] loss:   5.70, perplexity: 298.38\n",
            "[Epoch 3  ] loss:   5.67, perplexity: 290.15\n",
            "[Epoch 3  ] loss:   5.65, perplexity: 283.83\n",
            "[Epoch 3  ] loss:   5.63, perplexity: 278.32\n",
            "\n",
            "[Epoch 3  ] ended with train_loss:   5.63, ppl: 278.32\n",
            "[Epoch 3  ] ended with valid_loss:   5.41, valid_ppl: 222.66\n",
            "[Epoch 3  ] completed in 17.70 seconds\n",
            "\n",
            "[Epoch 4  ] loss:   5.98, perplexity: 395.75\n",
            "[Epoch 4  ] loss:   5.52, perplexity: 249.60\n",
            "[Epoch 4  ] loss:   5.50, perplexity: 244.15\n",
            "[Epoch 4  ] loss:   5.48, perplexity: 239.46\n",
            "[Epoch 4  ] loss:   5.46, perplexity: 235.95\n",
            "\n",
            "[Epoch 4  ] ended with train_loss:   5.46, ppl: 235.95\n",
            "[Epoch 4  ] ended with valid_loss:   5.34, valid_ppl: 209.41\n",
            "[Epoch 4  ] completed in 17.70 seconds\n",
            "\n",
            "[Epoch 5  ] loss:   5.84, perplexity: 342.14\n",
            "[Epoch 5  ] loss:   5.39, perplexity: 219.21\n",
            "[Epoch 5  ] loss:   5.37, perplexity: 215.53\n",
            "[Epoch 5  ] loss:   5.36, perplexity: 211.73\n",
            "[Epoch 5  ] loss:   5.34, perplexity: 209.37\n",
            "\n",
            "[Epoch 5  ] ended with train_loss:   5.34, ppl: 209.37\n",
            "[Epoch 5  ] ended with valid_loss:   5.30, valid_ppl: 200.43\n",
            "[Epoch 5  ] completed in 17.71 seconds\n",
            "\n",
            " ---> Final test set performance:   5.22, test_ppl: 184.54\n"
          ]
        }
      ],
      "source": [
        "# Set the seed for reproducible results\n",
        "fix_seed(30494)\n",
        "\n",
        "def run_with_bptt(bptt_steps):\n",
        "  rnnlm_model = RNNLM(\n",
        "      vocab_size=len(vocab),  # vocabulary size\n",
        "      emb_dim=128,            # word embedding dim\n",
        "      hid_dim=128,            # hidden layer dim\n",
        "      rnn_type='GRU',         # RNN type\n",
        "      n_layers=1,             # Number of stacked RNN layers\n",
        "      clip_gradient_norm=1.0, # gradient clip threshold\n",
        "      bptt_steps=bptt_steps,          # Truncated BPTT window size\n",
        "      dropout=0.4,            # dropout probability\n",
        "  )\n",
        "\n",
        "  # move to device\n",
        "  rnnlm_model.to(DEVICE)\n",
        "\n",
        "  # Initial learning rate for the optimizer\n",
        "  RNNLM_INIT_LR = 0.002\n",
        "\n",
        "  # Create the optimizer\n",
        "  rnnlm_optimizer = torch.optim.Adam(rnnlm_model.parameters(), lr=RNNLM_INIT_LR)\n",
        "  print(rnnlm_model)\n",
        "\n",
        "  print('Starting training!')\n",
        "  # NOTE: If you happen to have memory errors, try decreasing the batch size\n",
        "  rnnlm_model.train_model(\n",
        "      rnnlm_optimizer, train, valid, test, n_epochs=5, batch_size=16)\n",
        "  return rnnlm_model\n",
        "  \n",
        "for i in [16,60]:\n",
        "  model = run_with_bptt(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "LJ0YWGBP2aZr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNNLM(\n",
            "  (loss): CrossEntropyLoss()\n",
            "  (drop): Dropout(p=0.4, inplace=False)\n",
            "  (emb): Embedding(33233, 128, padding_idx=0)\n",
            "  (rnn): LSTM(128, 128)\n",
            "  (out): Linear(in_features=128, out_features=33233, bias=True)\n",
            ")\n",
            "# of parameters: 8.67M\n",
            "Starting training!\n",
            "[Epoch 1  ] loss:  10.41, perplexity: 33316.11\n",
            "[Epoch 1  ] loss:   7.05, perplexity: 1152.10\n",
            "[Epoch 1  ] loss:   6.85, perplexity: 948.37\n",
            "[Epoch 1  ] loss:   6.75, perplexity: 854.02\n",
            "[Epoch 1  ] loss:   6.67, perplexity: 791.60\n",
            "[Epoch 1  ] loss:   6.61, perplexity: 746.17\n",
            "[Epoch 1  ] loss:   6.57, perplexity: 710.57\n",
            "[Epoch 1  ] loss:   6.52, perplexity: 678.61\n",
            "\n",
            "[Epoch 1  ] ended with train_loss:   6.52, ppl: 678.61\n",
            "[Epoch 1  ] ended with valid_loss:   5.78, valid_ppl: 324.77\n",
            "[Epoch 1  ] completed in 21.83 seconds\n",
            "\n",
            "[Epoch 2  ] loss:   6.40, perplexity: 602.39\n",
            "[Epoch 2  ] loss:   6.03, perplexity: 415.43\n",
            "[Epoch 2  ] loss:   6.01, perplexity: 406.54\n",
            "[Epoch 2  ] loss:   6.00, perplexity: 402.31\n",
            "[Epoch 2  ] loss:   5.99, perplexity: 398.81\n",
            "[Epoch 2  ] loss:   5.98, perplexity: 393.58\n",
            "[Epoch 2  ] loss:   5.97, perplexity: 390.66\n",
            "[Epoch 2  ] loss:   5.95, perplexity: 385.53\n",
            "\n",
            "[Epoch 2  ] ended with train_loss:   5.95, ppl: 385.53\n",
            "[Epoch 2  ] ended with valid_loss:   5.58, valid_ppl: 265.58\n",
            "[Epoch 2  ] completed in 22.05 seconds\n",
            "\n",
            "[Epoch 3  ] loss:   6.16, perplexity: 474.46\n",
            "[Epoch 3  ] loss:   5.79, perplexity: 328.57\n",
            "[Epoch 3  ] loss:   5.78, perplexity: 322.38\n",
            "[Epoch 3  ] loss:   5.77, perplexity: 320.26\n",
            "[Epoch 3  ] loss:   5.76, perplexity: 318.81\n",
            "[Epoch 3  ] loss:   5.75, perplexity: 315.55\n",
            "[Epoch 3  ] loss:   5.75, perplexity: 314.50\n",
            "[Epoch 3  ] loss:   5.74, perplexity: 311.47\n",
            "\n",
            "[Epoch 3  ] ended with train_loss:   5.74, ppl: 311.47\n",
            "[Epoch 3  ] ended with valid_loss:   5.48, valid_ppl: 239.24\n",
            "[Epoch 3  ] completed in 22.50 seconds\n",
            "\n",
            "[Epoch 4  ] loss:   6.00, perplexity: 403.56\n",
            "[Epoch 4  ] loss:   5.64, perplexity: 280.95\n",
            "[Epoch 4  ] loss:   5.62, perplexity: 276.61\n",
            "[Epoch 4  ] loss:   5.62, perplexity: 275.30\n",
            "[Epoch 4  ] loss:   5.62, perplexity: 274.70\n",
            "[Epoch 4  ] loss:   5.61, perplexity: 272.19\n",
            "[Epoch 4  ] loss:   5.60, perplexity: 271.52\n",
            "[Epoch 4  ] loss:   5.60, perplexity: 269.42\n",
            "\n",
            "[Epoch 4  ] ended with train_loss:   5.60, ppl: 269.42\n",
            "[Epoch 4  ] ended with valid_loss:   5.41, valid_ppl: 224.15\n",
            "[Epoch 4  ] completed in 22.48 seconds\n",
            "\n",
            "[Epoch 5  ] loss:   5.93, perplexity: 374.97\n",
            "[Epoch 5  ] loss:   5.52, perplexity: 249.96\n",
            "[Epoch 5  ] loss:   5.51, perplexity: 246.29\n",
            "[Epoch 5  ] loss:   5.50, perplexity: 245.11\n",
            "[Epoch 5  ] loss:   5.50, perplexity: 244.51\n",
            "[Epoch 5  ] loss:   5.49, perplexity: 242.41\n",
            "[Epoch 5  ] loss:   5.49, perplexity: 242.00\n",
            "[Epoch 5  ] loss:   5.48, perplexity: 240.62\n",
            "\n",
            "[Epoch 5  ] ended with train_loss:   5.48, ppl: 240.62\n",
            "[Epoch 5  ] ended with valid_loss:   5.36, valid_ppl: 212.92\n",
            "[Epoch 5  ] completed in 22.23 seconds\n",
            "\n",
            " ---> Final test set performance:   5.28, test_ppl: 196.07\n",
            "RNNLM(\n",
            "  (loss): CrossEntropyLoss()\n",
            "  (drop): Dropout(p=0.4, inplace=False)\n",
            "  (emb): Embedding(33233, 128, padding_idx=0)\n",
            "  (rnn): RNN(128, 128)\n",
            "  (out): Linear(in_features=128, out_features=33233, bias=True)\n",
            ")\n",
            "# of parameters: 8.57M\n",
            "Starting training!\n",
            "[Epoch 1  ] loss:  10.53, perplexity: 37338.80\n",
            "[Epoch 1  ] loss:   7.08, perplexity: 1188.40\n",
            "[Epoch 1  ] loss:   6.90, perplexity: 988.37\n",
            "[Epoch 1  ] loss:   6.80, perplexity: 893.99\n",
            "[Epoch 1  ] loss:   6.73, perplexity: 833.23\n",
            "[Epoch 1  ] loss:   6.67, perplexity: 787.59\n",
            "[Epoch 1  ] loss:   6.62, perplexity: 752.84\n",
            "[Epoch 1  ] loss:   6.58, perplexity: 721.11\n",
            "\n",
            "[Epoch 1  ] ended with train_loss:   6.58, ppl: 721.11\n",
            "[Epoch 1  ] ended with valid_loss:   5.87, valid_ppl: 354.30\n",
            "[Epoch 1  ] completed in 21.13 seconds\n",
            "\n",
            "[Epoch 2  ] loss:   6.49, perplexity: 657.39\n",
            "[Epoch 2  ] loss:   6.08, perplexity: 438.49\n",
            "[Epoch 2  ] loss:   6.06, perplexity: 430.14\n",
            "[Epoch 2  ] loss:   6.06, perplexity: 426.95\n",
            "[Epoch 2  ] loss:   6.05, perplexity: 424.26\n",
            "[Epoch 2  ] loss:   6.04, perplexity: 419.64\n",
            "[Epoch 2  ] loss:   6.03, perplexity: 417.42\n",
            "[Epoch 2  ] loss:   6.02, perplexity: 412.68\n",
            "\n",
            "[Epoch 2  ] ended with train_loss:   6.02, ppl: 412.68\n",
            "[Epoch 2  ] ended with valid_loss:   5.72, valid_ppl: 304.27\n",
            "[Epoch 2  ] completed in 21.20 seconds\n",
            "\n",
            "[Epoch 3  ] loss:   6.26, perplexity: 523.44\n",
            "[Epoch 3  ] loss:   5.86, perplexity: 351.19\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 347.18\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 346.01\n",
            "[Epoch 3  ] loss:   5.85, perplexity: 345.63\n",
            "[Epoch 3  ] loss:   5.84, perplexity: 342.31\n",
            "[Epoch 3  ] loss:   5.83, perplexity: 341.19\n",
            "[Epoch 3  ] loss:   5.83, perplexity: 338.71\n",
            "\n",
            "[Epoch 3  ] ended with train_loss:   5.83, ppl: 338.71\n",
            "[Epoch 3  ] ended with valid_loss:   5.63, valid_ppl: 279.44\n",
            "[Epoch 3  ] completed in 21.22 seconds\n",
            "\n",
            "[Epoch 4  ] loss:   6.06, perplexity: 429.92\n",
            "[Epoch 4  ] loss:   5.72, perplexity: 304.62\n",
            "[Epoch 4  ] loss:   5.71, perplexity: 303.04\n",
            "[Epoch 4  ] loss:   5.71, perplexity: 303.07\n",
            "[Epoch 4  ] loss:   5.71, perplexity: 302.64\n",
            "[Epoch 4  ] loss:   5.70, perplexity: 300.20\n",
            "[Epoch 4  ] loss:   5.70, perplexity: 299.72\n",
            "[Epoch 4  ] loss:   5.70, perplexity: 297.97\n",
            "\n",
            "[Epoch 4  ] ended with train_loss:   5.70, ppl: 297.97\n",
            "[Epoch 4  ] ended with valid_loss:   5.60, valid_ppl: 269.74\n",
            "[Epoch 4  ] completed in 21.28 seconds\n",
            "\n",
            "[Epoch 5  ] loss:   6.02, perplexity: 409.91\n",
            "[Epoch 5  ] loss:   5.62, perplexity: 276.57\n",
            "[Epoch 5  ] loss:   5.62, perplexity: 275.69\n",
            "[Epoch 5  ] loss:   5.62, perplexity: 275.51\n",
            "[Epoch 5  ] loss:   5.62, perplexity: 275.52\n",
            "[Epoch 5  ] loss:   5.61, perplexity: 273.38\n",
            "[Epoch 5  ] loss:   5.61, perplexity: 273.28\n",
            "[Epoch 5  ] loss:   5.61, perplexity: 271.90\n",
            "\n",
            "[Epoch 5  ] ended with train_loss:   5.61, ppl: 271.90\n",
            "[Epoch 5  ] ended with valid_loss:   5.57, valid_ppl: 262.61\n",
            "[Epoch 5  ] completed in 21.62 seconds\n",
            "\n",
            " ---> Final test set performance:   5.49, test_ppl: 241.47\n"
          ]
        }
      ],
      "source": [
        "# Set the seed for reproducible results\n",
        "fix_seed(30494)\n",
        "\n",
        "def run_with_type(rnn_type):\n",
        "  rnnlm_model = RNNLM(\n",
        "      vocab_size=len(vocab),  # vocabulary size\n",
        "      emb_dim=128,            # word embedding dim\n",
        "      hid_dim=128,            # hidden layer dim\n",
        "      rnn_type=rnn_type,         # RNN type\n",
        "      n_layers=1,             # Number of stacked RNN layers\n",
        "      clip_gradient_norm=1.0, # gradient clip threshold\n",
        "      bptt_steps=35,          # Truncated BPTT window size\n",
        "      dropout=0.4,            # dropout probability\n",
        "  )\n",
        "\n",
        "  # move to device\n",
        "  rnnlm_model.to(DEVICE)\n",
        "\n",
        "  # Initial learning rate for the optimizer\n",
        "  RNNLM_INIT_LR = 0.002\n",
        "\n",
        "  # Create the optimizer\n",
        "  rnnlm_optimizer = torch.optim.Adam(rnnlm_model.parameters(), lr=RNNLM_INIT_LR)\n",
        "  print(rnnlm_model)\n",
        "\n",
        "  print('Starting training!')\n",
        "  # NOTE: If you happen to have memory errors, try decreasing the batch size\n",
        "  rnnlm_model.train_model(\n",
        "      rnnlm_optimizer, train, valid, test, n_epochs=5, batch_size=16)\n",
        "  return rnnlm_model\n",
        "  \n",
        "for i in [\"LSTM\",\"RNN\"]:\n",
        "  model = run_with_type(i)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbvkysROCmht"
      },
      "source": [
        "## More exercises\n",
        "\n",
        "**We do not provide any codes or answers for these extra questions. They are meant to motivate the interested ones to further go into the details of language modelling.**\n",
        "\n",
        "- Modify the `Vocabulary` class so that it knows about the counts of the tokens in the training set to apply a frequency threshold on the words that will be accepted to the vocabulary. Try with different thresholds such as 2, 3, 5 and train one of the models on top to see what is the impact of reducing the size of the vocabulary and eventually letting the model to learn an embedding for `<unk>`.\n",
        "\n",
        "- Implement a function to generate sentences from the RNNLM language model. How would you do that? There will not be any batches involved so you can directly feed the model with some prefix embeddings and sample (or take the word with the maximum probability, i.e. greedy search) from the output probability distribution.\n",
        "\n",
        "- Try implementing **weight tying** for RNNLM, which is an [approach](https://arxiv.org/abs/1608.05859) to reduce the number of parameters in sequence-to-sequence models. Notice that, we actually have 2 embeddings in the network: first is used to encode a given input (`self.emb`), second is the output layer! Yes the output layer is also a sort of embedding layer since it has a dimension of $V$ in its weight matrix. If you set the correct sizes in your network in a way that the embedding layer and the output layer has exactly the same sizes, you can let PyTorch share/tie those matrices, effectively removing one of them completely! The solution is actually a one-liner with PyTorch.  \n",
        "\n",
        "- Try taking one of the LSTM-papers below and implement one of the ideas that you like from there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq1fn-x9aS5e"
      },
      "source": [
        "## Further Reading\n",
        " - [Original FFLM paper from Bengio et al. 2003](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)\n",
        " - [Original RNNLM paper from Mikolov et al. 2010](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)\n",
        " - Some recent state-of-the-art LSTM-based RNNLMs\n",
        "\n",
        "  - [Regularizing and Optimizing LSTM Language Models](https://arxiv.org/pdf/1708.02182.pdf)\n",
        "  - [An Analysis of Neural Language Modeling at Multiple Scales](https://arxiv.org/pdf/1803.08240.pdf)\n",
        "  - [Scalable Language Modeling: WikiText-103 on a Single GPU in 12 hours](https://mlsys.org/Conferences/2019/doc/2018/50.pdf)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "lab03_2_NeuralLMs_Solutions.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit ('py38_pytorch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "ff27035c8dc0a26468b79942def09685664b2e815f4bca7e9395dcaceb48c986"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
