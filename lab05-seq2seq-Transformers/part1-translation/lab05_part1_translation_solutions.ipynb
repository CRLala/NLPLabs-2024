{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "military-character",
      "metadata": {
        "id": "military-character"
      },
      "source": [
        "# Lab 5, part 1: Seq2Seq Transformers â€” Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EslAmsE-tWMB",
      "metadata": {
        "id": "EslAmsE-tWMB"
      },
      "source": [
        "In this lab, we will take you through a practical use of Transformers. This notebook shows you how to use [Hugging face](https://huggingface.co/)'s package to import and train pretrained models for the tasks of hate speech classification and machine translation.\n",
        "\n",
        "We first show you all necessay components to use the ``transformers`` package before asking you to implement some code in the later sections.\n",
        "\n",
        "\n",
        "**Note:** The training of models will take quite some time so make sure to run this session with the GPU enabled. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rwXjbNUJHzZ0",
      "metadata": {
        "id": "rwXjbNUJHzZ0"
      },
      "source": [
        "## Setting up the Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Vina6dhaHz1N",
      "metadata": {
        "id": "Vina6dhaHz1N"
      },
      "source": [
        "First, we need to install Hugging Face [transformers](https://huggingface.co/transformers/index.html) and [Sentence piece Tokenizers](https://github.com/google/sentencepiece) with the following commands"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "other-scottish",
      "metadata": {
        "id": "other-scottish"
      },
      "outputs": [],
      "source": [
        "# ! pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "modern-olympus",
      "metadata": {
        "id": "modern-olympus"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install ipywidgets\n",
        "!jupyter nbextension enable --py widgetsnbextension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c14b3277",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pandas\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "developing-france",
      "metadata": {
        "id": "developing-france"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  print('WARNING: You may want to change the runtime to GPU for faster training!')\n",
        "  DEVICE = 'cpu'\n",
        "else:\n",
        "  DEVICE = 'cuda:0'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58jPlYaWk7fh",
      "metadata": {
        "id": "58jPlYaWk7fh"
      },
      "source": [
        "If you work in Colab, mount your google drive to save models and training checkpoints. Run the following code to connect your google drive to colab. Click on the link and copy and past the code you saw into the input box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qjdWhQHBk6RW",
      "metadata": {
        "id": "qjdWhQHBk6RW"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# %cd '/content/drive/MyDrive/Colab Notebooks/'\n",
        "# %mkdir './Lab 7'\n",
        "# %cd './Lab 7' "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "exterior-afghanistan",
      "metadata": {
        "id": "exterior-afghanistan"
      },
      "source": [
        "## Machine Translation (MT)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "assigned-sapphire",
      "metadata": {
        "id": "assigned-sapphire"
      },
      "source": [
        "Recall the previous lab on machine translation [(Lab 4)](https://colab.research.google.com/github/ImperialNLP/NLPLabs-2023/blob/main/lab04-MT-with-RNNs/lab04_mt.ipynb) where we trained a recurrent neural network on the [Multi30k](https://github.com/multi30k/dataset) dataset. \n",
        "\n",
        "We will now train a Transformer model for the same task on the same dataset and compare the results. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "wkbsnSm_oK8e",
      "metadata": {
        "id": "wkbsnSm_oK8e"
      },
      "source": [
        "### Downloading dataset and evaluation function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fr-2a2z5kS9a",
      "metadata": {
        "id": "fr-2a2z5kS9a"
      },
      "source": [
        "We will start by downloading the dataset for German, English and French and installing `sacreBLEU` to compute the BLEU score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "musical-series",
      "metadata": {
        "id": "musical-series"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# install sacreBLEU\n",
        "pip install sacrebleu==1.5.0\n",
        "echo\n",
        "\n",
        "# Download the corpus\n",
        "URL=\"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok\"\n",
        "\n",
        "#cd data\n",
        "\n",
        "for split in \"train\" \"val\" \"test_2016_flickr\"; do\n",
        "    for lang in en de fr; do\n",
        "        fname=\"${split}.lc.norm.tok.${lang}\"\n",
        "        if [ ! -f $fname ]; then\n",
        "            echo \"Downloading $fname\"\n",
        "            wget -q \"${URL}/$fname\" -O \"${split/_2016_flickr/}.${lang}\"\n",
        "        fi\n",
        "    done\n",
        "done\n",
        "echo \n",
        "\n",
        "# Print the first 10 lines with line numbers of \n",
        "# the English and French training data\n",
        "cat -n train.en | head -n10\n",
        "echo\n",
        "cat -n train.fr | head -n10\n",
        "echo\n",
        "cd .."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "qAB2AV_Ns-aY",
      "metadata": {
        "id": "qAB2AV_Ns-aY"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Hs-Wy6uZkyJF",
      "metadata": {
        "id": "Hs-Wy6uZkyJF"
      },
      "source": [
        "Just as in the previous lab we define our own dataset class to handle ``Multi30k``. However, we modify it to adapt it to a transformer model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wanted-venice",
      "metadata": {
        "id": "wanted-venice"
      },
      "outputs": [],
      "source": [
        "class Multi30K:\n",
        "    \"\"\"A dataset wrapper for Multi30K.\"\"\"\n",
        "    def __init__(self, tokenizer, src_file, trg_file):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "        self.src_sents, self.trg_sents = self.read_sentences(src_file, trg_file)\n",
        "\n",
        "    def read_sentences(self, src_file, trg_file):\n",
        "        src_sents = []\n",
        "        trg_sents = []\n",
        "\n",
        "        # Read source side\n",
        "        with open(src_file) as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                src_sents.append(line) \n",
        "            \n",
        "        # Read target side\n",
        "        with open(trg_file) as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                trg_sents.append(line)\n",
        "\n",
        "        assert len(src_sents) == len(trg_sents), \"Files are not aligned!\"\n",
        "        return src_sents, trg_sents\n",
        "    \n",
        "    def collate_fn(self, idx):\n",
        "        src_texts = [self.src_sents[i] for i in idx]\n",
        "        trg_texts = [self.trg_sents[i] for i in idx]\n",
        "        \n",
        "        output = self.tokenizer.prepare_seq2seq_batch(src_texts=src_texts, \n",
        "                                                      tgt_texts=trg_texts, \n",
        "                                                      max_length=128, \n",
        "                                                      max_target_length=128,\n",
        "                                                      return_tensors='pt',\n",
        "                                                      truncation=True)\n",
        "        return output\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.src_sents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return idx"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "xyGLGLBArnHL",
      "metadata": {
        "id": "xyGLGLBArnHL"
      },
      "source": [
        "### Transformer model for MT"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "Uh7mzoLBmQhJ",
      "metadata": {
        "id": "Uh7mzoLBmQhJ"
      },
      "source": [
        "\n",
        "For this task, we won't be using Bert but instead, we will import another model, [MarianMT](https://huggingface.co/transformers/model_doc/marian.html), which is a  specifically used for MT. \n",
        "\n",
        "Running the code below you can see all modules within MarianMT model and compare it with Bert. \n",
        "\n",
        "**Q1: What's missing in BERT architecture necessary for MT?**\n",
        "\n",
        "**A:** BERT doesn't have a decoder module. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SABFaL3UlK6r",
      "metadata": {
        "id": "SABFaL3UlK6r"
      },
      "outputs": [],
      "source": [
        "from transformers import EncoderDecoderModel, MarianMTModel, MarianTokenizer, BartModel, BartConfig, BertConfig, BartForCausalLM,Trainer,TrainingArguments\n",
        "\n",
        "model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
        "\n",
        "model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "linear-diabetes",
      "metadata": {
        "id": "linear-diabetes"
      },
      "source": [
        "\n",
        "**Q2: Import Transformer model and train for Machine Translation**\n",
        "\n",
        "Modify ``main_mt()`` to:\n",
        "- Import the ``\"Helsinki-NLP/opus-mt-en-de\"`` variant of ``MarianMT`` model and the tokenizer it was trained with.\n",
        "- Create the train (``train.en`` and ``train.de`` files) and test (``test.en`` and ``test.de``) dataset objects using our ``Multi30k`` class.\n",
        "- Finetune the model on the train set.\n",
        "\n",
        "**A:** See the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fewer-england",
      "metadata": {
        "id": "fewer-england"
      },
      "outputs": [],
      "source": [
        "from transformers import EncoderDecoderModel, MarianMTModel, MarianTokenizer, BartModel, BartConfig, BertConfig, BartForCausalLM,Trainer,TrainingArguments\n",
        "\n",
        "def main_mt():\n",
        "    \n",
        "    ## QUESTION 5 ##\n",
        "\n",
        "    mt_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
        "    mt_dataset = Multi30K(mt_tokenizer, 'train.en', 'train.de')\n",
        "    mt_test_dataset = Multi30K(mt_tokenizer, 'test.en', 'test.de')\n",
        "    \n",
        "    model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./experiment/mt',\n",
        "        learning_rate = 0.00005,\n",
        "        logging_steps= 5000,\n",
        "        save_steps = 10000,\n",
        "        num_train_epochs = 1,\n",
        "        per_device_train_batch_size=2\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,                         \n",
        "        args=training_args,                 \n",
        "        train_dataset=mt_dataset,                     \n",
        "        data_collator=mt_dataset.collate_fn\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    ## when you already trained your model and want to start from a checkpoint\n",
        "    #trainer.train(\"./experiment/mt/checkpoint-40000\")\n",
        "\n",
        "    trainer.save_model('./models/mt_marianmt/')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8LdM1J-wtPWd",
      "metadata": {
        "id": "8LdM1J-wtPWd"
      },
      "source": [
        "The following code should run for ~50min if you train the model for 3 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "resident-sight",
      "metadata": {
        "id": "resident-sight"
      },
      "outputs": [],
      "source": [
        "main_mt()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mJcA9ihzY6Pf",
      "metadata": {
        "id": "mJcA9ihzY6Pf"
      },
      "source": [
        "Let's evaluate our model using BLEU metric\n",
        "."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rowUbyM_enUw",
      "metadata": {
        "id": "rowUbyM_enUw"
      },
      "outputs": [],
      "source": [
        "import sacrebleu\n",
        "\n",
        "def evaluate_mt(model,mt_tokenizer, mt_test_dataset):\n",
        "\n",
        "  bleu = []\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  for file in tqdm(range(len(mt_test_dataset))):\n",
        "\n",
        "    src_text = mt_test_dataset.src_sents[file]\n",
        "    targ_text_origin = mt_test_dataset.trg_sents[file]\n",
        "\n",
        "    translated = model.generate(**mt_tokenizer.prepare_seq2seq_batch(src_text, return_tensors=\"pt\"))\n",
        "    translated_text = [mt_tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "\n",
        "    bleu.append(sacrebleu.corpus_bleu(translated_text, targ_text_origin, force=True).score)\n",
        "\n",
        "  bleu = np.asarray(bleu)\n",
        "\n",
        "  return np.average(bleu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OghdALVBpz1b",
      "metadata": {
        "id": "OghdALVBpz1b"
      },
      "outputs": [],
      "source": [
        "model = MarianMTModel.from_pretrained('./models/mt_marianmt/')\n",
        "\n",
        "mt_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
        "mt_test_dataset = Multi30K(mt_tokenizer, 'test.en', 'test.de')\n",
        "\n",
        "bleu = evaluate_mt(model,mt_tokenizer, mt_test_dataset)\n",
        "\n",
        "print(bleu)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfQXV7TABFVw",
      "metadata": {
        "id": "dfQXV7TABFVw"
      },
      "source": [
        "What BLEU score do you get ? How does it compare to the performance of the RNN with and without attention? \n",
        "\n",
        "Can you improve the score ?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "zrLXxS1hHjuu",
      "metadata": {
        "id": "zrLXxS1hHjuu"
      },
      "source": [
        "## Extra exercises"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "aIuj99ZNHl8z",
      "metadata": {
        "id": "aIuj99ZNHl8z"
      },
      "source": [
        "### Questions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "MznOidHWnfD7",
      "metadata": {
        "id": "MznOidHWnfD7"
      },
      "source": [
        "**Q3.1**: Why do RNN suffer from long term dependency issues and how do Transformers fix that?\n",
        "\n",
        "**A**: RNN based models process tokens one by one and each state depend on previous seen states. In vanilla RNN each token encoding relies on the previous step only. As the sentence grows in size, earlier tokens are forgotten (the gradient tends to vanish/explode after multiple passes through layers).\n",
        "LSTM and GRU architectures deal with some of the problem with the vanishing/exploding gradient and are able to handle longer sentences, but still can't handle very long inputs without forgetting earlier parts. \n",
        "\n",
        "Another shortcoming is that RNN process sentences in one direction and cannot look into future token, only the ones preceding the current state. Bi-directional models were introduced to solve that issue but add more computations as the model needs to go through the sequences twice. \n",
        "\n",
        "Thanks to self-attention and positional encodings, Transformers are able to compute relationships between words and their position without sequential training/inference. \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Q3.2**: Which architecture is faster to train: RNN or Transformer? Why?\n",
        "\n",
        "**A**: RNNs are complex stuctures due to the recursive training and because they take one token at a time, the training process is quite slow. They are also not parallelizable, since you can't compute hidden states of one word without first computing the representations of the previous words. Transformers, on the other hand, can process each step independently. \n",
        "\n",
        "---\n",
        "\n",
        "**Q3.3** What issues do Transformers have due to the attention mechanism? \n",
        "\n",
        "**A**: Self-attention becomes computationaly expensive as the input size increases. Because we compute attention for every pair of tokens, the time complexity is $O(N^2)$ with $N$ being the input length.\n",
        "On the other hand, RNNs are linear with the length of the input sequence  and have a computing complexity of $O(N)$. However, they still suffer from the issues mentioned above and remain slower to train.   \n",
        "\n",
        "However, Transformers can be modified to alleviate this issue (e.g transformer XL applies the simple truncated backpropagation, see lecture on Transformers).\n",
        "\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "HPbsBF2LHpLV",
      "metadata": {
        "id": "HPbsBF2LHpLV"
      },
      "source": [
        "### MT Extension"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "31749a0b",
      "metadata": {},
      "source": [
        "#### Preliminaries"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3qREhnCecNhy",
      "metadata": {
        "id": "3qREhnCecNhy"
      },
      "source": [
        "In this section, we mainly define the following classes:\n",
        "\n",
        "\n",
        "*   ``SinusoidalPositionalEmbedding`` -- *Note that this is different from the PositionalEmbedding in original Bert model.*\n",
        "*   ``Attention``\n",
        "*   ``PreTrainedModel``\n",
        "\n",
        "\n",
        "*   ``EncoderLayer``\n",
        "*   ``DecoderLayer``\n",
        "*   ``Encoder``\n",
        "*   ``Decoder``\n",
        "\n",
        "\n",
        "*   ``Model``\n",
        "*   ``MTModel``\n",
        "\n",
        "\n",
        "**Your** task is to fill in the missing blocks in ***Model*** and ***MTModel*** and understand the process of transformer for MT. \n",
        "\n",
        "\n",
        "Search the coding blocks by ***TODO*** in the next subsection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Uub3GiD9QT8F",
      "metadata": {
        "id": "Uub3GiD9QT8F"
      },
      "outputs": [],
      "source": [
        "from transformers import PreTrainedModel, MarianConfig as MyConfig\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers.modeling_outputs import (BaseModelOutput, \n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    CausalLMOutputWithCrossAttentions,\n",
        "    Seq2SeqLMOutput,\n",
        "    Seq2SeqModelOutput)\n",
        "\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "import logging\n",
        "from typing import Optional, Tuple\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"Helsinki-NLP/opus-mt-en-de\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XuFULwNcSIkr",
      "metadata": {
        "id": "XuFULwNcSIkr"
      },
      "outputs": [],
      "source": [
        "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
        "    \"\"\"\n",
        "    Shift input ids one token to the right.\n",
        "    \"\"\"\n",
        "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
        "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
        "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
        "\n",
        "    assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n",
        "    # replace possible -100 values in labels by `pad_token_id`\n",
        "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
        "\n",
        "    return shifted_input_ids\n",
        "\n",
        "\n",
        "\n",
        "def _make_causal_mask(input_ids_shape: torch.Size, dtype: torch.dtype, past_key_values_length: int = 0):\n",
        "    \"\"\"\n",
        "    Make causal mask used for bi-directional self-attention.\n",
        "    \"\"\"\n",
        "    bsz, tgt_len = input_ids_shape\n",
        "    mask = torch.full((tgt_len, tgt_len), float(\"-inf\"))\n",
        "    mask_cond = torch.arange(mask.size(-1))\n",
        "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
        "    mask = mask.to(dtype)\n",
        "\n",
        "    if past_key_values_length > 0:\n",
        "        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)\n",
        "    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
        "\n",
        "\n",
        "\n",
        "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
        "    \"\"\"\n",
        "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
        "    \"\"\"\n",
        "    bsz, src_len = mask.size()\n",
        "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
        "\n",
        "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
        "\n",
        "    inverted_mask = 1.0 - expanded_mask\n",
        "\n",
        "    return inverted_mask.masked_fill(inverted_mask.bool(), torch.finfo(dtype).min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l5TyyQpsSKhg",
      "metadata": {
        "id": "l5TyyQpsSKhg"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionalEmbedding(nn.Embedding):\n",
        "    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n",
        "\n",
        "    def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None):\n",
        "        super().__init__(num_positions, embedding_dim)\n",
        "        self.weight = self._init_weight(self.weight)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weight(out: nn.Parameter):\n",
        "        \"\"\"\n",
        "        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n",
        "        the 2nd half of the vector. [dim // 2:]\n",
        "        \"\"\"\n",
        "        n_pos, dim = out.shape\n",
        "        position_enc = np.array(\n",
        "            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n",
        "        )\n",
        "        out.requires_grad = False  # set early to avoid an error in pytorch-1.8+\n",
        "        sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n",
        "        out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
        "        out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
        "        out.detach_()\n",
        "        return out\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0):\n",
        "        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
        "        bsz, seq_len = input_ids_shape[:2]\n",
        "        positions = torch.arange(\n",
        "            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n",
        "        )\n",
        "        return super().forward(positions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eqqb50tESOpp",
      "metadata": {
        "id": "eqqb50tESOpp"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        dropout: float = 0.0,\n",
        "        is_decoder: bool = False,\n",
        "        bias: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert (\n",
        "            self.head_dim * num_heads == self.embed_dim\n",
        "        ), f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`: {num_heads}).\"\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "        self.is_decoder = is_decoder\n",
        "\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "\n",
        "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
        "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        key_value_states: Optional[torch.Tensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
        "\n",
        "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
        "        # for the decoder\n",
        "        is_cross_attention = key_value_states is not None\n",
        "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
        "\n",
        "        # get query proj\n",
        "        query_states = self.q_proj(hidden_states) * self.scaling\n",
        "        # get key, value proj\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_states = past_key_value[0]\n",
        "            value_states = past_key_value[1]\n",
        "        elif is_cross_attention:\n",
        "            # cross_attentions\n",
        "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
        "        elif past_key_value is not None:\n",
        "            # reuse k, v, self_attention\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "        else:\n",
        "            # self_attention\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_states, value_states)\n",
        "\n",
        "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
        "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
        "        key_states = key_states.view(*proj_shape)\n",
        "        value_states = value_states.view(*proj_shape)\n",
        "\n",
        "        src_len = key_states.size(1)\n",
        "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
        "\n",
        "        assert attn_weights.size() == (\n",
        "            bsz * self.num_heads,\n",
        "            tgt_len,\n",
        "            src_len,\n",
        "        ), f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {attn_weights.size()}\"\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            assert attention_mask.size() == (\n",
        "                bsz,\n",
        "                1,\n",
        "                tgt_len,\n",
        "                src_len,\n",
        "            ), f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "\n",
        "        if layer_head_mask is not None:\n",
        "            assert layer_head_mask.size() == (\n",
        "                self.num_heads,\n",
        "            ), f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n",
        "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if output_attentions:\n",
        "            # this operation is a bit akward, but it's required to\n",
        "            # make sure that attn_weights keeps its gradient.\n",
        "            # In order to do so, attn_weights have to reshaped\n",
        "            # twice and have to be reused in the following\n",
        "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "        else:\n",
        "            attn_weights_reshaped = None\n",
        "\n",
        "        attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "\n",
        "        attn_output = torch.bmm(attn_probs, value_states)\n",
        "\n",
        "        assert attn_output.size() == (\n",
        "            bsz * self.num_heads,\n",
        "            tgt_len,\n",
        "            self.head_dim,\n",
        "        ), f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is {attn_output.size()}\"\n",
        "\n",
        "        attn_output = (\n",
        "            attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
        "            .transpose(1, 2)\n",
        "            .reshape(bsz, tgt_len, embed_dim)\n",
        "        )\n",
        "\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "\n",
        "        return attn_output, attn_weights_reshaped, past_key_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-_HNBj_4SZgk",
      "metadata": {
        "id": "-_HNBj_4SZgk"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, config: MyConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.self_attn = Attention(\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=config.encoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "        )\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        layer_head_mask: torch.Tensor,\n",
        "        output_attentions: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states (:obj:`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            attention_mask (:obj:`torch.FloatTensor`): attention mask of size\n",
        "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
        "            layer_head_mask (:obj:`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
        "                `(config.encoder_attention_heads,)`.\n",
        "            output_attentions (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
        "                returned tensors for more detail.\n",
        "        \"\"\"\n",
        "        residual = hidden_states\n",
        "        hidden_states, attn_weights, _ = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            layer_head_mask=layer_head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
        "        hidden_states = F.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n",
        "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
        "            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (attn_weights,)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J4xQ5kL6SesZ",
      "metadata": {
        "id": "J4xQ5kL6SesZ"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, config: MyConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "\n",
        "        self.self_attn = Attention(\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=config.decoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            is_decoder=True,\n",
        "        )\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.encoder_attn = Attention(\n",
        "            self.embed_dim,\n",
        "            config.decoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            is_decoder=True,\n",
        "        )\n",
        "        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        use_cache: Optional[bool] = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states (:obj:`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            attention_mask (:obj:`torch.FloatTensor`): attention mask of size\n",
        "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
        "            encoder_hidden_states (:obj:`torch.FloatTensor`): cross attention input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            encoder_attention_mask (:obj:`torch.FloatTensor`): encoder attention mask of size\n",
        "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
        "            layer_head_mask (:obj:`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
        "                `(config.encoder_attention_heads,)`.\n",
        "            encoder_layer_head_mask (:obj:`torch.FloatTensor`): mask for encoder attention heads in a given layer of\n",
        "                size `(config.encoder_attention_heads,)`.\n",
        "            past_key_value (:obj:`Tuple(torch.FloatTensor)`): cached past key and value projection states\n",
        "            output_attentions (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
        "                returned tensors for more detail.\n",
        "        \"\"\"\n",
        "        residual = hidden_states\n",
        "\n",
        "        # Self Attention\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        # add present self-attn cache to positions 1,2 of present_key_value tuple\n",
        "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "            attention_mask=attention_mask,\n",
        "            layer_head_mask=layer_head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "        # Cross-Attention Block\n",
        "        cross_attn_present_key_value = None\n",
        "        cross_attn_weights = None\n",
        "        if encoder_hidden_states is not None:\n",
        "            residual = hidden_states\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n",
        "                hidden_states=hidden_states,\n",
        "                key_value_states=encoder_hidden_states,\n",
        "                attention_mask=encoder_attention_mask,\n",
        "                layer_head_mask=encoder_layer_head_mask,\n",
        "                past_key_value=cross_attn_past_key_value,\n",
        "                output_attentions=output_attentions,\n",
        "            )\n",
        "            hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "            hidden_states = residual + hidden_states\n",
        "            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
        "\n",
        "            # add cross-attn to positions 3,4 of present_key_value tuple\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        # Fully Connected\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
        "        hidden_states = F.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (self_attn_weights, cross_attn_weights)\n",
        "\n",
        "        if use_cache:\n",
        "            outputs += (present_key_value,)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jxLkSqyuREvQ",
      "metadata": {
        "id": "jxLkSqyuREvQ"
      },
      "outputs": [],
      "source": [
        "class PreTrainedModel(PreTrainedModel):\n",
        "    config_class = MyConfig\n",
        "    base_model_prefix = \"model\"\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = self.config.init_std\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, SinusoidalPositionalEmbedding):\n",
        "            pass\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "    @property\n",
        "    def dummy_inputs(self):\n",
        "        pad_token = self.config.pad_token_id\n",
        "        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n",
        "        dummy_inputs = {\n",
        "            \"attention_mask\": input_ids.ne(pad_token),\n",
        "            \"input_ids\": input_ids,\n",
        "            \"decoder_input_ids\": input_ids,\n",
        "        }\n",
        "        return dummy_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2CaNtVNpQUGP",
      "metadata": {
        "id": "2CaNtVNpQUGP"
      },
      "outputs": [],
      "source": [
        "class Encoder(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
        "    :class:`EncoderLayer`.\n",
        "\n",
        "    Args:\n",
        "        config: MyConfig\n",
        "        embed_tokens (torch.nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: MyConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.encoder_layerdrop\n",
        "\n",
        "        embed_dim = config.d_model\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.max_source_positions = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n",
        "\n",
        "        self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "            config.max_position_embeddings,\n",
        "            embed_dim,\n",
        "            self.padding_idx,\n",
        "        )\n",
        "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
        "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
        "                provide it.\n",
        "\n",
        "                Indices can be obtained using :class:`~transformers.MarianTokenizer`. See\n",
        "                :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__`\n",
        "                for details.\n",
        "\n",
        "                `What are input IDs? <../glossary.html#input-ids>`__\n",
        "            attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "                Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "\n",
        "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "            head_mask (:obj:`torch.Tensor` of shape :obj:`(num_layers, num_heads)`, `optional`):\n",
        "                Mask to nullify selected heads of the attention modules. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the heas is **masked**.\n",
        "\n",
        "            inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "                Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded\n",
        "                representation. This is useful if you want more control over how to convert :obj:`input_ids` indices\n",
        "                into associated vectors than the model's internal embedding lookup matrix.\n",
        "            output_attentions (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
        "                returned tensors for more detail.\n",
        "            output_hidden_states (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
        "                for more detail.\n",
        "            return_dict (:obj:`bool`, `optional`):\n",
        "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # retrieve input_ids and inputs_embeds\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "\n",
        "        embed_pos = self.embed_positions(input_shape)\n",
        "\n",
        "        hidden_states = inputs_embeds + embed_pos\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "        # expand attention_mask\n",
        "        if attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)\n",
        "\n",
        "        encoder_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "\n",
        "        # check if head_mask has a correct number of layers specified if desired\n",
        "        if head_mask is not None:\n",
        "            assert head_mask.size()[0] == (\n",
        "                len(self.layers)\n",
        "            ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n",
        "        for idx, encoder_layer in enumerate(self.layers):\n",
        "            if output_hidden_states:\n",
        "                encoder_states = encoder_states + (hidden_states,)\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
        "                layer_outputs = (None, None)\n",
        "            else:\n",
        "                if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "\n",
        "                    def create_custom_forward(module):\n",
        "                        def custom_forward(*inputs):\n",
        "                            return module(*inputs, output_attentions)\n",
        "\n",
        "                        return custom_forward\n",
        "\n",
        "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                        create_custom_forward(encoder_layer),\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                        (head_mask[idx] if head_mask is not None else None),\n",
        "                    )\n",
        "                else:\n",
        "                    layer_outputs = encoder_layer(\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
        "                        output_attentions=output_attentions,\n",
        "                    )\n",
        "\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            encoder_states = encoder_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dhaT7vtjSs0D",
      "metadata": {
        "id": "dhaT7vtjSs0D"
      },
      "outputs": [],
      "source": [
        "class Decoder(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a :class:`DecoderLayer`\n",
        "\n",
        "    Args:\n",
        "        config: MyConfig\n",
        "        embed_tokens (torch.nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: MyConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
        "        super().__init__(config)\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.decoder_layerdrop\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.max_target_positions = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\n",
        "\n",
        "        self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "            config.max_position_embeddings,\n",
        "            config.d_model,\n",
        "            self.padding_idx,\n",
        "        )\n",
        "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.decoder_layers)])\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed_tokens = value\n",
        "\n",
        "    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n",
        "    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n",
        "        # create causal mask\n",
        "        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "        combined_attention_mask = None\n",
        "        if input_shape[-1] > 1:\n",
        "            combined_attention_mask = _make_causal_mask(\n",
        "                input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length\n",
        "            ).to(self.device)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n",
        "            combined_attention_mask = (\n",
        "                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n",
        "            )\n",
        "\n",
        "        return combined_attention_mask\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_head_mask=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
        "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
        "                provide it.\n",
        "\n",
        "                Indices can be obtained using :class:`~transformers.MarianTokenizer`. See\n",
        "                :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__`\n",
        "                for details.\n",
        "\n",
        "                `What are input IDs? <../glossary.html#input-ids>`__\n",
        "            attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "                Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "\n",
        "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "            encoder_hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, encoder_sequence_length, hidden_size)`, `optional`):\n",
        "                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n",
        "                of the decoder.\n",
        "            encoder_attention_mask (:obj:`torch.LongTensor` of shape :obj:`(batch_size, encoder_sequence_length)`, `optional`):\n",
        "                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\n",
        "                selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "\n",
        "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "            head_mask (:obj:`torch.Tensor` of shape :obj:`(num_layers, num_heads)`, `optional`):\n",
        "                Mask to nullify selected heads of the attention modules. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the heas is **masked**.\n",
        "\n",
        "            encoder_head_mask (:obj:`torch.Tensor` of shape :obj:`(num_layers, num_heads)`, `optional`):\n",
        "                Mask to nullify selected heads of the attention modules in encoder to avoid performing cross-attention\n",
        "                on hidden heads. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the heas is **masked**.\n",
        "\n",
        "            past_key_values (:obj:`Tuple[Tuple[torch.Tensor]]` of length :obj:`config.n_layers` with each tuple having 2 tuples each of which has 2 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "                Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up\n",
        "                decoding.\n",
        "\n",
        "                If :obj:`past_key_values` are used, the user can optionally input only the last\n",
        "                :obj:`decoder_input_ids` (those that don't have their past key value states given to this model) of\n",
        "                shape :obj:`(batch_size, 1)` instead of all :obj:`decoder_input_ids`` of shape :obj:`(batch_size,\n",
        "                sequence_length)`.\n",
        "            inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "                Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded\n",
        "                representation. This is useful if you want more control over how to convert :obj:`input_ids` indices\n",
        "                into associated vectors than the model's internal embedding lookup matrix.\n",
        "            output_attentions (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
        "                returned tensors for more detail.\n",
        "            output_hidden_states (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
        "                for more detail.\n",
        "            return_dict (:obj:`bool`, `optional`):\n",
        "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # retrieve input_ids and inputs_embeds\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "\n",
        "        attention_mask = self._prepare_decoder_attention_mask(\n",
        "            attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
        "        )\n",
        "\n",
        "        # expand encoder attention mask\n",
        "        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            encoder_attention_mask = _expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n",
        "\n",
        "        # embed positions\n",
        "        positions = self.embed_positions(input_shape, past_key_values_length)\n",
        "\n",
        "        hidden_states = inputs_embeds + positions\n",
        "\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "        # decoder layers\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attns = () if output_attentions else None\n",
        "        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "\n",
        "        # check if head_mask has a correct number of layers specified if desired\n",
        "        if head_mask is not None:\n",
        "            assert head_mask.size()[0] == (\n",
        "                len(self.layers)\n",
        "            ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n",
        "        for idx, decoder_layer in enumerate(self.layers):\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states += (hidden_states,)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):\n",
        "                continue\n",
        "\n",
        "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
        "\n",
        "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "\n",
        "                if use_cache:\n",
        "                    logger.warn(\n",
        "                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n",
        "                        \"`use_cache=False`...\"\n",
        "                    )\n",
        "                    use_cache = False\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        # None for past_key_value\n",
        "                        return module(*inputs, output_attentions, use_cache)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(decoder_layer),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    head_mask[idx] if head_mask is not None else None,\n",
        "                    encoder_head_mask[idx] if encoder_head_mask is not None else None,\n",
        "                    None,\n",
        "                )\n",
        "            else:\n",
        "\n",
        "                layer_outputs = decoder_layer(\n",
        "                    hidden_states,\n",
        "                    attention_mask=attention_mask,\n",
        "                    encoder_hidden_states=encoder_hidden_states,\n",
        "                    encoder_attention_mask=encoder_attention_mask,\n",
        "                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
        "                    encoder_layer_head_mask=(encoder_head_mask[idx] if encoder_head_mask is not None else None),\n",
        "                    past_key_value=past_key_value,\n",
        "                    output_attentions=output_attentions,\n",
        "                    use_cache=use_cache,\n",
        "                )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attns += (layer_outputs[1],)\n",
        "\n",
        "                if encoder_hidden_states is not None:\n",
        "                    all_cross_attentions += (layer_outputs[2],)\n",
        "\n",
        "        # add hidden states from the last decoder layer\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states += (hidden_states,)\n",
        "\n",
        "        next_cache = next_decoder_cache if use_cache else None\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attns,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "831dd313",
      "metadata": {},
      "source": [
        "#### Model definition"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "YMt1N5teXskj",
      "metadata": {
        "id": "YMt1N5teXskj"
      },
      "source": [
        "**Q4:** Complete the 6 TODOs in the **Model** class.\n",
        "\n",
        "**A:** See the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nQM4v4HlS65I",
      "metadata": {
        "id": "nQM4v4HlS65I"
      },
      "outputs": [],
      "source": [
        "class Model(PreTrainedModel):\n",
        "    def __init__(self, config: MyConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
        "\n",
        "        ##  TODO 1: initialise embedding layer with nn.Embedding ##\n",
        "        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n",
        "\n",
        "\n",
        "        ##  TODO 2: define the encoder and decoder with previously defined classes ##\n",
        "        self.encoder = Encoder(config, self.shared)\n",
        "        self.decoder = Decoder(config, self.shared)\n",
        "\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.shared\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.shared = value\n",
        "        self.encoder.embed_tokens = self.shared\n",
        "        self.decoder.embed_tokens = self.shared\n",
        "\n",
        "    def get_encoder(self):\n",
        "        ##  TODO 3: return the encoder ##\n",
        "        return self.encoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        ##  TODO 4: return the decoder ##\n",
        "        return self.decoder\n",
        "      \n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        decoder_inputs_embeds=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if encoder_outputs is None:\n",
        "\n",
        "            ##  TODO 5: get the encoder outputs ##\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                head_mask=head_mask,\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
        "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "            encoder_outputs = BaseModelOutput(\n",
        "                last_hidden_state=encoder_outputs[0],\n",
        "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
        "            )\n",
        "\n",
        "        ##  TODO 6: get the decoder outputs ##\n",
        "        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            attention_mask=decoder_attention_mask,\n",
        "            encoder_hidden_states=encoder_outputs[0],\n",
        "            encoder_attention_mask=attention_mask,\n",
        "            head_mask=decoder_head_mask,\n",
        "            encoder_head_mask=head_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        if not return_dict:\n",
        "            return decoder_outputs + encoder_outputs\n",
        "\n",
        "        return Seq2SeqModelOutput(\n",
        "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
        "            past_key_values=decoder_outputs.past_key_values,\n",
        "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
        "            decoder_attentions=decoder_outputs.attentions,\n",
        "            cross_attentions=decoder_outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "            encoder_attentions=encoder_outputs.attentions,\n",
        "        )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7lSTY1TmV93Y",
      "metadata": {
        "id": "7lSTY1TmV93Y"
      },
      "source": [
        "**Q5:** In the following **MTModel**, fill in the missing parts in ***__init__*** and ***forward*** function.\n",
        "\n",
        "**A:** See the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MwmR18i5TCTX",
      "metadata": {
        "id": "MwmR18i5TCTX"
      },
      "outputs": [],
      "source": [
        "class MTModel(PreTrainedModel):\n",
        "    base_model_prefix = \"model\"\n",
        "    _keys_to_ignore_on_load_missing = [\n",
        "        r\"final_logits_bias\",\n",
        "        r\"encoder\\.version\",\n",
        "        r\"decoder\\.version\",\n",
        "        r\"lm_head\\.weight\",\n",
        "        r\"embed_positions\",\n",
        "    ]\n",
        "\n",
        "    _keys_to_ignore_on_save = [\n",
        "        \"model.encoder.embed_positions.weight\",\n",
        "        \"model.decoder.embed_positions.weight\",\n",
        "    ]\n",
        "\n",
        "    def __init__(self, config: MyConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "         ##  TODO 7: Define the model ##\n",
        "        self.model = Model(config)\n",
        "        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
        "\n",
        "        ##  TODO 8: Define the head ##\n",
        "        self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_encoder(self):\n",
        "\n",
        "        return self.model.get_encoder()\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model.get_decoder()\n",
        "\n",
        "    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
        "        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n",
        "        self._resize_final_logits_bias(new_num_tokens)\n",
        "        return new_embeddings\n",
        "\n",
        "    def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n",
        "        old_num_tokens = self.final_logits_bias.shape[-1]\n",
        "        if new_num_tokens <= old_num_tokens:\n",
        "            new_bias = self.final_logits_bias[:, :new_num_tokens]\n",
        "        else:\n",
        "            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n",
        "            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n",
        "        self.register_buffer(\"final_logits_bias\", new_bias)\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        decoder_inputs_embeds=None,\n",
        "        labels=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the masked language modeling loss. Indices should either be in ``[0, ...,\n",
        "            config.vocab_size]`` or -100 (see ``input_ids`` docstring). Tokens with indices set to ``-100`` are ignored\n",
        "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``.\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if labels is not None:\n",
        "            if decoder_input_ids is None:\n",
        "                decoder_input_ids = shift_tokens_right(\n",
        "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
        "                )\n",
        "        \n",
        "\n",
        "        ##  TODO 9: get the model output using self.model ##\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            decoder_head_mask=decoder_head_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "\n",
        "        lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (lm_logits,) + outputs[1:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return Seq2SeqLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=lm_logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
        "            decoder_attentions=outputs.decoder_attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
        "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
        "            encoder_attentions=outputs.encoder_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self,\n",
        "        decoder_input_ids,\n",
        "        past=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        use_cache=None,\n",
        "        encoder_outputs=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past is not None:\n",
        "            decoder_input_ids = decoder_input_ids[:, -1:]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
        "            \"encoder_outputs\": encoder_outputs,\n",
        "            \"past_key_values\": past,\n",
        "            \"decoder_input_ids\": decoder_input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"head_mask\": head_mask,\n",
        "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
        "        }\n",
        "\n",
        "    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n",
        "        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
        "\n",
        "    def adjust_logits_during_generation(self, logits, cur_len):\n",
        "        logits[:, self.config.pad_token_id] = float(\"-inf\")  # never predict pad token.\n",
        "        return logits\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past:\n",
        "            # cached cross_attention states don't have to be reordered -> they are always the same\n",
        "            reordered_past += (\n",
        "                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n",
        "            )\n",
        "        return reordered_past"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3fc1df5c",
      "metadata": {},
      "source": [
        "#### Model Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-bdr-vbyeHjd",
      "metadata": {
        "id": "-bdr-vbyeHjd"
      },
      "source": [
        "Let's try the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9V_0tmUkeGmv",
      "metadata": {
        "id": "9V_0tmUkeGmv"
      },
      "outputs": [],
      "source": [
        "mt_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
        "mt_dataset = Multi30K(mt_tokenizer, 'train.en', 'train.de')\n",
        "mt_test_dataset = Multi30K(mt_tokenizer, 'test.en', 'test.de')\n",
        "\n",
        "\n",
        "model = MTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
        "\n",
        "bleu = evaluate_mt(model,mt_tokenizer, mt_test_dataset)\n",
        "\n",
        "print(bleu)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Lab 05 - Part 1",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
