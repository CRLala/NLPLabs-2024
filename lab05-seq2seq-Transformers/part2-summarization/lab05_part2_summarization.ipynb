{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 5, part 2: Seq2Seq Transformers â€” Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXk4O_B___rO"
      },
      "outputs": [],
      "source": [
        "! pip install datasets transformers rouge-score nltk torch numpy matplotlib      # << Uncomment to install packages\n",
        "import transformers\n",
        "print(transformers.__version__)     # Should be >= 4.11.0\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kTCFado4IrIc"
      },
      "source": [
        "In this notebook, we will see how to fine-tune one of the [HuggingFace Transformers](https://github.com/huggingface/transformers) model for a summarization task. We will use the [XSum dataset](https://arxiv.org/pdf/1808.08745.pdf) (for extreme summarization) which contains BBC articles accompanied with single-sentence summaries.\n",
        "\n",
        "![Widget inference on a summarization task](https://raw.githubusercontent.com/huggingface/notebooks/main/examples/images/summarization.png)\n",
        "\n",
        "We will see how to easily load the dataset for this task using HuggingFace Datasets and how to fine-tune a model on it using the `Trainer` API.\n",
        "\n",
        "This tutorial is draws from the Huggingface Summarization Tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "We will use the [HuggingFace Datasets](https://github.com/huggingface/datasets) library to download the data we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset`. The Datasets library is a great resource for conveniently working with common datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IreSlFmlIrIm",
        "outputId": "6564e8c9-ec95-4527-84ba-b83e3b5a2cbb"
      },
      "outputs": [],
      "source": [
        "from datasets import list_datasets, load_dataset\n",
        "\n",
        "# Can see all possible datasets as follows\n",
        "all_datasets = list_datasets()\n",
        "print(all_datasets[:10], len(all_datasets))\n",
        "\n",
        "raw_datasets = load_dataset(\"xsum\")\n",
        "\n",
        "# Each dataset has a train, val and test split\n",
        "print(raw_datasets.keys(), '\\n', raw_datasets['train'])\n",
        "\n",
        "# An XSum sample looks as follows\n",
        "print(raw_datasets[\"train\"][0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "### EDA"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get a sense of what the data looks like, we will write a function to display some random elements. We will also perform some basic EDA (exploratory data analysis) by computing the mean and standard deviation token counts for the source and target documents.\n",
        "\n",
        "**Q1:** Complete `show_random_elements` below. This should display a table containing `num_examples` samples from the specified `dataset`, with fields `Souce Document`, `Target Document` and `Document ID`\n",
        "\n",
        "**Q2:** EDA: \n",
        " - Tokenize the document (the best option would be to use the tokenizer but here we will just split on spaces) and print the mean count and standard deviation for source and target documents for each dataset\n",
        " - Plot histograms of the source and target token counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_random_elements(dataset, num_examples=5):\n",
        "    ## YOUR CODE HERE ##\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))\n",
        "\n",
        "\n",
        "def get_token_counts(dataset):\n",
        "    ## YOUR CODE HERE ##\n",
        "    return src_counts, tgt_counts\n",
        "\n",
        "\n",
        "def token_counts_summary(raw_dataset):\n",
        "    for name,dataset in raw_dataset.items():\n",
        "        src_counts, tgt_counts = get_token_counts(dataset)\n",
        "        ## YOUR CODE HERE ##\n",
        "\n",
        "\n",
        "def plot_token_counts(dataset):\n",
        "    src_counts, tgt_counts = get_token_counts(dataset)\n",
        "    ## YOUR CODE HERE ##\n",
        "\n",
        "\n",
        "show_random_elements(raw_datasets[\"val\"])\n",
        "plot_token_counts(raw_datasets[\"validation\"])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jAWdqcUBIrJC"
      },
      "source": [
        "### Loading the metric"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To evaluate our model's performance, we will use the ROUGE summarization metric. This is provided natively within the dataset library and can be loaded similarly to how we loaded the dataset above. **Note** this is a big advantage in practise as i) metrics can be fiddly to implement manually and ii) difficult to align completely across implmentations as decisions like lemmatization, tokenization and punctation-handling can create large discrepancies in scores.\n",
        "\n",
        "You can call its `compute` method with your predictions and labels, which need to be list of decoded strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XN1Rq0aIrJC",
        "outputId": "a4405435-a8a9-41ff-9f79-a13077b587c7"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "metric = load_metric(\"rouge\")\n",
        "\n",
        "# help(metric)      # << Uncomment to see more about the ROUGE eval metric\n",
        "\n",
        "# Try it out below\n",
        "# fake_preds = ## YOUR CODE HERE\n",
        "# fake_labels = ## YOUR CODE HERE\n",
        "## COMPUTE METRIC HERE ## \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "We will proprocess the data using the Huggingface `Tokenizer`. This tokenizes and indexes the inputs and put it in a format the model expects. It also generate the other inputs that the model requires.\n",
        "\n",
        "**Q3:** Each Huggingface model has a paired tokenizer. Why is it important to use the appropriate tokenizer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"t5-small\"       # Can find more options at https://huggingface.co/models?sort=downloads&search=t5    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the tokenizer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try out the tokenizer in the cell below.\n",
        "\n",
        "**Q4:** The tokenizer output has different fields for different tokenizers. \n",
        "\n",
        "**Q4.i:** Confirm this by writing code below to instantiate a BERT tokenizer and comparing the outputs across the two tokenizers of when tokenizing a string. You may find [this page](https://huggingface.co/models) helpful.\n",
        "\n",
        "**Q4.ii:** Why does the BERT tokenizer have an additional field to the T5 tokenizer?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the tokenizer below\n",
        "print(tokenizer(\"We love NLP!\"))\n",
        "# print(tokenizer(## YOUR CODE HERE))\n",
        "\n",
        "## CODE FOR 4.i) HERE ##\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Additional tokenizer functionalities:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The tokenizer can be fed a list of strings\n",
        "- When tokenizing the target documents, using `tokenizer.as_target_tokenizer()` to ensure the target receives the appropriate special tokens (although here the source and target are tokenized identically)\n",
        "- We can convert back from ids to tokens by using `tokenizer.convert_ids_to_tokens()`\n",
        "\n",
        "**Q5:** The output of `tokenizer.convert_ids_to_tokens()` is different from the initial string. Why is this?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIaPIvgF__rs",
        "outputId": "3e87d343-a4a5-4d72-b147-9cd3d5f8efc9"
      },
      "outputs": [],
      "source": [
        "print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))\n",
        "\n",
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))\n",
        "\n",
        "print(tokenizer.convert_ids_to_tokens(tokenizer(\"We love NLP!\")['input_ids']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C0hcmp9IrJQ"
      },
      "source": [
        "T5 was trained within a multitask framework such that it can perform multiple tasks out-of-the-box. We prefix the inputs with \"summarize: \" to prompt the model to deliver the correct outputs.\n",
        "\n",
        "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that any input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset.\n",
        "\n",
        "Complete the `preprocess_function` below to tokenize the text (**hint**: don't forget prefix or the context manager ;) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "prefix = \"summarize: \" if model_checkpoint.startswith(\"t5-\") else \"\"\n",
        "\n",
        "max_input_length = 1024\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    ## YOUR CODE HERE ##\n",
        "\n",
        "    return model_inputs # should be a dict containing the indices for the source and target docs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-b70jh26IrJS",
        "outputId": "acd3a42d-985b-44ee-9daa-af5d944ce1d9"
      },
      "outputs": [],
      "source": [
        "# Test it using the below call\n",
        "preprocess_function(raw_datasets['train'][:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "This function can be applied to all our datasets using the `map` method of our `dataset` object. The results are automatically cached to avoid spending time on this step the next time you run your notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDtsaJeVIrJT",
        "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = raw_datasets.map(\n",
        "    preprocess_function, \n",
        "    batched=True            # This employs multithreading to speed up tokenization\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using T5 out-of-the-box"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The image below shows the idea behind T5.\n",
        "\n",
        "![T5 img](figs/t5.png)\n",
        "\n",
        "In case you are curoius, the following [link](https://paperswithcode.com/method/t5) shows more information on T5: \n",
        "\n",
        "Here, we will experiment with some of T5's capabilities without pre-training. First, we will download the model using `AutoModelForSeq2SeqLM` class using the `from_pretrained` method (this caches the model for us).\n",
        "\n",
        "To illustrate the impact of the prompt, we will try T5 using two different prompts using the same string. Note that we are using the small version without fine-tuning so the results may not be great.\n",
        "\n",
        "**Note:** If GPUs are not available for you, `.cuda()` calls will throw errors. If this is the case we suggest you remove the corresponding calls and work on CPU. However, we do not recommend to run this lab session on CPUs because doing so takes more than 200 hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "model.cuda()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Q6**. Complete the `tok_and_gen` function below. This requires the following steps:\n",
        "- Tokenize the inputs\n",
        "- Generate output ids (hint: using model.generate. Google this for more details)\n",
        "- Convert the ids to string\n",
        "- Print the string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_str = 'Artificial general intelligence (AGI) is the hypothetical ability of an intelligent agent to understand or learn any intellectual task that a human being can.[1] It is a primary goal of some artificial intelligence research and a common topic in science fiction and futures studies.'\n",
        "\n",
        "prefixes = [\"translate English to German: \", \"summarize: \"]     # What happens if we make up a prompt? Why?\n",
        "\n",
        "def tok_and_gen(model, tokenizer, input, prefix=''):\n",
        "    ## YOUR CODE HERE ##\n",
        "\n",
        "    print(f'Using prefix {prefix}: ', decoded)\n",
        "\n",
        "for task_prefix in prefixes:\n",
        "    tok_and_gen(model, tokenizer, task_prefix, input_str)    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready and we have played around with our model, we can fine-tune it. \n",
        "\n",
        "HuggingFace provides an API for training a seq2seq model: the `Seq2SeqTrainer`. To instantiate this, we will need to define three more things. The most important is the [`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments), which is a class that contains all the attributes to customize the training. It requires a folder name for saving checkpoints of the model, and all other arguments are optional:\n",
        "\n",
        "**Note:** Set `fp16` argument to `False` if using CPU computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8WyIhF-__sL"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-xsum\",     # Ouptut folder\n",
        "    # Eval strategy\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    # Could alternatively be the following to eval every epoch:\n",
        "    # evaluation_strategy=\"epoch\",\n",
        "\n",
        "    # LR. Should be small (<1e-4)\n",
        "    learning_rate=2e-5,\n",
        "\n",
        "    # Batch size during training and eval\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "\n",
        "    # Limits the number of models saved during training. Important to prevent memory clogging up!\n",
        "    save_total_limit=3,\n",
        "\n",
        "    # Properly generate summaries during eval\n",
        "    predict_with_generate=True,\n",
        "    \n",
        "    # Mixed precision training (speeds up training - see Nvidia-apex for more details)\n",
        "    fp16=True,\n",
        "\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3pGVdTIrJc"
      },
      "source": [
        "Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tMJC9gy__sL"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sZOdRlRIrJd"
      },
      "source": [
        "The last thing to define for our `Seq2SeqTrainer` is how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, and we have to do a bit of pre-processing to decode the predictions into texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    \n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "Then we just need to pass all of this along with our datasets to the `Seq2SeqTrainer`. This is the general Huggingface API adapted for training seq2seq models.\n",
        "\n",
        "**Q7:** Instantiate the trainer class and begin finetuning on the XSum dataset. You may find [this page](https://huggingface.co/docs/transformers/main_classes/trainer) helpful.\n",
        "\n",
        "**Note:** training will likely take a while so you may want to end training once you are satisfied it is working correctly in order to progress to the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE ##\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complexity Analysis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we will analyze the computational complexity of the Transformer. One of the main drawbacks of this architecture is that the time and memory complexity scales poorly with respect to input length. This is particularly problematic for summarization as long document summarization can become prohibitively expensive.\n",
        "\n",
        "**Q8:** Which component of the Transformer does the poor complexity problems mentioned above referred to? Explain why this is the case.\n",
        "\n",
        "**Q9:** We will profile the time complexity of T5 for different input sequence lengths. Complete the function below which will plot the time of T5's forward pass. You can use the inbuilt pytorch profiler or a simpler method (e.g. `time.time`) as you prefer.\n",
        "- We will first do this up to 512 tokens on a log base 2 scale (i.e. 1, 2, 4, 8, ... 512)\n",
        "- Do this for the encoder (holding decoder input length fixed) and the decoder (holding encoder input length fixed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def profile_seq2seq(model, max_len, batch_size=1):\n",
        "    # Create dummy input tensors\n",
        "    base_encoder_input = torch.tensor([[10]]).cuda()\n",
        "    base_decoder_input = torch.tensor([[10]]).cuda()\n",
        "    # Make a larger batch to make trends more evident\n",
        "    base_encoder_input = base_encoder_input.repeat(batch_size,1)\n",
        "    base_decoder_input = base_decoder_input.repeat(batch_size,1)\n",
        "    \n",
        "    ## YOUR CODE HERE ##\n",
        "\n",
        "profile_seq2seq(model, 512, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above plots may or may not show an upward trajectory. Now try again with a larger batch size (e.g. 8). Now they should show an upward slope as the forward pass becomes more expensive relative to other overheads."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Lab 05 - Part 2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
